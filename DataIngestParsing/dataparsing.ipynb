{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7e3afed",
   "metadata": {},
   "source": [
    "load pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "022e4b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    PyMuPDFLoader\n",
    ")\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ed7d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 0, 'page_label': '1'}, page_content='. \\n. \\n Latest updates: h\\ue03cps://dl.acm.org/doi/10.1145/3768165\\n. \\n. \\n SURVEY\\nA Comprehensive Survey of Small Language Models\\nin the Era of Large Language Models: Techniques,\\nEnhancements, Applications, Collaboration with LLMs,\\nand Trustworthiness\\nFALI WANG, Pennsylvania State University, University Park, PA, United\\nStates\\n. \\n ZHIWEI ZHANG, Pennsylvania State University, University Park, PA,\\nUnited States\\n. \\n XIANREN ZHANG, Pennsylvania State University, University Park, PA,\\nUnited States\\n. \\n ZONGYU WU, Pennsylvania State University, University Park, PA, United\\nStates\\n. \\n TZU HAO MO, University of Pennsylvania, Philadelphia, PA, United\\nStates\\n. \\n QIUHAO LU, University of Texas Health Science Center at Houston,\\nHouston, TX, United States\\n. \\n View all\\n. \\n. \\n Open Access Support provided by:\\n. \\n Pennsylvania State University\\n. \\n Rensselaer Polytechnic Institute\\n. \\n University of Texas Health Science Center at Houston\\n. \\n University of Pennsylvania\\n. \\nAmazon.com, Inc.\\n. \\n PDF Download\\n3768165.pdf\\n23 December 2025\\nTotal Citations: 12\\nTotal Downloads:\\n2364\\n. \\n. \\n Published: 24 November 2025\\nOnline AM: 18 September\\n2025\\nAccepted: 23 August 2025\\nRevised: 18 August 2025\\nReceived: 02 January 2025\\n. \\n. \\n Citation in BibTeX format\\n. \\n. \\n ACM Transactions on Intelligent Systems and Technology, Volume 16, Issue 6 (December 2025)\\nh\\ue03cps://doi.org/10.1145/3768165\\nEISSN: 2157-6912\\n.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 1, 'page_label': '2'}, page_content='A Comprehensive Survey of Small Language Models in the\\nEra of Large Language Models: Techniques, Enhancements,\\nApplications, Collaboration with LLMs, and Trustworthiness\\nFALI W ANG, ZHIWEI ZHANG, XIANREN ZHANG, and ZONGYU WU, The Pennsylvania\\nState University, University Park, PA, USA\\nTZUHAO MO, University of Pennsylvania, Philadelphia, PA, USA\\nQIUHAO LU, W ANJING W ANG, and RUI LI, The University of Texas Health Science Center at\\nHouston, Houston, TX, USA\\nJUNJIE XU, The Pennsylvania State University, University Park, PA, USA\\nXIANFENG TANG and QI HE, Amazon.com Inc., Palo Alto, CA, USA\\nYAO MA, Rensselaer Polytechnic Institute, Troy, NY, USA\\nMING HUANG, The University of Texas Health Science Center at Houston, Houston, TX, USA\\nSUHANG W ANG, The Pennsylvania State University, University Park, PA, USA\\nLarge language models (LLMs) have demonstrated emergent abilities in text generation, question answering,\\nand reasoning, facilitating various tasks and domains. Despite their proficiency in various tasks, LLMs like\\nPaLM 540B and Llama-3.1 405B face limitations due to large parameter sizes and computational demands,\\noften requiring cloud API use, which raises privacy concerns, limits real-time applications on edge devices, and\\nincreases fine-tuning costs. Additionally, LLMs often underperform in specialized domains such as healthcare\\nand law due to insufficient domain-specific knowledge, necessitating specialized models. Therefore, Small\\nLanguage Models (SLMs) are increasingly favored for their low inference latency, cost-effectiveness, efficient\\ndevelopment, and easy customization and adaptability. These models are particularly well-suited for resource-\\nlimited environments and domain knowledge acquisition, addressing LLMs’ challenges and proving ideal for\\napplications that require localized data handling for privacy, minimal inference latency for efficiency, and\\ndomain knowledge acquisition through lightweight fine-tuning. The rising demand for SLMs has spurred\\nThis material is based upon work supported by, or in part by, the Army Research Office under grant number W911NF21-1-\\n0198, the Department of Homeland Security CINA under grant number 17STCIN0000105-00, Cisco Faculty Research Award.\\nThe findings and conclusions in this article do not necessarily reflect the views of the funding agencies.\\nAuthors’ Contact Information: Fali Wang, The Pennsylvania State University, University Park, PA, USA; e-mail:\\nfqw5095@psu.edu; Zhiwei Zhang, The Pennsylvania State University, University Park, PA, USA; e-mail: zbz5349@psu.edu;\\nXianren Zhang, The Pennsylvania State University, University Park, PA, USA; e-mail: xzz5508@psu.edu; Zongyu Wu, The\\nPennsylvaniaStateUniversity,UniversityPark,PA,USA;e-mail:zzw5373@psu.edu;TzuHaoMo,UniversityofPennsylvania,\\nPhiladelphia, PA, USA; e-mail: investdmo@gmail.com; Qiuhao Lu, e-mail: qiuhao.lu@uth.tmc.edu; Wanjing Wang, The\\nUniversity of Texas Health Science Center at Houston, Houston, TX, USA; e-mail: wanjing.wang@uth.tmc.edu; Rui Li,\\nThe University of Texas Health Science Center at Houston, Houston, TX, USA; e-mail: rui.li.1@uth.tmc.edu; Junjie Xu, The\\nPennsylvania State University, University Park, PA, USA; e-mail: jmx5097@psu.edu; Xianfeng Tang; Amazon.com Inc., Palo\\nAlto, CA, USA; e-mail: tangxianfeng@outlook.com; Qi He, Amazon.com Inc., Palo Alto, CA, USA; e-mail: qih@amazon.com;\\nYao Ma, Rensselaer Polytechnic Institute, Troy, NY, USA; e-mail: may13@rpi.edu; Ming Huang, The University of Texas\\nHealth Science Center at Houston, Houston, USA; e-mail: ming.huang@uth.tmc.edu; Suhang Wang (corresponding author),\\nThe Pennsylvania State University, University Park, PA, USA; e-mail: szw494@psu.edu.\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the\\nfull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\\nprior specific permission and/or a fee. Request permissions frompermissions@acm.org.\\n© 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM 2157-6912/2025/11-ART145\\nhttps://doi.org/10.1145/3768165\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 2, 'page_label': '3'}, page_content='145:2 F. Wang et al.\\nextensive research and development. However, a comprehensive survey investigating issues related to the\\ndefinition, acquisition, application, enhancement, and reliability of SLM remains lacking, prompting us to\\nconduct a detailed survey on these topics. The definition of SLMs varies widely; thus, to standardize, we\\npropose defining SLMs by their capability to perform specialized tasks and suitability for resource-constrained\\nsettings, setting boundaries based on the minimal size for emergent abilities and the maximum size sustainable\\nunder resource constraints. For other aspects, we provide a taxonomy of relevant models/methods and develop\\ngeneral frameworks for each category to enhance and utilize SLMs effectively. We have compiled the collected\\nSLM models and related methods on GitHub:https://github.com/FairyFali/SLMs-Survey.\\nCCS Concepts: •Computing methodologies→ Natural language generation;\\nAdditional Key Words and Phrases: Small Language Models, On-Device LLMs, Domain-specific Models,\\nTrustworthiness\\nACM Reference format:\\nFali Wang, Zhiwei Zhang, Xianren Zhang, Zongyu Wu, TzuHao Mo, Qiuhao Lu, Wanjing Wang, Rui Li, Junjie\\nXu, Xianfeng Tang, Qi He, Yao Ma, Ming Huang, and Suhang Wang. 2025. A Comprehensive Survey of Small\\nLanguageModelsintheEraofLargeLanguageModels:Techniques,Enhancements,Applications,Collaboration\\nwith LLMs, and Trustworthiness.ACM Trans. Intell. Syst. Technol. 16, 6, Article 145(November 2025), 87 pages.\\nhttps://doi.org/10.1145/3768165\\n1 Introduction\\nThe evolution of neurallanguage models (LMs)from BERT’s [86] pre-training and fine-tuning\\nparadigm to T5’s [291] pre-training plus prompting approach and finally to GPT-3’s [38] pre-\\ntraining plus in-context learning has greatly enhancednatural language processing (NLP).\\nThese advancements have broadened NLP’s application across various fields, including language\\nunderstanding[356],programming[ 262,337],recommendationsystems[ 375],informationretrieval\\n[45, 155, 232, 323], mobile-device control [90], scientific discovery [318, 441], medical question\\nanswering [35, 372], and legal question answering [12]. In particular, the recent emergence of\\nproprietary commercial models, including ChatGPT, Bard, and Claude, and open-sourced models\\nsuch as Llama [96, 344, 345] has led to rapid growth in the development oflarge language\\nmodels (LLMs). Even though neural networks consistently improve on various tasks with longer\\ntraining times, larger datasets, and increased model sizes—a phenomenon known as a neural scaling\\nlaw [169], these models unpredictably exhibit a sudden acquisition of versatile abilities, termed\\n“emergent ability ,” once they reach a critical scale threshold, thereby supporting the “larger is\\nbetter” trend. This ability is not present in small-scale models. For instance, the latest Llama-3.1\\nmodel with 405 billion parameters performs better in dialogue, logical reasoning, and programming\\ncompared to the smaller 7B counterpart [96].\\nDespite their prowess in complex tasks, LLMs’ huge parameters and computational needs impose\\nsignificant limitations, hindering their adoption in many real-world applications. For example,\\nthe LLaMa 3.1 model with 405 billion parameters [96], trained on 16\\u2009K H100 GPUs for 54\\u2009days,\\nrequires about 202.5\\xa0GB of GPU memory using int4 precision and has large inference latency.\\nThese issues present several challenges in specific contexts: (1) LLMs are generally hosted in the\\ncloud and used via cloud-based APIs due to the large GPU memory and computational cost. Users\\nneed to upload their data to query LLMs, raising data leakage and privacy concerns, especially in\\nhigh-stake scenarios such as healthcare, finance, and e-commerce; (2) Driven by personal agents,\\non-device deployment is a critical requirement. Several factors, including cloud costs, latency, and\\nprivacy concerns, hinder the on-device processing of cloud-based LLMs, and direct deployment is\\nimpractical due to their high parameter and cache requirements, which often exceed the capabilities\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 3, 'page_label': '4'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:3\\nFig. 1. Overview of small language models.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 4, 'page_label': '5'}, page_content='145:4 F. Wang et al.\\nFig. 2. Download statistics last month in Hugging face for LLMs with various model sizes, obtained on\\nOctober 7, 2024.\\nof devices such as mobile phones; (3) Their large parameter count can cause inference delays\\nfrom seconds to minutes, unsuitable for real-time applications. For instance, Llama 2 7B takes\\napproximately 84\\u2009seconds to process 100 tokens on benchmarks including HellaSwag, TruthfulQA,\\nMMLU, and Arc_C when run on a smartphone equipped with a Snapdragon 685 processor [342];\\n(4) To boost performance in specialized domains such as healthcare and law, where generic LLMs\\nunderperform, LLMs are often fine-tuned. However, this process is computationally expensive due\\nto their large size. (5) Though general-purpose LLMs are powerful, many real-world applications\\nrequire only specific abilities and domain knowledge; deploying general-purpose LLMs would be\\na waste of resources, and such LLMs often cannot match the performance of models tailored for\\nspecific tasks [54, 127, 159, 282, 375].\\nRecently,small language models (SLMs)have shown great potential in alleviating these issues\\nwhileachievingperformance comparable to LLMs for domain-specific problems[1,27,118,146,227,\\n281, 339, 342, 359, 407, 444]. Owing to fewer parameters, SLMs excel in efficiency, cost, flexibility,\\nand customization. They provide significant computational savings in pre-training and inference\\nwith reduced memory and storage needs, which is vital for applications requiring efficient resource\\nuse. These small models are especially effective in resource-limited settings, performing well on\\nlow-power devices such as edge devices. Besides, SLMs improve on-device processing by enhancing\\nprivacy, security, response times, and personalization. This supports advanced personal assistants\\nand cloud-independent applications, boosting energy efficiency and reducing carbon emissions. For\\nexample, the Llama 3.2 models (1B and 3B) demonstrate that local processing enables immediate\\nexecution of prompts and responses [7]. This approach protects privacy by keeping sensitive data\\nsuch aspatient health information (PHI), business data, personal messages, and calendar details\\nlocal, enhancing confidentiality. It also allows for precise control over which queries are processed\\non-device versus those requiring cloud-based models. Therefore, SLMs are gaining increasing\\nattention as alternatives to LLMs, as indicated in Figure2, which shows that SLMs are downloaded\\nmore frequently than larger models in the Hugging Face community, and Figure3, which illustrates\\nthe growing popularity of SLM releases over time.\\nTypically, LMs that exhibit emergent abilities are classified as LLMs. However, the categorization\\nof SLMs remains unclear. Studies vary in their contexts: some define SLMs as models with fewer\\nthan one billion parameters [227], while others consider the term “small language model” relative\\nto the larger counterparts [186, 333, 375], with no consensus on a unified definition in the current\\nlandscape of LLMs. Research suggests SLMs for mobile devices, typically possessing around 6\\xa0GB\\nof memory, consist of sub-billion parameter models [227], whereas others classify models with up\\nto 10 billion parameters as small, noting their lack of emergent abilities [107]. Given their use in\\nresource-constrained environments and for specific tasks, we propose a generalized definition:\\nGiven specific tasks and resource constraints, we define SLMs as falling within a range where the lower\\nbound is the minimum size at which the model exhibits emergent abilities for a specialized task, and\\nthe upper bound is the largest size manageable within limited resource conditions. This definition\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 5, 'page_label': '6'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:5\\nFig. 3. A timeline of existing SLMs.\\nintegrates various perspectives and addresses factors related to mobile computing and capability\\nthresholds.\\nDue to the growing demand for SLMs, extensive literature has emerged on various aspects of\\nSLMs. For example, several resource-efficient techniques [403] and training methods optimized for\\nSLMs, such asQuantization-Aware Training (QAT)[226, 363, 405] and selective architectural\\ncomponent choices [227, 287, 342], aim to enhance performance in specific applications [9, 37,\\n54, 282, 299, 387]. These methods have led to the development of numerous open-source, general-\\npurpose, and domain-specific SLMs [3, 27, 35, 339, 407, 440]. Beyond their inherent capabilities,\\nSLMs can enhance LLMs by serving as modules or effective proxies [252, 304, 388, 404, 418, 454].\\nFurthermore, the complementary advantages of SLMs and LLMs can be leveraged collectively\\nto better complete tasks [82, 208, 239, 314, 360, 442]. Despite the commendable performance of\\nSLMs, it is crucial not to overlook their credibility issues, such as the risks of adversarial attacks,\\nproducing hallucinations, and privacy breaches [91, 97, 141, 179, 261, 280, 254, 357, 376, 430].\\nHowever, currently, there is no comprehensive survey thoroughly exploring these works on SLMs\\nin the era of LLMs. Therefore, this article presents the first comprehensive survey analyzing various\\naspects of SLMs in the LLM era and their future directions. The overview structure of our article is\\nshown in Figure1. To summarize, our major contributions are:\\n—InSection 3,weexaminevarioustechniquesforimprovingtheperformanceofSLMs,including\\ntrainingfromscratch,fine-tuning, knowledgedistillation(KD) ,quantization,andleveraging\\nLLM-enhancing technologies to optimize SLMs.\\n—In Section4, we discuss the tasks that SLMs can enhance and the deployment strategies\\nthat enable models to fit within the resource constraints of edge devices while maintaining\\nacceptable inference speed.\\n—In Section 5, we collect SLMs with fewer than 7 billion parameters across both general-\\npurpose and domain-specific applications, reviewing common architectural choices, training\\ntechniques, and datasets, and providing a comparative summary of performance across\\ndifferent model sizes. Recent SLMs are listed.\\n—In Section6, we explore how SLMs can address key challenges faced by LLMs, such as high\\ninference latency, labor-intensive fine-tuning, susceptibility to knowledge noise, and risks of\\ncopyright infringement.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 6, 'page_label': '7'}, page_content='145:6 F. Wang et al.\\n—In Section7, we survey two kinds of synergies between LLMs and SLMs: one involves cloud-\\nbased LLMs and local SLMs, while the other leverages the unique advantages of both to more\\neffectively solve tasks.\\n—In Section8, we investigate the trustworthiness issues of SLMs, including hallucination and\\nprivacy concerns, by providing a taxonomic summary of current evaluation methods.\\nConcurrently with our survey, Lu et al. [233] evaluate open-source SLMs, focusing on their\\narchitectures, datasets, algorithms, and on-device performance metrics such as inference latency\\nand memory usage. Van Nguyen et al. [351] delve into optimization strategies for SLMs, including\\nmodel compression, pruning, and quantization. Chen and Varoquaux [51] investigate how SLMs\\nenhance LLMs and vice versa. In contrast, our survey offers a more comprehensive review with the\\nfollowing differences: (1) we present a detailed taxonomy of recent advancements in SLMs in the\\nera of LLMs; (2) we define SLMs based on emergent capabilities and device specifications, which\\nrefines previous unclear definitions related to LLMs; (3) we discuss SLM applications, especially in\\non-device tasks and deployment, topics previously unexplored; (4) we examine domain-specific\\nSLMspreviouslyoverlooked;and(5)weadditionallyconsiderthesynergybetweenSLMsand\\xa0LLMs.\\n2 Foundational Concepts in Building LMs\\nThis section will introduce foundational concepts and background knowledge for LMs, including\\nthe concepts of architecture and the training process, as well as methods for obtaining SLMs from\\nLLMs. The advanced training strategy to improve SLM performance will be introduced in Section3.\\n2.1 Architecture of SLMs\\nSLMs commonly employ the Transformer architecture [352] (see Figure4), which utilizes self-\\nattention mechanisms to manage long-range text dependencies, essential for maintaining\\nFig. 4. Transformer architecture [ 352].\\nperformance with constrained resources. However, due\\nto the attention mechanism, Transformers have a large\\ninference cost. Hence, to alleviate the issue, several\\nsubquadratic-time architectures such as Mamba [119],\\nHymba [93], and xLSTM [26] are proposed. Next, we\\nwill give details of Transformer due to its popularity and\\nbriefly introduce newly emerged models.\\n2.1.1 Transformer. The Transformer’s self-attention\\nmechanism [352] allows LMs to efficiently capture con-\\ntextual information across longer sequences, even with\\nlimited resources. The Transformer generally adopts\\nan encoder–decoder structure featuring self-attention\\nmechanisms, FFNs positional embeddings, and layer nor-\\nmalization. The Transformer architecture design tai-\\nlored for SLMs is detailed in Section 5; this section\\nwill provide only foundational concepts.\\nThe Self-Attention Mechanism enables the model to\\nevaluate the importance of tokens relative to each other.\\nThe self-attention mechanism is written as\\nAttention(Q,K,V) = softmax\\n( QK⊤\\n√3:\\n)\\nV,\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 7, 'page_label': '8'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:7\\nwhere Q, K, andV are query, key, and value matrices, scaled by√3: for stability where3: is the\\ndimension of key matrices. The dot productQK⊤ reflects the similarity between the query and key\\nvectors.\\nMulti-Head Attention (MHA)[352] is the first method that uses multiple heads to capture\\ndiverse information. MHA allows the model to attend to different parts of the input sequence using\\nmultiple attention heads as\\nMultiHead(Q,K,V) = Concat(head1,head2,..., headℎ)W$, with head8\\n= Attention(QW&\\n8 ,KW \\n8 ,VW+\\n8 ), (1)\\nEach head in the MHA mechanism operates independently, allowing the model to capture diverse\\naspects of the data. The outputs are combined using learned projection matricesW&\\n8 , W \\n8 , andW+\\n8 ,\\nconcatenated, and passed through the output projection matrixW$.\\nBuilding on this foundation, several modifications have been introduced to further optimize self-\\nattention mechanisms for specific challenges such as memory efficiency and computational speed.\\nTo address the KV-cache bottleneck in MHA,Multi-Query Attention (MQA)[310] proposes\\nthat all attention heads share the same set of keys and values, which reduces the memory and\\ncomputational overhead associated with storing and managing multiple key-value pairs.Grouped\\nQuery Attention (GQA)[8] serves as a middle ground between MHA and MQA. It introduces\\nsubgroups of query heads (fewer than the total number of attention heads), where each subgroup\\nshares a single key and value head. Unlike MQA and GQA, which reduce the number of key\\nand value heads,Multi-Head Latent Attention (MLA)[216] compresses the keys and values\\ninto a joint latent vector. This compression allows for efficient handling of key-value pairs while\\nmaintaining high performance, significantly reducing the KV-cache and improving inference\\nefficiency. Flash Attention [76, 77] accelerates the self-attention mechanism by minimizing the\\nmemory overhead typical of standard attention calculations. This optimization allows SLMs to\\nprocess longer sequences more efficiently, enhancing their functionality under strict hardware\\nconstraints.\\nFeedforwardNetwork(FFN) comprisestwolineartransformationsseparatedbyanon-linearity,\\ntypically modeled asFFN(x) = f(xW1 + 11)W2 + 12, whereW1 and W2 are the weight matrices,\\nand11 and12 are bias terms.f is the activation function, which introduces non-linearity, allowing\\nmodels to learn complex patterns. Generally, ReLU is used as the activation function. In addition to\\nReLU, activation functions such as GeLU and SiLU are also used in SLMs to improve performance.\\nWegivethedetailshere:(i) ReLU(RectifiedLinearUnit) [5]isdefinedas f(G) = max(0,G),which\\nis commonly used for its simplicity and effectiveness. (ii)Gaussian Error Linear Unit (GELU)\\n[138] is defined asGELU(G) = G· Φ(G) = G· 1\\n2\\n[\\n1 + erf\\n(\\nG√\\n2\\n)]\\n, whereΦ(G) is the standard Gaussian\\nCDFand erf isthe errorfunction. Itis smootherthan ReLUand widelyusedin modelssuch asBERT\\n[86] and GPT [289] for better gradient flow control. Since calculating the Gaussian error function\\nfor each neuron is computationally expensive and time-consuming, there are approximations\\nusing tanh and sigmoid functions, corresponding toGELUtanh and SiLU: (iii)GELU with tanh is\\ndefined asGELUtanh(G) = 0.5 · G ·\\n(\\n1 + tanh\\n(√\\n2\\nc · ( G+ 0.044715 · G3)\\n))\\n. This approximation uses\\nthe Tanh function to simplify computations. (iv)Sigmoid Linear Unit (SiLU)[99] is calculated as\\nSiLU(G) = G · sigmoid(G) = G · 1\\n1+4−G . It effectively combines the sigmoid function with its input,\\nenhancing modeling capabilities. (v)Swish-Gated Linear Units (SwiGLU)[311] integrates the\\nSwishactivationfunctionwithGatedLinearUnits,definedas SwiGLU(G) = Swish(G·,+1)⊙( G·++\\n2), where,,+ are the weight matrix and1,2 are the bias terms. The Swish function is expressed as\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 8, 'page_label': '9'}, page_content='145:8 F. Wang et al.\\nSwish(G) = G·sigmoid(G).Thiscombinationenhancesexpressivenessandcomputationalefficiency,\\nmaking it a preferred choice in advanced models such as the Qwen series [407].\\nPositional Embeddings in Transformer models [352] are essential for capturing token order, pro-\\nviding context about relative positions within a sequence. Traditional positional embeddings in the\\nTransformer architecture utilize a sinusoidal function, defined as: %\\x1a(?>B,28) =\\nsin\\n(\\n?>B\\n1000028/3model\\n)\\n,%\\x1a (?>B,28+ 1) = cos\\n(\\n?>B\\n1000028/3model\\n)\\n, where?>B represents the position within the\\nsequence,8is the dimension index, and3model is the dimensionality of the model. To improve the\\nmodel’s capacity for understanding the relative positions of tokens within a sequence,Rotary\\nPositional Embedding (RoPE)[324] introduces a rotational matrix to the embeddings. RoPE\\nsignificantly enhances the positional encoding by maintaining the relative distances through ro-\\ntational transformations, thus optimizing the model’s interpretative ability regarding sequence\\ndynamics.\\nLayer Normalization [188] stabilizes the training process by normalizing layer outputs, acceler-\\nating convergence. Two types of layer normalization are commonly used [188]: (i)Non-Parametric\\nLayer Norm normalizes inputs using the mean and variance calculated across the layer’s dimen-\\nsions without learnable parameters asLN(G) = G−`\\nf , where` is the mean andf is the standard\\ndeviation of the inputs. Its simplicity makes it ideal for SLMs. (ii)Parametric Layer Norm in-\\ncludes learnable parametersW and V for adaptive scaling and bias, enhancing model flexibility:\\nPLN(G) = W (G−`\\nf\\n) + V Additionally,RMSNorm (Root Mean Square Layer Normalization) [436] sim-\\nplifies the calculation by using the root mean square of inputs, reducing computational demands:\\nRMSNorm(G) = W G√\\n1\\n#\\nÍ#\\n8=1 G2\\n8 +n\\n+ V, where# is the number of inputs,G8 is the8th input, andn is a\\nsmall constant to prevent division by zero.\\nFig. 5. Mamba 1 architecture [ 119].\\n2.1.2 Mamba. TheattentionmechanisminTrans-\\nformer suffers from a drawback: it requires recalcu-\\nlating attention scores with every previous token for\\neach new token generated during inference, lead-\\ning to quadratic time complexity. This increases the\\ninference cost as sequence lengths grow. In con-\\ntrast, Mamba [78, 119], based onstate space mod-\\nels(SSMs) [167],whichareasuperclassofrecurrent\\nneural networks, relies only on the last hidden state\\nfor generating the next token, enabling faster infer-\\nence speeds, as shown in Figure5. To address the\\nLinear Time Invariant nature of traditional SSMs,\\nwhich hinders their ability to focus on or ignore\\nspecific inputs, Mamba improves SSMs with a dynamic selection mechanism. This mechanism\\nselectively filters out irrelevant information while retaining essential data, tailored to the content\\nof the input. Leveraging this selective SSM foundation, Mamba adeptly captures complex global\\nrelationships within sequence data. Due to its focus on the immediate previous hidden state, as\\nopposed to Transformer, which requires access to all previous hidden states, Mamba achieves\\na higher utilization rate of model parameters. This makes it more suitable for SLMs. However,\\nwe identify two drawbacks of Mamba: (i) its focus on selectively capturing global information\\nmay compromise performance on tasks that require nuanced understanding, such as detailed\\nsentiment analysis or complex entity recognition and (ii) to balance inference speed, Mamba’s\\nrecurrent structure primarily encodes static global information, which limits its effectiveness in\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 9, 'page_label': '10'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:9\\nhandling multi-round tasks within a single query, such as interactive dialogue systems or iterative\\nproblem-solving scenarios.\\nIn language modeling, Mamba 1 [119] is pre-trained on the Pile dataset [109] using the training\\nrecipe from [38] and ranges from 125M to 1.3\\xa0B parameters. It outperforms comparable models\\nsuch as Pythia [32] and RWKV [277] in various tasks; for instance, Mamba-1.4B achieves a 32.8%\\naccuracy on the Arc-Challenge [68] dataset, surpassing Pythia-1.4B’s 28.5% and RWKV-1.5B’s\\n29.4%. Mamba 2 [78] develops a theoretical framework linking SSMs with attention mechanisms\\nthrough structured semi-separable matrices, enhancing the selective SSM to achieve 2–8× faster\\nspeeds while competing with Transformer models. Training and configuration for Mamba 2 align\\nwith Mamba 1. Additionally, Mamba-series models are applied widely across different fields [163,\\n287, 288, 467]. Other follow-up Mamba-based LMs, such as Falcon Mamba 7B [467] and Jamba\\n[210], also demonstrate the efficiency and scalability of the Mamba architecture for NLP tasks.\\nFalcon Mamba 7B scales Mamba’s long-sequence processing capabilities to large-scale language\\ndata, reducing memory overhead and excelling in long-form generation. Jamba further extends this\\nby blending Transformer and Mamba layers (withMixture-of-Experts (MoEs)), achieving both\\nhigh throughput and a compact memory footprint even at a massive scale.\\nFig. 6. Hymba [ 93] architecture.\\n2.1.3 Hymba. Attentionheads\\nin the Transformer facilitate\\nhigh-resolutionrecall,whileSSM\\nheadsinMambaefficientlysum-\\nmarize context. To balance per-\\nformance and efficiency for\\nSLMs, Hymba [93] integrates\\nboth attention and SSM heads\\nwithin the same layer, allowing\\nfor parallel and complementary\\nprocessing of inputs, as depicted\\nin Figure6. This hybrid-head approach enables each layer to simultaneously leverage the high-\\nresolution recall of attention heads and the contextual summarization of SSMs, increasing the\\nmodel’s flexibility and expressiveness in managing diverse information flows and memory access\\npatterns.\\nHymba has been developed in models of varying sizes—125M, 350M, and 1.5B, trained on a\\ncombination of the DCLM-Baseline-1.0 [195], SmolLM-Corpus [28], and a proprietary high-quality\\ndataset, with token counts of 1 trillion, 250 billion, and 50 billion, respectively. The models incor-\\nporate theWarmup-Stable-Decay (WSD)learning rate scheduler [146] and the data annealing\\ntechnique [96] to ensure stable pretraining, conducted on 128 NVIDIA A100 GPUs. The 1.5B base\\nmodel was post-trained usingfull fine-tuning (FFT), followed bydirect preference optimiza-\\ntion (DPO)[290] to develop the Hymba-1.5B-Instruct model. In commonsense reasoning tasks,\\nthe Hymba 1.5B model surpasses Llama-3.2-3B [7] by achieving 1.32% higher average accuracy,\\nrequiring an 11.67× smaller cache size, and delivering a 3.49× increase in processing speed.\\n2.1.4 xLSTM. Long Short-Term Memory (LSTM)[140] shares a conceptual similarity with\\nMamba that achieves success in language modeling through the introduction of time-dependent\\nweights. This similarity raises an intriguing question: how effective would LSTMs be at language\\nmodeling if scaled to billions of parameters, incorporating advanced techniques from modern LLMs\\nwhile addressing known LSTM limitations? Inspired by this question, Beck et al. [26] propose the\\nxLSTM architecture, which performs favorably compared to state-of-the-art Transformers and\\nSSMs in empirical evaluations. To address the limitations of LSTM, xLSTM designsexponential\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 10, 'page_label': '11'}, page_content='145:10 F. Wang et al.\\ngates to enhance effectiveness with long sequences, expands memory cells from scalars to matrices\\nto increase storage capacity, and removes memory mixing to enable parallel processing.\\nTo test the language modeling capabilities of xLSTM scaled to billions of parameters, it is trained\\non a large dataset comprising 300 billion tokens from SlimPajama [321] across various model sizes\\n(125M, 350M, 760M, 1.3B). The performance of pre-trained xLSTM is compared against RWKV-4\\n[277], Llama [156], and Mamba [119] across 571 text domains of the PALOMA benchmark [242] and\\nvarious downstream tasks. Across all model sizes and the majority of tasks, xLSTM consistently\\noutperforms the others, suggesting that larger xLSTM models could become formidable competitors\\nto existing LLMs that utilize Transformer technology.\\n2.2 Training SLMs from Scratch\\nTraining SLMs from scratch entails several critical steps: (i) Pre-training, focused on acquiring\\ngeneral features and knowledge from the corpus; (ii) Fine-tuning, targeted at boosting the model’s\\nabilities and performance for specific tasks; (iii) Decoding strategies, which involve the methods\\nused for iteratively selecting the next token during generation.\\n2.2.1 Pre-Training. Typically, the pre-training paradigm for LMs is divided into encoder-based\\nand decoder-based approaches. Encoder-based models, such as BERT [86], utilizeMasked Lan-\\nguage Modeling (MLM)tasks where the goal is to predict masked tokens within a sentence.\\nThis is achieved by maximizing:%(masked token | context) = softmax(W · hmask + 1), where\\nthe masked token is the original token that has been masked, context represents the other un-\\nmasked tokens in the sentence,W and1are trainable parameters of a linear output layer,hmask\\nis the output from the transformer encoder for the masked position, and softmax is the activa-\\ntion function that converts logits to probabilities over the vocabulary. This process enhances the\\nmodel’s language encoding capabilities. Decoder-based models, such as GPT [289], employ Next\\nToken Prediction (NTP) tasks, aiming to model the distribution of the next token by maximizing\\n%(next token| context) = softmax(W · hlast + 1),where next token is the token that the model\\naims to predict, context represents the sequence of tokens preceding the token to be predicted,\\nand hlast is the output from the transformer encoder for the last token in the context. Effective\\ndata preprocessing, crucial for optimizing the performance of SLMs trained from scratch, involves\\nmeticulous data cleaning and strategic tokenization.\\nData Cleaning involves techniques such as filtering, deduplication, and noise reduction, which\\nimprove data quality and help the model generalize better. Filtering noisy or irrelevant data,\\naddressing outliers, and handling imbalances in the dataset ensure that the training data is both\\nrepresentative and efficient. Deduplication, in particular, helps prevent overfitting by removing\\nrepeated instances, making the model more robust with efficient parameter usage.\\n2.2.2 Fine-Tuning. After the initial training, SLMs are fine-tuned on specific tasks using task-\\nspecific data and loss functions. Parameter-efficient fine-tuning methods [120, 143, 145, 203], such\\nas Low-Rank Adaptation (LoRA), prefix-tuning, and adapter modules, are particularly effective\\nfor SLMs. LoRA [145] modifies Transformer weights by introducing trainable low-rank matricesA\\nand B for efficient fine-tuning, avoiding significant alterations to pretrained weights. The update\\nis represented as:ΔW = AB⊤ The fine-tuned weight matrix used in Transformer operations then\\nbecomes: Wft = W + UΔW, whereU is a scaling factor adjusting the adaptation’s impact, allowing\\nfine-tuning on a smaller set of parameters while retaining the model’s foundational capabilities.\\nPrefix-Tuning [203] prepends learnable prefixes to the input sequence, guiding the model’s attention\\nwithout altering core model parameters. It is especially useful for generative tasks.Adapter Modules\\n[143] are small, trainable layers inserted into the pre-trained model. These layers are fine-tuned on\\ntask-specific data, allowing the base model to remain fixed while the adapters learn the necessary\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 11, 'page_label': '12'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:11\\nadjustments.Thetypicalstructureofanadaptermoduleincludesadown-projection,anon-linearity,\\nand an up-projection:Adapter(h) = h + Wup · f(Wdown · h + bdown) + bup, whereh is the input\\nhidden state,Wdown and Wup are the projection matrices,bdown and bup are the bias terms, andf\\nis a non-linear activation function.\\n2.2.3 Decoding Strategies. After pre-training or fine-tuning, employing an effective decoding\\nstrategy is crucial for generating output from language models. Decoding, the process of text\\ngeneration from SLMs, involves iteratively selecting the next word. A fundamental method is\\nthe greedy search, which predicts the most likely token at each step. This is formally modeled\\nas: G8 = arg maxG %(G | G<8), whereG8 is the token with the highest probability at the8th step,\\nconditioned on the preceding contextG<8. Other decoding strategies, such as beam search or top-k\\nsampling, are crucial for generating high-quality outputs. Beam search balances exploration and\\nexploitation by considering multiple possible sequences simultaneously, while top-k sampling\\nintroduces diversity and creativity in text generation. These strategies collectively ensure that\\nSLMs are efficient and capable of delivering high performance across various natural language\\nprocessing tasks.\\n2.3 Obtain SLMs from LLMs\\nObtaining an SLM from an LLM is crucial for deploying in resource-constrained environments.\\nInsteadoftrainingfromscratch,leveraginganLLMallowsforknowledgetransfer,enablingSLMsto\\nretain much of the LLM’s linguistic and domain knowledge with reduced training time and data. To\\nobtainSLMsfromLLMs,threeprimarytechniquesareused:pruning,KD,andquantization.Pruning\\nremoves less critical parameters, reducing model size while aiming to maintain performance. KD\\ntransfers knowledge from a large teacher model to a smaller student model, preserving much of the\\noriginal model’s understanding. Quantization decreases parameter precision, significantly lowering\\nmemory and computation needs with minimal impact on accuracy. These methods balance size\\nreduction, efficiency, and performance retention.\\nFig. 7. Unstructured and structured prun-\\ning.\\n2.3.1 Pruning. Pruningisatechniqueusedtoreducea\\nmodel’ssizeandcomputationalrequirements(e.g.,LLMs)\\nwithout significantly sacrificing its performance [128].\\nThis process involves identifying and removing less im-\\nportant or redundant parameters and components from\\nthe model. The primary goal of LLM pruning is to make\\nthe model more efficient, faster, and suitable for deploy-\\nment in resource-constrained environments. Typically,\\npruning can be categorized into two main types:unstruc-\\ntured pruning and structured pruning. An illustration of\\nunstructured pruning and structured pruning is shown\\nin Figure7.\\nUnstructured Pruning [79, 104, 205, 307, 327, 443,\\xa0447]\\nprunes an LLM by removing weights individually without considering its internal structure. The\\nleast significant parameters are pruned according to specific criteria, e.g., magnitude or impact\\non the output). This method can achieve significant compression while maintaining performance.\\nHowever, it can also lead to irregular memory access patterns and reduced hardware efficiency\\nbecausetheprunedmodellacksaregularstructure.SparseGPT[ 104]isarepresentativeunstructured\\npruning method that can reduce large-scale GPT models like OPT-175B [445] and BLOOM-176B\\n[184] to up to 60% sparsity using a novel sparse regression solver. Wanda [327] combines weight\\nmagnitudes with input activations to efficiently identify and discard less impactful parameters.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 12, 'page_label': '13'}, page_content='145:12 F. Wang et al.\\nIt operates in a single forward pass, rapidly achieving high sparsity without retraining. It is also\\nworth noting that recent studies specifically address the compatibility issues between pruning and\\nLoRA [145], such as LoRAPrune [443].\\nStructured Pruning [14, 20, 53, 112, 126, 191, 200, 237, 248, 312, 313, 390, 415, 450], which prunes\\nan LLM by targeting entire structural components—such as neurons, channels, or layers—rather.\\nThis approach allows for a direct reduction in dimensionality, thus efficiently reducing model com-\\nplexity and memory usage. Although structured pruning may lead to higher accuracy degradation\\nthan unstructured pruning, it simplifies implementation without requiring specialized hardware.\\nShortGPT [248] proposes theBlock Influence (BI)metric, which measures the significance of\\neach layer based on its transformation of hidden states. Essentially, a transformer block’s influence\\nis measured by how much it alters the hidden states. By calculating BI scores, ShortGPT determines\\nwhich layers contribute minimally to the overall performance and removes these low-importance\\nlayers. This simple yet effective layer removal strategy significantly reduces the model’s parameters\\nand computational requirements. LLM Pruner [237] offers a method to efficiently prune LLMs\\nwithout access to the original training dataset. It employs a three-step compression pipeline: Dis-\\ncovery (identifying interdependent structures), Estimation (evaluating the performance impact of\\neach group), and Recovery (post-training to address performance loss). NutePrune [200] enhances\\nstructured pruning with a Numerous-teacher method, employing variable sparsity masks and LoRA\\nmodules to guide the pruning process. This approach effectively reduces model size and complexity.\\nCOST-EFF [312] introduces a slenderized backbone—a form of structured pruning—and a multi-exit\\nmodel that employs task-specific calibration through KD. This slenderization reduces the model’s\\nspatial footprint, while the multi-exit strategy effectively balances utility and runtime costs. To\\nenhance the flexibility of structural pruning, DISP-LLM [112] breaks the structural dependencies\\nin regular methods by allowing different layers to have different subsets of features along the\\nembedding dimension.\\n2.3.2 KD. KD compresses a larger teacher model into a smaller student model by training the\\nstudenttomimictheteacher’soutputs[ 139].Thisenablesthestudenttoretainmuchoftheteacher’s\\ncapabilities with fewer parameters, making it ideal for scaling down LLMs for resource-limited\\nenvironments while maintaining performance. KD can be categorized intowhite-box and black-box\\napproaches [367, 408, 461] as shown in Figure8. InWhite-Box KD, the student has access to the\\nteacher’s internal states or output distributions [6, 121, 160, 172, 176, 272, 386, 439]. Generalized\\nKnowledge Distillation (GKD)[176] introduces skew KL divergence to stabilize gradients and\\nenhance performance, using an adaptive off-policy approach to minimize noisy feedback and\\nimprove efficiency.Black-Box KD relies only on teacher outputs without having access to model\\ninternals [50, 185, 198, 278, 366]. Methods like Distilling Step-by-Step [144] use teacher-generated\\nrationales to train smaller models, improving performance with fewer examples. LaMini-LM [385]\\ncreates a diverse instruction dataset with GPT-3.5 Turbo responses, enabling robust performance\\nin smaller models.\\n2.3.3 Quantization. Quantization reduces the storage and computational demands of LLMs by\\nconverting floating-point representations into lower-precision formats, significantly cutting both\\nstorage requirements and computational complexity. Existing methods fall into two categories:\\nPost-Training Quantization (PTQ)and QAT. Figure9 illustrates the two quantization methods.\\nPTQ, applied after training, simplifies model compression without altering the architecture or\\nrequiring retraining, though it may result in precision loss. Consider a group or block of weightsw;\\nthelinearoperationcanbeexpressedas ~ = wx,whilethequantizedversionisgivenby ~ = &(w)x.\\nGenerally, the quantization function& is defined as [212]: &(w) = Δ · Round ( w\\nΔ\\n) ,Δ = max(| w|)\\n2# −1 ,\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 13, 'page_label': '14'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:13\\nFig. 8. Illustration of white-box and black-box KD [ 251].\\nFig. 9. Illustration of QAT and PTQ.\\nwhere # is the number of quantization bits, andΔ is the quantization scale factor determined by\\nthe absolute maximum value ofw. Quantizing weights reduces model size and storage bandwidth,\\nwhile quantizing activations directly reduces memory traffic and inference latency—especially\\ncritical for hardware accelerators such as GPUs and TPUs. However, activation quantization is\\noften more sensitive due to its dynamic range variation across inputs and layers.QAT enhances\\nLLM efficiency by directly incorporating quantization into the training process, often resulting in\\nhigher accuracy than PTQ. During QAT, the forward pass utilizes quantized weights&(W) and\\nactivations&(X), while retaining full-precision values during the backward pass and for updating\\ngradients to ensure stable learning dynamics. The comparisons of the PTQ methods are summarized\\nin Table1, detailing precision, addressed problems, and technical contributions of each method.\\nIn Table2, we summarize recent quantization methods for language models, which demonstrate\\nthat substantial compression can be achieved with minimal performance degradation. Several\\napproaches, such as OneBit [405], BitNet [363], and BiLLM [150], achieve 8–16× parameter com-\\npression while maintaining perplexity or accuracy within 1–2% of full-precision baselines. Notably,\\nBitNet b1.58 [236] and PEQA [171] even match or surpass the performance of their full-precision\\ncounterparts. These methods are evaluated on a wide range of benchmarks, including WikiText2\\n[249], C4 [291], MMLU [136], and LM-Eval [111], ensuring a comprehensive assessment. Hybrid\\ncompression techniques are also gaining popularity; for example, JSQ [124] combines 4-bit quanti-\\nzation with 50% sparsity, achieving up to 8× compression with less than a 2% accuracy drop. In\\naddition, hardware-aware strategies are becoming increasingly important: I-LLM [147] emphasizes\\ninteger-onlyinferenceforefficientdeployment,andQLoRA[ 84]reducesmemoryusageby4 × while\\nenabling low-resource fine-tuning with negligible performance loss. Overall, these results suggest\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 14, 'page_label': '15'}, page_content='145:14 F. Wang et al.\\nTable 1. Representative Quantization Methods\\nMethods Bit TypeTechnical Contribution Problems\\nSqueezeLLM [173] 3-Bit PTQ Sensitivity-based non-uniform\\nquantization, dense and sparse\\ndecomposition\\nUltra-low bit quantization\\nJSQ [124] Flexible PTQ Joint Sparsification and QuantizationBetter compression-accuracy trade-offs.\\nFrameQuant [4] Fractional bitPTQ Fractional bit widths Better compression-accuracy trade-offs.\\nOneBit [405] 1-bit PTQ Quantization-aware KD 1-Bit quantization\\nBiLLM [150] 1-bit PTQ Crucial Weights Selection, Block-based\\nerror compensation\\n1-Bit quantization\\nLQER [437] Flexible PTQ Quantization Error MinimizationBetter compression-accuracy trade-offs\\nI-LLM [147] Flexible PTQ Fully Smooth Block Reconstruction,\\nDynamic Integer-only MatMul and\\nInteger-only Non-linear Operators\\nInteger-only Quantization\\nPV-Tuning [244] 1-Bit/2-bit PTQ PV algorithm Better compression-accuracy trade-offs.\\nBitNet [363] 1-Bit QAT 1-Bit Transformer Architecture 1-Bit quantization\\nBitNet b1.58 [236] −1, 0, 1 QAT Ternary Parameters 1-Bit quantization\\nPEQA [171] Flexible QAT Quantization Scales OptimizationParameter-Efficient Fine-tuning\\nQLoRA [84] NF4 QAT 4-Bit Normal Float and Double\\nQuantization\\nParameter-Efficient Fine-tuning\\nthat 3–4\\u2009bit quantization strikes an effective balance between compression, inference efficiency,\\nand accuracy, making it a practical solution for deploying LLMs in resource-constrained settings.\\n2.3.4 Low-Rank Techniques. Low-rank techniques compress LLMs by approximating a high-\\ndimensional weight matrix with two lower-dimensional matrices, reducing computational and\\nmemory requirements. A matrixW ∈ R<×= is approximated asW ≈ A × B, whereA ∈ R<×A and\\nB ∈ RA×=, withA much smaller than<or=, reducing the number of parameters. Building on this\\nconcept, Ji et al. [161] propose a low-rank method tailored for LLMs, leveraging the observation that\\nwhile LLMs have high-rank weights, their feature interactions tend to exhibit low-rank properties.\\nThe method estimates feature distributions using pooled covariance matrices and allocates distinct\\ncompression ratios to layers based on their sensitivity to low-rank compression. A Bayesian\\noptimization strategy, using a Gaussian process as the surrogate model, optimizes the allocation\\nof low-rank dimensions, ensuring the model maintains performance while achieving significant\\ncompression.Transitioningfrommodelcompressiontofine-tuning,Choetal.[ 64]tacklesystemand\\ndataheterogeneitywiththeHETLORAmethod,whichusesheterogeneouslow-rankapproximations\\nto accommodate the diverse capabilities of clients and data complexities. By combining local rank\\nself-pruning with sparsity-weighted aggregation, it balances high- and low-rank LoRA modules,\\nimproving convergence speed and performance compared to uniform approaches. LLM-Neo [414]\\ncombines KD with low-rank adaptation (LoRA) to improve the efficiency of transferring knowledge\\nfrom a teacher LLM to a compact student model.\\n3 Advanced Enhancement Strategies for SLMs\\nWith the foundational concepts introduced in Section2, this section explores various advanced\\ntechniques that enhance the performance of SLMs, including innovative training methods for\\ntraining SLMs from scratch,supervised fine-tuning (SFT)to align SLMs to adhere to instructions,\\nadvancedKDandquantizationtechniques,andtechniquesfrequentlyusedinLLMs,suchasMoEsto\\nenhance SLMs for specific applications. A summary of enhancement techniques is also summarized\\nin Table3.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 15, 'page_label': '16'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:15\\nTable 2. Summary of Quantization Methods with Evaluation Details\\nMethod Evaluated\\nBase Models\\nBenchmarks Full-Precision vs\\nQuantized Perfor-\\nmance\\nCompres-\\nsion\\nSpeed up\\nSqueezeLLM [173] LLaMA-2\\n7/13/70B\\nWikiText2 PPL\\n[249]; C4 [291]\\nFP16PPL5.52(C4);3-\\nbit PPL 5.73 (C4)\\n∼5.3×\\nsmaller\\nUp to∼2.3×\\nfaster\\nJSQ [124] LLaMA-\\n7B/13B/33B;\\nLLaMA2-7B\\nPIQA [33]; BoolQ\\n[67];MMLU[136]\\nSimilar acc @ 50%\\nsparsity\\u2009+\\u20094b quant.;\\n<2% avg. drop\\nUp to ∼8×\\n(sparsity+\\nquant.)\\n–\\nFrameQuant [4] LLaMA-\\n7B/13B\\nWikiText2 PPL;\\nC4\\n<0.5\\u2009pt avg. drop vs\\n4\\u2009b GPTQ\\n4–8× Faster than\\nGPTQ (same\\nprecision)\\nOneBit [405] LLaMA-\\n7B/13B;\\nOPT-2.7B\\nWikiText2 PPL;\\nC4; LM-Eval\\nsubset\\n≥81% of FP16 avg.;\\nsimilar in few-shot\\n16× –\\nBiLLM [150] LLaMA fam-\\nily\\nWikiText2 PPL;\\nC4; LM-Eval\\nsubset\\n∼2% PPL increase 8–16× –\\nLQER [437] LLaMA and\\nOPT families\\nWikiText2 PPL 0.3% accuracy drop4× (W4) to\\n5.3×(W3)\\n–\\nI-LLM [147] LLaMA fami-\\nlies\\nWikiText2 PPL;\\nC4\\n<1pt drop at 8\\u2009b int-\\nonly; modest at 4\\u2009b\\n2–4× Integer mat-\\nmul accel.\\nBitNet [363] BitNet\\n(1.3B–6.7B)\\nWikiText2 PPL;\\nZero/Few-shot\\nΔavg≲2ptszero-shot\\nvs FP16\\n16× 1.5–3×\\nBitNet b1.58 [236] BitNet b1.58\\n(3B+)\\nWikiText2 PPL Matches FP 10.1× –\\nPEQA [171] LLaMA fami-\\nlies\\nLM-Eval subset Recoversorimproves\\nvs FP<4b\\n4–8× Fine-tuning\\nmemory\\n↓10–20×\\nQLoRA [84] LLaMA fami-\\nlies\\nMT-Bench;\\nMMLU; custom\\ndata\\n<1%dropvsFPLoRA 4× –\\n3.1 Innovative Training Methods for SLMs from Scratch\\nInscenarioswithlimitedresources,weaimtotrainSLMstoprovideefficient,cost-effectivesolutions\\ntailored for specific domains while still maintaining competitive performance with larger models.\\nTrainingSLMsfromscratchinvolvesuniquestrategiesthatdivergesignificantlyfromthoseusedfor\\nLLMs.Thissectionsynthesizescutting-edgetechniquestailoredtooptimizetheinherentcapabilities\\nof SLMs, underscoring their potential to match or surpass larger counterparts in efficiency and\\neffectiveness. As shown in Figure10, the methods for training SLMs from scratch can be categorized\\ninto three primary categories:Architecture Design, Data Construction, andOptimization Strategy.\\nNext, we introduce each category in detail.\\nArchitecture Design for SLMs. When designing SLM architectures, parameter-sharing techniques\\nare employed to minimize space usage and reduce the model’s size. As shown in the first part of\\nFigure 10, parameter sharing is achieved by two approaches: (i) a single Feed-Forward Network\\n(FFN) module is shared by every transformer layer. As shown in Figure10(1) middle,FFN layer\\nsharing/reusing can maintain a smaller size while still benefiting from the depth and complexity\\ngained through repeated processing of input data. This technique is firstly applied in MobiLlama\\n[342] which surpasses the performance of existing SLMs of comparable size. (ii) Entire transformer\\nblocks are shared. As shown in Figure10(1) right, Transformer Block-wise Sharing is another\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 16, 'page_label': '17'}, page_content='145:16 F. Wang et al.\\nTable 3. Advanced Enhancement Methods for SLM\\nTopic Method Main Contribution\\nTraining\\nfrom\\nScratch\\nMindLLM [417] Bilingual models with advanced features.\\nMobiLlama [342] On-device SLM with dual objectives for efficiency and capability.\\nMobileLLM [227] Optimizes LLM deployment on mobile with advanced architecture.\\nSFT\\nMobileBERT [328] Compact BERT for efficient fine-tuning.\\nAlpaca 7B [335] Uses ChatGPT-generated tasks to tune Llama 7B.\\nRLHF [271] Trains using human-preferred data and reinforcement learning.\\nDPO [290] Dynamically adjusts log probabilities to prevent model degradation.\\nData\\nQuality\\nin KD\\nTinyStory [98] Enhances narrative coherence in child-friendly datasets.\\nAS-ES [389] Improves CoT by categorizing reasoning steps.\\nSelf-Amplify [30] Automates CoT data annotation for small models.\\nDistillation\\nfor SLM\\nGKD [6] Aligns training and inference distributions using on-policy sequences.\\nDistiLLM [176] Uses skew KL divergence and adaptive off-policy for output utilization.\\nAdapt-and-Distill [419] Domain adapts both teacher and student models before distillation.\\nQuantization\\nSmoothQuant [391] Balances quantization difficulty using per-channel scaling.\\nBiLLM [150] Applies Hessian-based metrics for binary residual approximation.\\nLLM-QAT [226] Uses data-free KD and logit distillation for fine-tuning.\\nPB-LLM [306] Binarizes non-salient weights while preserving others in higher preci-\\nsion.\\nOneBit [405] Achieves near 1-bit quantization with minimal performance loss.\\nBitNet [363] Introduces 1-bit Transformer architecture with BitLinear layers.\\nBitNet b1.58 [236] Implements a ternary weight system in enhanced BitNet.\\nLLM techniques\\nfor SLM\\nMa et al. [239] Combines filtering and re-ranking to improve Information Extraction\\ntasks.\\nMoQE [175] Applies quantization to expert weights to outperform dense models.\\nSLM-RAG [220] Shows that SLMs with RAG can match LLM performance.\\nFig. 10. Innovative training methods for SLMs from scratch.\\nparameter-sharing approach that maintains depth and complexity. There are different transformer\\nblock-wise sharing strategies, such as repeating the transformer blocks all over again or repeating\\nthe immediate transformer block. This technique is applied in MobileLLMs [227], which have 125M\\nand 350M parameters. MobileLLMs demonstrate performance improvements of 2.7% and 4.3%,\\nrespectively, compared to previous models with equivalent parameters. Moreover, they exhibit\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 17, 'page_label': '18'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:17\\naccuracy comparable to LLaMa-2-7B on API call tasks, highlighting the capabilities of smaller\\nmodels in mobile environments.\\nData Construction. ForSLMs,theemphasisondataqualitysurpassesthatofquantityanddiversity\\n[417]. Experiments demonstrate that using a quality filtering approach to remove low-quality data\\ncan lead to improved performance in SLMs [417]. Unlike large models, which can handle diverse\\nand large datasets, SLMs benefit more from cleaner, high-quality data probably due to their limited\\ncapacity against noise. Generally, data processing has several steps: (i) Remove HTML, CSS, JS, and\\nnon-textelementsforcleantext;(ii)Filterlowtext-to-contentratiowebpages;(iii)Deduplicateusing\\nSimHash [80, 300]; (iv) Exclude sensitive/offensive content with heuristics and token replacements;\\n(v) Remove self-repeating phrases of advertisements to enhance dataset informativeness [48, 417].\\nThese steps collectively ensure that training data has high-quality, informative texts. SLMs also\\nsignificantly benefit from these techniques. For example, MindLLMs [417], which are bilingual\\nlightweight language models (available in 1.3B and 3B versions), adopt these data processing\\ntechniques and achieve improved capability acquisition.\\nTraining Strategy for SLMs. For LLMs, due to the large model size and data volume, LLMs are\\nusuallytrainedwithoneround.ForSLMs,multiple-roundtrainingcanbeapplied[ 334].Considering\\nsome examples are hard to fit, hard examples can be trained with a high probability [334]. For each\\nround of training, the data sampling probability is updated according to the overall loss of that\\nsample. Experiment results show that two rounds of training and a 50% sampling rate are a good\\ntradeoff between performance and training efficiency. Tang et al. [334] show that a deep and thin\\nneural architecture and multiple-round training can enhance the performance of the trained Pangu\\n1.5B pro model. This model outperforms the conventionally trained Pangu 1.5B and a series of\\nother comparable LLMs with similar model sizes on multiple benchmark datasets, achieving an\\naverage performance increase of 8.87%.\\n3.2 SFT for Enhancing SLM Performance\\nSFT employs a training methodology similar to pre-training but is specifically tailored to align\\nmodels to adhere to the instructions encapsulated within various instructional datasets. This\\napproach is designed to refine the model’s responsiveness and appropriateness to given contexts\\nas the training data dictates. For example, various models, such as Alpaca [335], UltraChat [89],\\nWizardLM [397], SlimOrca [207], ShareGPT [362], Capybara [75], Deita [221], VLAA-Thinking [49],\\nGALLM [234], and MetaMathQA [425], incorporate a suite of conversational datasets to enhance\\ntheir capabilities in context-aware dialogue and instruction adherence. Usually, as shown in Figure\\n11, existing SFT methods can be categorized into three categories:\\n(i) Classical fine-tuning with downstream data [86, 289] trains SLMs on task-specific annotated\\ndata, transferring general language representations to specific tasks such as sentiment\\nanalysis. In the LLM era, this approach remains effective, such as enhancing LLMs by\\ncalibrating responses or assigning risk scores with smaller models such as BERT [454], or\\noptimizing for mobile devices with MobileBERT [328].\\n(ii) Instruction tuning withLLM-generateddata[ 89,207,335]orhuman-generatedquestionswith\\nLLM annotations [362] aims to align generative models with specific instructions, enhancing\\ntheir instruction-following and reasoning capabilities. For example,Alpaca 7B [335] uses 52k\\nChatGPT-generated instruction-following examples from 175 self-instructed seed tasks to\\ntune Llama 7B [344]. Meanwhile, StableLM [27, 346] is trained on the Restruct-v1 dataset,\\nwhich includes summarization,question-answering (QA), and sentiment analysis tasks,\\nusing instruction data from [230].\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 18, 'page_label': '19'}, page_content='145:18 F. Wang et al.\\nFig. 11. Fine-tuning for enhancing SLMs.\\n(iii) Preference optimization with human feedback [271, 290, 362] aims to better align language\\nmodels with human preferences. Reinforcement Learning from Human Feedback (RLHF)\\n[271] gathers human-preferred data, trains a reward model, and fine-tunes the LM using\\nreinforcement learning. DPO [290] provides a simpler alternative to RLHF. Unlike RLHF,\\nDPO avoids explicit reward modeling and reinforcement learning techniques. Instead, it\\nadjusts the log probabilities of preferred versus non-preferred responses using a dynamic\\nweighting mechanism, preventing model degradation issues typical of methods relying on\\nprobability ratios. For instance, Llama 3.2 1B and 3B apply SFT and DPO in post-training to\\nenhance alignment with instructions and human preferences.\\n3.3 Data Quality in KD\\nTransitioning from the discussion on training SLMs from scratch, this section delves into the critical\\nrole of data quality in KD. The motivation here is to highlight how high-quality data generated\\nfrom LLMs can significantly enhance the learning efficiency and performance of SLMs. The central\\nidea is that meticulously crafted datasets, when used in KD, enable SLMs to more effectively mimic\\nthe advanced capabilities of their larger counterparts. As shown in Figure12, the data can come\\neither from (1) other strong LLMs (e.g., GPT-4 [2]), which are much larger and more powerful than\\nthe target SLM, or (2) the target SLM itself.\\nAugment Data from LLMs. LLM-generated data could be categorized aspre-training data and\\nfine-tuning data. Firstly, due to the limitations of model size, studies have shown that training SLMs\\nrequires simple and comprehensible data [98, 186, 190, 389]. As shown in Figure12(1) left,TinyStory\\n[98] shows that small models (tens of millions of parameters) can generate coherent stories for\\n3-4-year-olds. GPT-3.5 or GPT-4 [2] prompts create simple stories from three keywords chosen\\nfrom a 1,500-word vocabulary, which are then used to train SLMs for similar outputs. This approach\\nshows that simple and comprehensible data can help smaller models exhibit behaviors similar to\\nthoseoflargerlanguagemodels,suchasobeyingscalinglawsandachievingenhancedperformance.\\nOn the other hand, many efforts to enhance theChain-of-Thought (CoT)capabilities of small\\nmodels involve using LLMs to generate high-quality fine-tuning CoT data. As shown in Figure\\n12 (1) right, these data train small models end-to-end to mimic CoT reasoning [240, 389]. AS-ES\\nLearning [389] highlights that small models struggle with complex reasoning, even when provided\\ndetailed steps, as these require nuanced extraction and abstraction. Therefore, the study introduces\\na paradigm that splits reasoning into extractive segments (context reminders) and abstractive\\nsegments (inferred insights).\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 19, 'page_label': '20'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:19\\nFig. 12. Data quality in KD.\\nAugment Data from Itself. Besides distilling data from other LLMs, language models can also\\ntrain on their own outputs [30, 148, 343]. Since voting strategies can improve the performance of\\nLLMs, reasoning paths that lead to the majority answer can be further utilized to fine-tune LLMs\\n[148]. Similarly, SLMs can generate their training data with the aid of existing rationale generation\\nmethods. Self-Amplify [30] notes that human annotation of CoT data is very time-consuming;\\nthus, automated rationale generation methods have been proposed. These methods involve three\\nmain steps: (1) Selection of samples(G,~) that the model predicts correctly as few-shot examples;\\n(2) Rationale generation, where rationales are produced using post hoc explanation methods; (3)\\nPrompt design for SLMs, where the final prompt is crafted based on the previously generated\\nrationales.\\n3.4 Distillation Techniques for Enhancing SLM Performance\\nFollowing the discussion on data quality in KD, this section reviews specialized KD training\\nstrategies designed to enhance the performance of SLMs. The motivation is to address the unique\\nchallenges and constraints involved in distilling knowledge from LLMs to SLMs, ensuring that\\nthe smaller models can maximize their performance gains. As shown in Figure13, two main gaps\\nbetween LLMs and SLMs lead to challenges in distillation:distribution mismatch and domain gap.\\nDistribution mismatch [6, 176] occurs when the distribution of output sequences during training\\ndoes not align with the distribution of sequences that SLMs produce during inference, leading\\nto suboptimal performance of the student model. Thedomain gap [419] arises when there is a\\ndiscrepancy between the domains or tasks on which the LLMs and SLMs are trained and applied.\\nThis gap can cause significant degradation in the performance of the student model if not properly\\naddressed during the distillation process. To address these issues, specialized strategies involve first\\naligning the teacher and student models with the target domain before proceeding with knowledge\\ndistillation. To explore these challenges further, we now delve into the details of these two branches\\nof methods.\\nDistribution Mismatch. In original KD illustrated in Figure13 Distribution Mismatch (a), the\\nteacher and student are provided with the same input sequencesG and output labels~, producing\\nprobability distributions for the next token (@and?). The loss is calculated as the difference between\\nthese two distributions,\\x19(@,?). However, a key challenge arises due to distribution mismatch: the\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 20, 'page_label': '21'}, page_content='145:20 F. Wang et al.\\nFig. 13. Distillation techniques for enhancing SLM performance. On-policy means learning only uses data\\nfrom the current student (policy), while off-policy permits the use of previously gathered data.\\noutput sequences during training (~) differ in distribution from those the SLMs produce during\\ninference (~′). To address this challenge, various techniques have been proposed. As shown in\\nFigure 13 Distribution Mismatch (b), one approach trains the student model using on-policy\\nsequences—sequences generated by the student itself—guided by the teacher model’s feedback.\\nSpecifically, both the student and teacher take the same input (G) and the student-generated output\\n(~′), producing probability distributions for the next token (@ and ?, respectively). The loss is\\ncalculated as the difference between these two distributions,\\x19(@,?). This approach helps the\\nstudent model reduce the distribution gap between training and inference by learning from the\\nteacher’s feedback on its own generated sequences.GKD [6] is the first work using this technique\\nand improves distillation outcomes. However, a drawback of this technique is that it requires the\\nstudent to constantly produce new training sequences, which can be computationally expensive.\\nTo improve efficiency, as shown in Figure13 Distribution Mismatch (c), an adaptive off-policy\\napproach can be used to efficiently manage student-generated outputs by storing them in a replay\\nbuffer, thereby reducing computational costs.DistiLLM [176] employs this off-policy approach\\nand improves the efficiency of KD. While it has approximately 1.5× the training time of vanilla\\ndistillation methods, it achieves up to a 2.3× speedup compared to on-policy approaches like GKD.\\nIn addition to improved efficiency, DistiLLM boosts ROUGE-L scores of distilled SLMs across five\\ninstruction-following benchmarks, outperforming both GKD and vanilla distillation.\\nDomain Gap. When training an SLM in a specific domain that differs from the domain of the\\nLLMs, the gap between the two domains becomes problematic. As illustrated in Figure13 Domain\\nGap (a), domain adaptation fine-tunes a language model, initially trained on a general corpus, using\\na specialized dataset such as PubMed to enhance performance in that specific domain. As illustrated\\nin Figure13 Domain Gap (b), KD transfers knowledge from the larger model to the smaller one.\\nHowever, because the teacher model may not produce high-quality outputs on specialized datasets,\\ndomain adaptation is needed before KD. As illustrated in Figure13 Domain Gap (c),Adapt-and-\\nDistill [419] tackles the domain gap by distilling general large models into smaller ones. This article\\nintroduces AdaLM and demonstrates that the “Adapt-and-Distill” strategy—first involving domain\\nadaptation of both the large teacher model and the small student model, followed by distillation—is\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 21, 'page_label': '22'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:21\\nthe most effective compared to three other strategies: training directly from scratch, distillation\\nfollowed by adaptation, and adapting the teacher model before distillation into a general small\\nstudent model. These innovative techniques are crucial for enhancing the capabilities of SLMs,\\nmaking them more efficient and effective for various applications. However, adapting both the\\nteacher (LLMs) and the student (SLMs) models to the target domain can be time-consuming. Future\\nresearch could focus on efficiently solving the domain gap problem. In experimental evaluations,\\nAdapt-and-Distill produces a 6-layer student model that is 3.3× smaller and 5.1× faster than the\\nBERTbasemodel,whileconsistentlyoutperformingBERTonbothbiomedicalandcomputer-science\\ndownstream tasks.\\n3.5 Performance Improvement through Quantization\\nAs mentioned in Section2, quantization is one of the most effective methods for adapting LLMs\\nto SLMs. However, compression to smaller sizes often compromises performance. To address the\\nperformance drop associated with quantization, various methods have been proposed. This section\\nexamines how these quantization methods specifically enhance the performance of SLMs. While\\nthe general introduction to compression methods is discussed in the compression section, the\\nfocus here is on detailing those approaches that boost the efficiency and effectiveness of SLMs.\\nAs shown in Figure9, we categorize these quantization methods into two main approaches: PTQ,\\nwhere quantization is conducted on a well-trained fixed model, and QAT, where quantization is\\nintegrated into the training process. This section introduces advanced techniques in PTQ and QAT,\\nrespectively.\\nPTQ primarily includes weight quantization and activation quantization. Weight quantization\\naims to quantize model parameters while preserving performance.GPTQ [105] compresses LLMs\\nto 4-bit or 2-bit by quantizing weights layer-by-layer to minimize layer-wise quantization errors.\\nGPTQ quantizes OPT-175B and BLOOM-176B in approximately four GPU hours with a negligible\\nincrease in perplexity, and the resulting 3-bit OPT-175B model achieves up to a 3.25× speedup\\non a single NVIDIA A100 GPU and 4.5× on two NVIDIA A6000 GPUs, while fitting the model\\ninto a single 80GB A100.PB-LLM [306], applicable to both PTQ and QAT, retains the most salient\\nweights while binarizing the rest based on magnitudes.BiLLM [150], another PTQ method, uses a\\nHessian-based metric to identify salient and non-salient weights. Salient weights undergo binary\\nresidual approximation to minimize loss, while non-salient weights are divided into sparse and\\nconcentrated groups for separate binarization, reducing quantization errors. Activation quantiza-\\ntion faces challenges with outliers that can stretch the quantization range, causing most values\\nto cluster at a few bits and introducing significant errors. To address this,LLM.int8() [83] isolates\\noutlier features for 16-bit processing and handles the rest in 8-bit.SmoothQuant [391] circumvents\\nper-channel quantization issues by employing a “smoothing” technique that shifts the quantization\\nchallenge from activations to weights through a per-channel scaling transformation. This balance\\nbetween activating and weight quantization allows effective 8-bit quantization (W8A8), preserving\\naccuracy while significantly reducing memory and computational costs. SmoothQuant thus en-\\nhances the efficiency of SLMs in resource-constrained environments. SmoothQuant achieves up to\\n1.56× inference speedup and 2× memory reduction on LLMs with negligible accuracy loss.\\nQAT differs from PTQ in that it includes a training phase after the model has been quantized.\\nWhen models are quantized to extremes, such as 2-bit or 1-bit, performance typically drops sig-\\nnificantly, but further training can help the model retain its capabilities. For instance, to mitigate\\nperformance degradation from binarization,PB-LLM [306] selectively binarizes only non-salient\\nweights, preserving the most salient ones at higher precision. This method effectively reduces the\\nmodel size without significantly impacting performance. Salient weights are chosen based on their\\nmagnitude, ensuring that the most influential weights maintain higher precision to preserve the\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 22, 'page_label': '23'}, page_content='145:22 F. Wang et al.\\nmodel’s reasoning capabilities. The article explores both PTQ and QAT to fine-tune and recover the\\nperformance of partially binarized models, achieving a balance between compression and accuracy.\\nOneBit [405] andBitNet [363] address the severe performance degradation associated with 1-bit\\nquantization by decomposing floating-point matrices and employing mixed-precision strategies.\\nSpecifically, OneBit introduces Sign-Value-Independent Decomposition (SVID), which decomposes\\na floating-point matrix into a 1-bit matrix and two floating-point vectors. This method allows\\nLLMs to be quantized to a 1-bit level while minimizing performance loss. By retaining critical\\ninformation with the floating-point vectors, OneBit effectively balances extreme compression with\\nmaintaining model accuracy.BitNet b1.58 [236] improves on the original BitNet by introducing a\\nternary matrix weight system of−1, 0, 1, resulting in a 1.58-bit model. BitNet b1.58 matches the\\nperformance of full-precision models starting from a 3 billion parameter size while further reducing\\nmemory and latency costs.LLM-QAT [226] employs data-free KD where the pre-trained model\\nitself generates data for fine-tuning the quantized model (student) using logit distillation from the\\nfull-precision model (teacher). This method incorporates quantization of weights, activations, and\\nkey-value cache, achieving accurate 4-bit quantization for weights and key-value caches and 6-bit\\nfor activations, demonstrating substantial improvements over existing PTQ methods.\\n3.6 Techniques in LLMs Contributing to SLMs\\nThissubsectionexploresthepotentialofadvancedtechniquessuchasRAGandMoE,whichenhance\\nLLM performance, to also maintain or boost SLM performance within constrained computational\\nbudgets. However, effectively integrating these techniques into SLMs, which inherently possess\\nlimited capabilities, remains an unresolved challenge.\\nRetrieval Augmented Generation (RAG)enhances the capabilities of language models in\\nknowledge-intensive tasks by incorporating a retrieval mechanism. This approach allows models\\nto access relevant contextual information from a data repository in response to user queries. By\\nintegrating this retrieved data, RAG-equipped models better understand specific topics, enabling\\nmore informed and accurate outputs. For SLMs, a significant concern is whether they possess the\\ncapacity for long-context reasoning. A recent study [220] compares SLMs at the 7B level with\\nRAG to larger models such as GPT-3.5 and GPT-4, suggesting that SLMs equipped with RAG can\\nsometimes perform comparably or even better than LLMs. These findings indicate that RAG for\\nSLMs is effective and represents a promising direction for future research.\\nMoE [40] has emerged as an effective method for substantially scaling up model capacity with\\nminimal computation overhead in LLMs. The MoE framework is founded on a straightforward\\nyet potent concept: distinct components of a model, referred to as “experts,” specialize in different\\ntasks or data facets. In this paradigm, only the relevant experts are activated for a specific input,\\nwhich manages computational costs while leveraging a vast pool of specialized knowledge. This\\nscalableandadaptableapproachenablesincreasedmodelcapacitywithoutproportionallyescalating\\ncomputational demands. We argue that MoE is particularly suitable for SLM architectures [164]\\nas it minimizes both computational load and memory overhead. However, research on MoE for\\nSLMs remains sparse. Future studies could investigate how large LLM MoE architectures can be\\neffectively compressed into small ones or how to develop an SLM with MoE tailored for specific\\ndevices from scratch.\\n4 Applications of SLMs\\nIn this section, we delve into the applications of SLMs across various NLP tasks and their deploy-\\nment strategies. Due to benefits such as enhanced privacy, faster inference, and lower memory\\nrequirements, many NLP applications are now leveraging SLMs over LLMs. Additionally, deploy-\\ning SLMs often involves considerations of memory and runtime efficiency, which are crucial for\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 23, 'page_label': '24'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:23\\nTable 4. Task-Specific SLM Applications\\nAspect Representative WorkKey Point\\nSLM in QA\\nAlpaca [335] Tune Llama 7B [344] using 52k ChatGPT-generated examples.\\nStable Beluga 7B [243] Employ explanation tuning to Llama-2 7B [345] on an Orca-style dataset.\\nFine-tuned BioGPT [127] Fine-tuning BioGPT (1.6B) [235] on PubMedQA.\\nFinancial SLMs [282] Transfer financial knowledge from GPT-4 [2] to multiple SLMs.\\nColBERT [114] Fetch retrieval documents for SLMs to answer domain-specific questions.\\nRationale Ranking [130] For unseen questions, combine retrieval with LLM-generated rationales.\\nT-SAS [159] Enhance SLMs adaptability with self-generated pseudo labels.\\nSLM in\\nCoding\\nPhi-3.5-mini [1] New addition to the Phi-3 series and focus on high-quality data.\\nTinyLlama [444] 1.1B Transformer model is trained on 3\\u2009T corpus.\\nCodeLlama [299] A derivative of Llama 2 fine-tuned on domain-specific datasets.\\nCodeGemma [337] Fine-tuning Gemma to enhance coding capabilities.\\nSLM in\\nRecommendation\\nPromptRec [387] Training on prompt templates.\\nSLIM [375] Step-by-step KD\\nBiLLP [316] LLaMa-2-7B as planner and reflector.\\nONCE [219] LLaMa-2-7B as Content Encoder.\\nRecLoRA [459] Personalized low-rank adaptation.\\nSLM in\\nWeb Search\\nContent encoder [45,155,232] Encode concatenated queries and documents.\\nRanker [65,267] Re-rank retrieved documents using a special SLM.\\nRewriter [238] Bridge the gap between queries and needed knowledge by rewriting inputs.\\nSLM in\\nMobile-\\ndevice\\nOctopus [54] Calling software APIs via learning in documents.\\nMobileAgent [90] Standard Operating Procedure (SOP).\\nU-UMI [314] SLMs serve as Multi-agents in tool uses.\\nMobile Interaction [43] Text-to-action control and tests on 6\\xa0GB and 4\\xa0GB Android devices.\\nAutoDroid [381] Interaction based on GUI and APP knowledge injection.\\nM4 [428] A foundation model handling all mobile AI tasks.\\nAgent for Text Rewriting [462] Data KD from LLMs.\\noptimizing resource use on budget-constrained edge devices, particularly mobile phones. Then, we\\nwill discuss task-specific applications of SLMs and their deployment methods on mobile and edge\\ndevices.\\n4.1 Task-Specific SLM Applications\\nThis subsection explores the diverse NLP tasks to which SLMs can contribute. QA and coding\\nrepresent generative tasks, while recommender systems and web search (though not strictly within\\nthe NLP domain) typically leverage the encoding capabilities of SLMs. Additionally, the application\\nof SLMs on mobile devices is particularly well-suited due to constraints in memory and computing\\nresources. The representative works are systematically organized in Table4.\\n4.1.1 SLM Applications in QA. QA is a fundamental task in the NLP field, demanding language\\nmodels to exhibit abilities in understanding language, reasoning, common sense, and recalling\\nspecializedknowledge.Typically,largerlanguagemodelsyieldbetterQAperformance.However,the\\nsubstantialsizeofthesemodelsintroduceschallengessuchasimmensecomputationalrequirements,\\nprivacy concerns when using proprietary LLMs, and difficulties in customization. These issues\\nlead researchers and developers to favor SLMs in scenarios that demand efficiency, privacy, and\\ncustomization.Therefore,weexploremethodstoenhancethecapabilitiesofSLMsinQAacrossthree\\nkey areas: (i) Instruction Tuning of Generic SLMs for QA, (ii) Instruction Tuning of Domain-Specific\\nSLMs for QA, and (iii) Enhancing SLMs for Out-of-Domain Questions.\\nInstruction Tuning Generic SLMs for QA. Despite the Phi series’ high QA capability, its training\\ncost with over 3.4\\u2009T tokens on 512 H100 GPUs for 10\\u2009days [1] is prohibitive for many researchers\\nand developers. Instruction tuning [377] offers a cost-effective alternative, enhancing small models\\nby fine-tuning on large model outputs. Alpaca 7B [335] tunes Llama 7B [344] with 52k ChatGPT-\\ngenerated examples from 175 seed tasks. This behavior cloning mimics teacher models effectively\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 24, 'page_label': '25'}, page_content='145:24 F. Wang et al.\\nTable 5. Comparison of Instruction-Tuned Domain SLMs for QA and LLMs on FinQA [ 58] and\\nPubMedQA [166]\\nModel Size Instruction Tuned?Task Name Shot Type Accuracy (%)\\nGPT-4 [2] - × FinQA Zero-shot 77.5\\nPhi-3-Mini [1] 2.7B ✓ FinQA Zero-shot 77.6\\nMeditron-70B [57] 70B × PubMedQA Zero-shot 81.6\\nRankRAG-llama3-70B [426] 70B × PubMedQA Zero-shot 79.8\\nFlan-PaLM [320] 540B × PubMedQA Few-shot 79.0\\nGAL 120B [336] 120B × PubMedQA Zero-shot 77.6\\nFlan-PaLM [320] 62B × PubMedQA Few-shot 77.2\\nBioGPT [235] 345M ✓ PubMedQA Zero-shot 78.2\\nBioGPT-Large [235] 1.5B ✓ PubMedQA Zero-shot 81.0\\nbut struggles in reasoning-intensive QA tasks where accuracy is key, not style [62]. To counter it,\\nexplanation tuning [243] enhances Llama-2 7B [345] using explanatory LLM answers to improve\\nreasoning. However, its effectiveness varies with system instructions, and those effective for larger\\nmodels like GPT-4 may not suit smaller ones. SLMs also struggle to identify optimal system instruc-\\ntions for different tasks. Therefore, Orca 2 [253] addresses this by promoting cautious reasoning,\\ndeciding which solution strategy to choose for a given task among direct answer generation, or\\n“Slow Thinking” strategies (step-by-step, guess and check or explain-then-answer, etc.) and erasing\\nspecific system instructions during training. This involves (1) a solution strategy guided by the\\nperformance of Orca 1 [258], (2) writing task-specific system instructions corresponding to the\\nchosen strategy to obtain teacher responses for each task, and (3) at training time, employing\\nPrompt Erasing to replace the student’s system instructions with generic ones vacated of details of\\nhow to approach the task, encouraging students to learn not just task solutions but also deeper\\nreasoning abilities.\\nInstruction Tuning Domain SLMs for QA. Beyond instruction tuning for generic SLMs, tuning\\ndomain-specific SLMs is also crucial, as they provide specialized assistance where generic SLMs\\nmay underperform. Instruction-tuning generic SLMs can derive domain SLMs. We summarize some\\nrepresentatives in several domains. (1) In finance, Phogat et al. [282] transfer financial QA abilities\\nfromteacherLLMssuchasGPT-4[ 2]tospecializedSLMssuchasPhi-3-Mini[ 1],usingdatasetssuch\\nas FinQA [58], ConvFinQA [59], and TATQA [458]. They train SLMs with Python programs created\\nby the teacher model, which detail steps for financial reasoning, including concept comprehension,\\nformula identification, entity extraction, and calculations. During inference, SLMs generate Python\\ncode that an external interpreter executes. (2) In the medical field, Guo et al. [127] enhance student\\nSLMs, including domain-specific BioGPT (1.6B) [235] and general Llama 7B [344], by fine-tuning\\non enriched PubMedQA [166] data. This enhancement is achieved by generating new samples or\\nrewriting existing ones using teacher LLMs, which include the highly knowledgeable GPT-4 and\\nthe relatively weaker ChatGPT. The best SLM, with under 1.6 billion parameters, achieves 75.4%\\naccuracy, surpassing GPT-4’s 74.4% in few-shot settings on the PubmedQA test sets. It demonstrates\\nthat LLMs effectively refine and diversify question-answer pairs, leading to enhanced performance\\nin a significantly smaller model after fine-tuning. We report the detailed results of comparisons of\\ninstruction-tuned domain-specific language models for QA and larger language models on FinQA\\n[58] and PubMedQA [166], as shown in Table5.\\nEnhancing SLMs for Out-of-Domain Questions. OneofthemajoradvantagesofLLMsistheirstrong\\ncomprehension and logical reasoning abilities, which SLMs often struggle to match due to their\\nlimited parameters, especially when handling unseen or out-of-domain questions. Various methods\\nhave been developed to address this limitation, including RAG and self-adaptive techniques.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 25, 'page_label': '26'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:25\\n(1) RAG: Incorporating External Knowledge for Domain-Specific QA.RAGaddressesOODquestions\\nby integrating external knowledge during inference, allowing models to access information\\nbeyond their pre-trained parameters. By retrieving relevant documents in real time, RAG\\nenables SLMs to provide accurate answers on specialized topics. In the telecommunications\\ndomain, Gichamba et al. [114] use ColBERT as a dense retrieval system to fetch documents\\nfrom technical datasets. By encoding queries and documents separately, ColBERT computes\\nrelevance scores, helping small models like Phi-2 and Falcon-7B retrieve precise technical\\ninformation to answer complex telecom-related queries.Rationale Ranking [130] addresses\\nanswering unseen questions using smaller language models by integrating external explana-\\ntory contexts from retrieval systems with reasoning rationales from LLMs. This method\\ninvolvesrankingboththeretrievedexplanatorycontextsandLLM-generatedrationalesusing\\na scoring module, which then combines them to form a cohesive context. Consequently, this\\nintegrated approach enhances the SLMs’ performance on unseen questions.\\n(2) Self-Adaptive Techniques: Enhancing Model Adaptability with Self-Generated Pseudo Labels.\\nFine-tuning, while effective in adapting domain knowledge, can be impractical in realistic\\nscenarios where labeled datasets are scarce. To overcome this, self-adaptive techniques\\nemploy self-generated pseudo labels to activate specific aspects of the target tasks, thereby\\nenhancing model adaptability [319, 353]. Test-time Self-Adaptive Small LMs (T-SAS) [159]\\nfirst stochastically generates multiple answers for an unlabeled question. The most plausible\\nanswer is then selected via majority voting to enhance pseudo-label accuracy, serving as a\\npseudo-label for training during test time.\\nComparison between LLMs and SLMs for QA.WhencomparingLLMssuchasGPT-4[ 2]orBLOOM-\\n175B [184] with fine-tuned SLMs in QA tasks, the benefits of SLMs are clear. LLMs, while versatile\\nacross multiple domains due to extensive pre-training, are computationally demanding, making\\nthem less ideal for resource-limited settings. SLMs, however, when fine-tuned for specific domains,\\noften match or exceed the performance of larger models within those specialties. The tradeoff is\\nbetween large-scale models’ generalization and small-scale models’ specialization: LLMs handle\\ndiverse domains but may need additional techniques such as knowledge injection for domain-\\nspecificqueries.Incontrast,domain-specificSLMs,thoughlessflexible,providehigheraccuracyand\\nmore relevant responses, making them ideal for edge deployments where computational resources\\nare scarce but domain precision is crucial.\\n4.1.2 SLM Applications in Coding. The adoption of SLMs for coding offers an alternative to\\nLLMs due to their lower computational needs and potential for domain-specific tuning. Despite\\nLLMs’ proficiency in code generation and programming support, SLMs are advantageous for\\ntheir faster inference, reduced operational costs, and suitability for real-time environments where\\nrapid responses are crucial. Representative works are discussed next. The Phi series [1, 158, 204]\\nshowcases SLMs’ evolution in coding tasks. For instance, Phi-1 [122], a Transformer with 1.3B\\nparameters, specializes in basic Python coding and achieves notable scores in benchmarks such as\\nHumanEval [122], which includes 164 programming problems. Subsequent models, Phi-1.5 and\\nPhi-2, have enhanced these capabilities, while Phi-3 demonstrated SLMs’ potential to rival larger\\nmodels [1]. The latest model, Phi-3.5-mini, with 3.8B parameters, excels in long-context tasks using\\nadvanced fine-tuning and optimization techniques, performing comparably to larger models such\\nas Llama-3.1-8B-instruct [96] and surpassing smaller ones like Gemma-2 [339].\\nAnother avenue of development is the fine-tuning of general-purpose SLMs for coding tasks [24,\\n123, 231, 299, 337]. For instance, CodeLlama models [299], derivatives of Llama 2 [345], undergo a\\nrigorous fine-tuning process on domain-specific datasets, enhancing their proficiency in specific\\nprogramming languages such as Python. They are trained to handle tasks such as syntax error\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 26, 'page_label': '27'}, page_content='145:26 F. Wang et al.\\ndetection, code suggestion, and infilling, where they learn to predict and complete missing parts\\nof the code. This specialized fine-tuning improves their ability to interpret and execute detailed\\nprogramming instructions, making them highly effective in real-time code editing environments\\n[299]. CodeGemma models [337], stemming from Google DeepMind’s Gemma framework, also\\nexhibit a focused approach to enhancing coding capabilities through fine-tuning. These models\\nare specifically engineered for high-performance code generation and infilling, underpinned by\\nextensive training on a vast corpus of over 500 billion to 1 trillion tokens, predominantly consisting\\nofcode.ThiscomprehensivedatasetenablesCodeGemmamodelstoexcelinmathematicalreasoning\\nand complex problem-solving within code contexts, setting new benchmarks in latency-sensitive\\napplications such as real-time IDE support and automated code reviews [337].\\nTable 6. Performance Comparison Between\\nSLMs and LLMs in Coding Benchmarks\\nModel Size HumanEval MBPP\\nDeepSeek-Coder [123] 1.3B 65.2 49.4\\nCodeGemma [337] 2B 37.8 49.2\\nGemma 2 [339] 2B 17.7 40.2\\nPhi-3.5-mini [348] 3.8B 62.8 69.6\\nDeepSeek-Coder [123] 6.7B 78.6 65.4\\nCodeGemma [337] 7B 60.4 55.2\\nLlama 3.1 [96] 8B 66.5 69.4\\nGemma 2 [339] 9B 61.0 69.3\\nGPT-3.5 Turbo - 68.0 71.2\\nDeepSeek-Coder [123] 33B 79.3 70.0\\nLlama 3.1 [96] 70B 80.5 75.4\\nLlama 3.1 [96] 405B 89.0 78.8\\nGPT-4o OpenAI [270] - 90.2 81.4\\nClaude 3.5 Sonnet [16] - 92.0 76.6\\nAll models listed are chat or instruct versions, and per-\\nformance are sourced from respective research papers\\nor technical reports [96, 123, 299, 337, 348].\\nComparison between SLMs and LLMs on Cod-\\ning. Table 6 provides a comparative analy-\\nsis of SLMs and LLMs on coding benchmarks\\nHumanEval [52] and MBPP [21]. Insights in-\\nclude: (i) Small SLMs (1.3B–3.8B Parameters)\\nlike Phi-3.5-mini [348] achieve high scores,\\ndemonstrating the efficacy of small models.\\nMid-sized SLMs (6.7B–9B Parameters), such as\\nDeepSeek-Coder 6.7B [123] and Llama 3.1 8B\\n[96], show improved performance, indicating\\nthat larger model sizes and enhanced train-\\ning contribute to better accuracy. Large models\\n(33B and above) like Llama 3.1 405B [96], GPT-\\n4o [270], and Claude 3.5 Sonnet [16] excel, sup-\\nporting the idea that bigger models generalize\\nbetter across diverse coding tasks; (ii) There’s\\na notable tradeoff between computational ef-\\nficiency and performance, with larger models\\nrequiringmoreresources,impactingtheirprac-\\ntical deployment in constrained environments; and (iii) Specialized training and fine-tuning, as\\nused in models like DeepSeek-Coder [123], are crucial for excelling in coding tasks, though such\\nmodels may not handle complex requests as effectively, highlighting the versatility of general SLMs\\nfor broader applications.\\n4.1.3 SLM Applications in Recommender Systems. Recommender systems are essential in various\\nonline services, helping to manage information overload and meet users’ personal needs. SLMs\\nenhance recommendation systems by (1) addressing the cold start problem; (2) reducing popularity\\nbias; (3) improving long-term planning; (4) serving as personalized recommenders; and (5) acting\\nas content encoders. These applications show the versatility and effectiveness of SLMs in boosting\\nperformance and personalization in recommendations. Next, we introduce the details.\\nSLM for System Cold Start Problem. Traditional recommendation systems, which utilize historical\\nuser–item interactions such as clicks, purchases, and ratings to learn representations and match\\nitems to users, fail in scenarios lacking any user–item interactions, known as the cold-start recom-\\nmendation problem, often occurring in start-up businesses [296]. Although LLMs address this with\\nin-context learning, their slow and costly inference restricts real-time use. Thus,PromptRec [387]\\nexplores using SLMs as in-context recommenders for recommendation system cold-start problems.\\nHowever, SLMs often struggle without emergent context-learning abilities. To overcome this, SLMs\\nare enhanced by pre-training on relevant corpora, using an improved C4 corpus subset [291], and\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 27, 'page_label': '28'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:27\\nby developing training prompts for different domains, enhancing cold-start performance. Results\\nshow that enhanced SLMs like BERT-mini [86], with 11.3M parameters, achieve BERT-large’s\\nperformance in cold-start scenarios, with only 17% of BERT-large’s inference time. Similarly, many\\nstudies have addressed the cold-start problem by leveraging BERT [135, 268, 446, 463]. For example,\\nADLRS [135] employs BERT to convert web-crawled item profiles into vectors that highlight key\\naspects, aiding recommender systems in acquiring essential initial information.\\nSLM for Mitigating Popularity Bias. Popularity bias in recommender systems, marked by dis-\\ncrepancies between item popularity in training datasets and the real world, often stems from\\nusing closed-loop datasets with limited information. Recent LLMs leverage their broad open-world\\nknowledge to better reason about user–item interactions [211, 219], reducing this bias by providing\\nrecommenders with more extensive item details. Using the CoT prompting, LLMs decompose\\ncomplex tasks into intermediate reasoning steps, enhancing understanding of user behavior and\\ninterests. However, LLMs’ high resource demands limit their practical use. To overcome this, the\\nStep-by-step Knowledge Distillation Framework for Recommendation (SLIM) [375] distills LLM\\nreasoning capabilities into SLMs, keeping just 4% of the original parameters, transitioning from\\nChatGPT to Llama 7B [344]. SLIM uses detailed LLM templates to extract reasoning steps and\\nstreamlined templates for fine-tuning, enabling SLMs to improve recommender systems by better\\nreasoning on richer item information.\\nSLM for Long-term Planning. Traditional recommender systems focus on optimizing immediate\\nuser responses, often maximizing short-term gains but overlooking long-term engagement. This\\ncan trap users in echo chambers and filter bubbles [108, 374]. To tackle this, integrating planning\\ncapabilities into recommendations to balance immediate and long-term outcomes is vital. LMs, with\\ntheir extensive knowledge and reasoning abilities, are expected to enhance planning capabilities.\\nBiLLP [316] adopts a hierarchical learning approach with macro- and micro-learning phases. In\\nmacro-learning, a Planner and a Reflector, both as SLM instances like Llama-2-7B [345], operate;\\nthe Planner forms long-term plans using high-level experiences, while the Reflector updates plans\\nbased on past actions. Micro-learning uses an SLM-based Actor-Critic mechanism for personalized\\nplanning, with the Actor implementing plans and the Critic assessing actions for long-term benefits.\\nFig. 14. The illustration of lifelong behavior sequence\\nand personalized low-rank adaption (LoRA) for recom-\\nmendation [459].\\nSLMs as a Personalized Recommender. Gener-\\native language model-based recommender sys-\\ntems require integrating user knowledge, typi-\\ncallyachievedthroughfine-tuning.Fine-tuning\\ntechniques like LoRA [145] can incorporate ex-\\ntensive knowledge across all users by training\\nan external module with a small number of pa-\\nrametersA andB,butthisapproachoftenover-\\nlooks individual user preferences. To address\\nthis, RecLoRA [459] utilizes Vicuna-7B [63] to\\nintegrate personalized knowledge into SLM-\\ns/LLMs tailored for recommendation tasks, as\\nillustrated in Figure14. Specifically, RecLoRA\\nmaintains a set of parallel, independent LoRA\\nweights (A8,B8), allowing for the customiza-\\ntion of language model parameters to match individual user preferences more effectively.\\nSLM as a Content Encoder. Language models, particularly when deep, provide an effective starting\\npoint for fine-tuning on downstream tasks. In news recommendation systems, the representational\\ncapability of a model significantly impacts performance. Consequently, many news recommender\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 28, 'page_label': '29'}, page_content='145:28 F. Wang et al.\\nFig. 15. Roles of SLM in web search.\\nsystems now employ language models fine-tuned on specific datasets as text encoders. For ex-\\nample, Wu et al. [384] conduct pioneering work using a pre-trained language model to enhance\\nlarge-scale news recommender systems by substituting traditional news encoders with a BERT\\nmodel [86]. However, BERT may struggle to capture content as it is pre-trained on limited data.\\nTherefore, ONCE [219] proposes using Llama-2-7B [345] as an encoder to overcome the limitations\\nof BERT in content-based recommendations. Additionally, the study explores the synergistic use of\\nLLMs in recommendation systems, finding that SLMs optimized with LoRA [145] outperform the\\nrecommendation results of systems assisted by generic LLMs such as ChatGPT.\\n4.1.4 SLM Applications in Web Search. Web search systems, involving retrieval and ranking, face\\nchallenges due to the diverse web documents and search queries. Traditional keyword-matching\\nmethods often fall short because of phrasing variations and the long-tail distribution of queries and\\ncontent, complicating accurate semantic inference. Effective integration of retrieval and ranking\\nmodels is also crucial. Language models, serving as content encoders, help overcome semantic\\nchallenges through their language understanding from pre-training [65, 92, 365]. Joint training of\\nretrieval and ranking models addresses integration, with SLMs ranking retrieved documents and\\nacting as re-rankers. Additionally, SLMs serve as rewriters in scenarios requiring enhanced query\\nunderstanding. Thus, in web search, SLMs fulfill three roles: (1)content encoder, (2)ranker, and (3)\\nrewriter, as depicted in Figure15. Next, we give details.\\nSLM as a Content Encoder. Text embeddings are vector representations that encode semantic\\ninformation, widely used in retrieval; SLM-based dense retrieval utilizes pre-trained deep language\\nunderstanding to effectively tackle semantic challenges.H-ERNIE [65] employs a hierarchical\\nmodel that encodes queries and documents at multiple granularity—character, word, and phrase—to\\nimprove specificity and relevance in web search results by aggregating finer details into coarser\\nlayers, addressing issues like ambiguous queries.Implicit Interaction (\\x1e3) [92] uses BERT [86]\\nas a content encoder, generating implicit pseudo-queries from passages to enable high online\\nefficiencywithofflinecachingofpassagevectors.However,ERNIE-andBERT-stylemodelsoverlook\\nadvancements in SLMs such as context length extension [299]. Thus, Peng et al. [279] employ\\nLLaMa-7B [344] and Vicuna-7B [63] as semantic encoders for embedding retrieval, demonstrating\\nimprovedperformancethroughsoftprompttuning. CoCondenser [110]addressessensitivitytonoisy\\ndataandlargebatchrequirementsduringdenseretrievertraining.UsingtheCondenserarchitecture\\nwith Transformer blocks, the model condenses information into dense vectors effectively.\\nSLM as a Ranker. The reranking task improves the order of multiple candidates to enhance\\nretrieval quality because rerankers are more accurate than embedding retrievers.InPars (Inquisitive\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 29, 'page_label': '30'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:29\\nParrots for Search) [37]employstheT5base220M[ 291]asare-rankertoenhancetheBM25retriever\\n[298]. Initially, BM25 selects 1\\u2009K candidates, re-ranked by a fine-tuned T5 model (monoT5) adapted\\nas a binary classifier to assess document-query relevance. Training data, generated by GPT-3 [38],\\nformulatesqueriesandselectsrandomnegativeexamples.ExperimentsshowthemonoT5-enhanced\\nretriever significantly outperforms GPT-3; for example, it achieves a 0.3599 MAP score on the\\nTREC-DL 2020 dataset [72], surpassing GPT-3’s 0.3163.\\nSLM as a Rewriter. Queries to the retriever, typically just a few keywords, may reveal a knowledge\\ngap between the query and the knowledge needed for effective retrieval, thus limiting performance.\\nTo address this, the“rewrite-retrieve-read” framework [238] uses T5-large [291] to bridge the\\nknowledge gap in queries by rewriting them for more effective retrieval. This rewriter, trained via\\nreinforcement learning with downstream LLM performance as a reward, outperforms general LLM\\nrewrites. For example, it achieves a 45.97 F1 score on HotpotQA, surpassing the generic LLM’s\\n43.85 F1 score.\\n4.1.5 SLM Applications in Mobile-Device. The use of cloud-based LLMs on devices raises privacy\\nconcerns, and their large size limits real-time responses in urgent scenarios such as medical\\nemergencies. To overcome these issues, researchers are creating smaller, domain-specific models\\n(SLMs) that offer accurate results and suit mobile use. This subsection discusses SLM applications\\non mobile devices, focusing on three aspects: (1) software API calls, (2) mobile control, and (3) basic\\nNLP applications.\\nSLM for Tool Learning. Integrating LLMs with APIs enhances capabilities but incurs high training\\ncosts, prompting a shift to smaller, task-specific models that cut costs but risk errors. In response,\\nOctopus [54] uses a diverse dataset from over 30\\u2009K APIs and curriculum learning [222] to improve\\nAPI function accuracy. This method boosts API performance in models like Codellama-7b [299] and\\nGoogle’s Gemma series [338]. PhoneLM-1.5B-Call [422] is fine-tuned on DroidCall [394] datasets\\nand achieves comparable performance compared to GPT-4o-mini [269].U-UMI [314] employs SLMs\\nas planners, callers, and summarizers within multi-agent systems, outperforming a single LLM in\\ntool uses.\\nFig. 16. An example workflow for an automated execution tool [ 90].\\nThe screenshot on the left is taken from [ 90].\\nSLM for Mobile Control. LM\\nagents facilitate user-device in-\\nteractions through taps, ges-\\ntures,andtext,automatingtasks\\nand enhancing user hands-\\nfree convenience. Unlike tra-\\nditional developer-based ap-\\nproaches that require extensive\\ndeveloper effort to design inter-\\nfaces and translate commands\\ninto API calls, LMs offer scalable\\nautomation via GUI-based text\\ncontent. MobileAgent [90] inte-\\ngrates instructions and Standard\\nOperating Procedures (SOP) to\\nimprove SLMs for mobile con-\\ntrol. As shown in Figure16, it processes goals (e.g., booking a dental appointment) by analyzing\\nscreens, queries, prior actions, and UI elements, forming prompts to generate outputs and execute\\nactions (e.g., selecting text, XPath). Fine-tuning Qwen-7B [24] on AIA medical data, it outperforms\\nGPT-4 [2] on AitW [297], a key mobile benchmark, without extra inference costs. Carreira et al.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 30, 'page_label': '31'}, page_content='145:30 F. Wang et al.\\n[43] run a small offline model on mobile devices, fine-tuned with ChatGPT-3.5 data, enabling tasks\\nlike calls and web searches. RedPajama-INCITE-Chat-3B-v1 Computer [70], selected for its size\\nand chat features, uses native code and quantization, performing well on 6\\xa0GB and 4\\xa0GB Android\\ndevices.\\nFig. 17. An illustration of Vicuna-7B-powered mobile task automation\\n[43] shows a user asking to be reminded about doing laundry on Aug\\n17. The figure is taken from [ 43].\\nAutoDroid [381] improves\\nAndroid app interactions via\\nGUIautomation.Figure 17shows\\nLLM-powered task automation\\n(e.g., setting a laundry reminder\\nforAug17)infoursteps:(1)click\\n“New Event,” (2) enter “laun-\\ndry” in “Title,” (3) click “Save,”\\nand (4) finish. Using Vicuna-\\n7B and app-specific knowledge,\\nAutoDroid generates privacy-\\nfiltered prompts for tasks. On\\nitsDroidTaskbenchmark,itout-\\nperforms GPT-3.5 (34.7%) and\\nGPT-4(54.5%)with57.7%accuracy. M4 (composable mobile foundation model) [428]introducesa9.2B\\nparameter model (7.5\\xa0GB memory) for mobile AI tasks, managed by the OS and hardware. Currently\\nlimited to high-end devices, its applicability will expand with increasing mobile memory/storage.\\nThese works highlight customizing smaller, domain-specific SLMs to address memory limits while\\npreserving functionality in mobile environments.\\nSLM for Basic NLP Applications on Devices. Performing basic NLP tasks such as text rewriting\\ndirectly on the device can enable personalization while ensuring privacy. The sparse annotation\\non devices is a challenge. Qin et al. [285] utilize self-supervised data selection and synthesis for\\non-device fine-tuning, leveraging sparse annotations and limited storage effectively. This approach,\\ndemonstrated in Figure18, employs the Llama-3B model [344] and the LoRA fine-tuning method\\n[145],enhancingpersonalizationbyefficientlymanagingdatathroughmetricsincludingembedding\\nentropy and domain-specific scores. In mobile text rewriting, Zhu et al. [462] train the compact\\nPalm 2-XXS model [15] using data generated by the larger Palm 2-L to ensure user privacy and\\naccommodatedeviceconstraints.Onitsnewbenchmark,MessageRewriteEval,Palm2-XXSachieves\\na BLEU score of 34.59, outperforming LLaMa-7B (16.65) [344]. Tests on the Samsung S23 show\\nlower latency (29 tokens/s) than a 4-bit LLaMa-7B on a MacBook M1 Pro (18-22 tokens/s), proving\\nits mobile efficiency for text rewriting.\\nInsights: We draw several key insights from the development of task-specific SLMs:\\n—There is considerable potential in enhancing the efficiency and effectiveness of small models by integrating\\nself-adaptive techniques with further fine-tuning and optimization of RAG-based methods.\\n—The growing relevance of SLMs in coding highlights their cost-effectiveness and efficiency as alternatives to\\nLLMs, providing quick processing and easy fine-tuning for specialized tasks; while LLMs handle complex tasks\\nwell, SLMs, optimized and fine-tuned on specific data, are increasingly essential in resource-limited settings.\\n—SLMs significantly enhance recommendation systems due to their robust generalization, reasoning abilities, and\\nin-context learning, addressing key challenges such as cold-start problems and distribution biases. They support\\nlong-term planning, replace traditional encoders, and use parallel low-rank parameters to inject personalized\\nuser knowledge effectively.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 31, 'page_label': '32'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:31\\n—SLMs play a crucial role in web searches such as document encoding, text reordering, and query rewriting,\\noften outperforming LLMs through techniques such as SFT, soft prompt tuning, unsupervised contrastive loss,\\nand reinforcement learning, thereby enhancing adaptability and efficiency.\\n—SLMs are utilized on mobile devices primarily for privacy and memory constraints, with applications in API\\ncalls and mobile control; they are typically developed by generating data with LLMs and fine-tuning with\\nSLMs, or by using local SLMs to handle privacy with LLMs boosting performance, and their training involves\\ninnovative techniques like learning from data streams and managing non-IID time series data.\\n4.2 SLM Deployment on Mobile and Edge Devices\\nFig. 18. Overview of fine-tuning SLMs with synthesized and user\\ndata [285]. Data synthesis generates semantically similar text via\\nprompts, creating dialogue sets. Data selection processes user\\ndata, tags domains, and calculates metrics (EOE, DSS, IDD). Se-\\nlected data fine-tunes SLMs with user annotations. The iterative\\nframework refines SLMs through continuous data generation and\\nselection.\\nOn-deviceapplicationsbenefituniquely\\nfrom the memory-saving efficiency\\nand rapid runtime performance of\\nSLMs, which offer advantages over\\nLLMs. However, devices with ex-\\ntremely limited resources may still\\nstruggle with the parameter sizes\\nof SLMs. To ensure both memory\\nand runtime remain within accept-\\nable ranges while still maintaining\\nperformance, it is crucial to inte-\\ngrate technologies that facilitate the\\ndeployment of SLMs on resource-\\nconstrained devices. The primary\\nchallenge for memory-efficient tech-\\nnologies arises from the inherent size\\nof the SLMs and their associated\\ncaches.Toaddressthis,wesurveyex-\\nisting works focused on compressing\\nSLMs and their caches. Additionally,\\nthe large size of models significantly\\nimpacts runtime efficiency due to the\\nextensive computing workload and potential weight transfers between the memory buffer and\\nRAM/GPU memory. Other challenges include switching the Mixture of Experts between CPU\\nand GPU memory and managing resource scheduling when deploying SLMs across multiple local\\ndevices. Therefore, in this subsection, we review representative works that address these chal-\\nlenges under two aspects:memory efficiency optimization and runtime efficiency optimization, as\\nsystematically compiled in Table7.\\n4.2.1 Memory Efficiency Optimization. Memory efficiency involves minimizing the memory\\nusage of both the model and the KV cache during deployment. This is typically achieved through\\nmodel compression techniques such as quantization [212, 292, 427, 452], the cache of MoE experts\\n[421], and KV cache compression [168].\\nCompression on model parameters. Quantization, a common method for deploying SLMs, lowers\\nnumerical precision to significantly save memory while preserving accuracy. We detail quantization\\nstrategies in Sections2.3.3 and 3.5, focusing here on representative works for edge devices.EDGE-\\nLLM [427] adapts LLMs for edge devices using a Layer-wise Unified Compression (LUC) method\\nthat combines layer-specific pruning and quantization to reduce computational demands and an\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 32, 'page_label': '33'}, page_content='145:32 F. Wang et al.\\nTable 7. On-Device Deployment Optimization Techniques\\nAspect Representative Work Key Point\\nMemory\\nEfficiency\\nOptimization\\nEDGE-LLM [427] Edge LLMs use LUC and adaptive tuning for efficiency.\\nLLM-PQ [452] Optimize quantization and layer partitioning for complex setups.\\nAWQ [212] Preserve key weights based on activation distribution.\\nMoE-I2 [409] Prune less important experts in MoE.\\nMobileAIBench [260] Evaluation.\\nEdgeMoE [421] Load experts on activation, tripling memory savings.\\nGEAR [168] Enhance KV cache quantization by integrating error-reduction techniques.\\nDMC [263] Adaptively compress KV cache, optimizing storage efficiency.\\nTransformer-Lite [196] Optimize KV cache to reduce redundancy and memory use.\\nLLMaaS [424] LLMaaS manages apps via chunk-wise KV cache optimization on mobiles.\\nRuntime\\nEfficiency\\nOptimization\\nmllm-NPU [400] On-device NPU (neural processing units) to reduce prefill latency.\\nCOST-EFF [312] Distill a multi-exit model from the original PLM.\\nLLMCad [399] Use SLM for fast token generation and cloud verification.\\nEdgeMoE [421] Predict expert needs, boosting inference speed and reducing latency.\\nLinguaLinked [451] Optimize dataflow and load, enhancing multi-threading efficiency.\\nAdaptive Layer Tuning and Voting scheme to optimize memory use while ensuring performance.\\nMeanwhile, LLM-PQ [452] addresses quantization and model layer partitioning for heterogeneous\\nclusters, incorporating a Cost Model and an Indicator Generator to optimize bit-width assignment\\nand layer partitioning through integer linear programming, enhancing quantization for complex\\ncomputational setups.Activation-aware Weight Quantization (AWQ)[212] is a hardware-\\nfriendly,low-bit,weight-onlyquantizationmethodforon-deviceLLMs,preservingessentialweights\\nbased on activation distribution to minimize quantization loss.MoE-I 2 [409] prunes less important\\nexperts and applies low-rank decomposition to the remaining experts to further optimize efficiency.\\nCache of MoE Experts. Beyond standard quantization, which reduces storage for all model pa-\\nrameters, another strategy involves caching a mixture of experts (MoE). Driven by the fact that\\nmemory storage is more cost-effective and scalable than computing capacity, the MoE architecture\\n[157] boosts performance while minimizing computational costs by activating only portions of the\\nLLM as needed. However, this approach incurs significant memory overhead, making it impractical\\nfor edge device memory constraints. For example, Switch Transformers [102], with 32 experts per\\nlayer, require 54\\xa0GBs of memory for inference, exceeding the capacity of most edge devices. Yi et al.\\n[421] note that in the Switch Transformers model, although most of the model weights (86.5%) are\\nattributed to experts, these weights account for only a small fraction (26.4%) of the computations. To\\naddress this,EdgeMoE [421] introduces a method where experts are loaded into an expert memory\\nbuffer only when activated, achieving approximately3× memory savings compared to the baseline\\nwhere all weights are held in memory.\\nKV Cache Compression . When serving LMs for inference, using a KV cache is common practice\\nto avoid intensive recalculations and speed up generation [283]. However, cache memory con-\\nsumption escalates with model size and sequence length, posing a challenge for edge devices. To\\nmanage this, offloading techniques transfer KV caches to CPU memory [13, 315], although this\\ncan introduce significant overhead due to the switching costs between GPUs and CPUs. Token\\ndropping compresses cache size by keeping only key tokens, often identified by low attention scores\\n[113, 225, 449]. However, this method struggles with complex tasks, especially at high compression\\nlevels, due to increased estimation errors in compressed KV values.GEAR [168] addresses these\\nissues by enhancing KV cache quantization with error-reduction techniques, including: (i) quantiz-\\ning caches of similar magnitudes to ultra-low precision, (ii) using a low-rank matrix for efficient\\nquantization residual approximation, and (iii) employing a sparse matrix for correcting outliers.\\nThis approach separates coherent from incoherent approximation errors, enabling near-lossless\\nKV cache compression and achieving up to2.29× peak memory reduction. Besides,Dynamic\\nMemory Compression (DMC)[263] adaptively compresses the KV cache by either adding new\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 33, 'page_label': '34'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:33\\nkey and value representations directly or blending them with the top cache item using a weighted\\naverage. Transformer-Lite [196] tackles the redundancy of storing the KV cache twice in model\\ninputs and outputs, which increases memory use. It optimizes storage by allocating a large tensor\\nbased on the maximum sequence length needed for inference. Sub-tensors are then created from\\nthis main tensor at different address offsets to serve as input and output KV caches, allowing direct\\nwriting to the correct locations during inference and removing extra copying steps.LLMaaS [424]\\nintroduces LLM as a Service for mobile devices, managing all apps through LLMS. This system\\nuses chunk-wise KV cache compression and swapping, enabling efficient context switching within\\nmemory constraints. By segmenting the KV cache into independently compressed and swapped\\nchunks, LLMS balances memory use and I/O bandwidth better than token-level or context-level\\nmanagement.\\nHowever, all these compression schemes trade off memory for computation: decompressing the\\nKV cache (e.g., upscaling 4-bit values back to full precision or reconstructing dropped tokens) adds\\nextra operations and latency. In fact, GEAR notes that its low-rank/sparse reconstruction “incurs\\nextra latency,” and Transformer-Lite explicitly targets reducing “dequantization overhead” from\\nlow-bit caches. Similarly, the LLMaaS system even pipelines recompute when loading compressed\\nchunks.\\n4.2.2 Runtime Efficiency Optimization. The goal of decreasing computing workload aligns with\\nenhancing memory efficiency through methods such as quantization, as mentioned in the previous\\nsection. Decreasing model weight precision or reducing the number of weights naturally lowers\\nlatency. Other runtime efficiency techniques of minimizing inference latency involve, reducing\\nprefill latency, early exits, large and small model collaboration, decreasing switching time in MoE,\\nand reducing latency in distributed SLMs.\\nReducing Prefill Latency. mllm-NPU [400] is the first LLM inference system that leverages on-\\ndevice NPU (neural processing units) to reduce prefill latency and energy consumption. It incorpo-\\nrates a chunk-sharing graph, shadow outlier execution, and out-of-order subgraph execution to\\nenhance NPU offloading efficiency. Experiments have shown mllm-NPU’s superior performance\\nbenefits, including up to 43.6× speedup and 59.5× energy savings.\\nDynamic Early Exits. A decoupled runtime saving technique is dynamic early exits. Originating\\nfrom BranchyNet [341], which introduces exit branches after specific convolution layers in the\\nCNN model, this concept has been adapted for PLMs as Transformer layer-wise early exiting [396].\\nEarly exiting enables dynamic acceleration during inference and reduces temporal overhead by\\nallowing exit without passing through all model layers. To address the inconsistency issue arising\\nfrom exiting at different layers,COST-EFF [312] distills a multi-exit model from the original PLM.\\nLarge and Small Model Collaboration. Modelcollaboration,deployingSLMsondeviceswithcloud-\\nbasedLLMsupport,enhancesruntimeefficiency.LLMswillincreaselatencywhendirectlydeployed\\nvia mobile engines like llama.cpp due to a large number of computing operations.LLMCad [399]\\naddresses this by using a real-time, memory-resident SLM for simple tokens such as determiners\\nand punctuation. The SLM generates tokens, while a cloud-based LLM verifies and corrects them,\\nspeeding up the process. LLMCad enhances token generation up to9.3× by pairing the memory-\\nresident SLM, Llama 2 7B, with the cloud-based LLM, Llama 2 13B, cutting latency from 16.2 to\\n3.9\\u2009seconds on Xiaomi Pro for TruthfulQA tasks [213].\\nReducing MoE Switching Time. To reduce latency in MoE architectures caused by frequently\\nswitching experts in limited device memory,EdgeMoE [421] enhances runtime efficiency by pre-\\nemptively predicting which expert will be needed, based on the observed long-tail distribution\\nof unbalanced expert activations. It utilizes a statistical model, built offline, to estimate expert\\nactivation probabilities in transformer layers from previous activations. During inference, EdgeMoE\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 34, 'page_label': '35'}, page_content='145:34 F. Wang et al.\\npreemptively loads the most likely needed expert, accelerating inference by1.11× to 2.78× and\\nsignificantly reducing latency. For instance, in a switch transformer model with 8 experts, latency\\ndrops from approximately 0.7\\u2009s to 0.3\\u2009s, outperforming the baseline method that preloads experts\\nbased on hit ratios.\\nReducing Latency in Distributed SLMs. Distributing an SLM across smaller devices reduces\\nthe need for extensive model compression while preserving accuracy. However, this approach\\nfaces challenges that incur latency, such as managing diverse device capabilities, handling data\\ndependencies between model segments, and adapting to dynamic resource availability. To address\\nthese issues,LinguaLinked [451] addresses these issues by optimizing model assignment to match\\ndevice capabilities and minimize data transmission, implementing runtime load balancing to\\nredistributetasksandpreventbottlenecks,andenhancingcommunicationforefficientdataexchange\\nbetweensegments.Withmulti-threading,thesystemimproves,achievingaccelerationratesbetween\\n1.73× and 2.65× for both quantized and full-precision models.\\nInsights: We draw several key insights from the deployment of SLMs:\\n—Model size remains a bottleneck for both memory and runtime efficiency. A common solution is model\\nquantization, which reduces model precision to save memory and lessen computing workload, thereby\\nboosting inference speed [212,227,260,292,427,452]. Similarly, KV cache compression also helps achieve\\nthese efficiency gains [168, 196, 263, 424].\\n—Mixture of Experts (MoE) is commonly used in SLMs to enhance performance using the same computing\\nresources, but it results in increased memory usage. To address this, only activated experts are loaded\\ninto the memory buffer while the majority are stored cold on disk. However, the cost of switching can\\nslow down inference. Designing a preemptive expert pre-load strategy could therefore accelerate the\\ninference [421].\\n—Model collaboration between local SLMs and cloud-based LLMs enhances both memory and runtime\\nefficiency by using smaller models on local devices, which are then verified by cloud LLMs to ensure\\nperformance is maintained. Using SLMs locally reduces memory usage and shortens the inference time\\nfrom the local model. However, internet latency and delays in cloud LLM inference can still introduce\\nlatency. Verifying SLM outputs every# tokens using LLMs can effectively mitigate this latency [399].\\n—One deployment approach involves deploying SLMs/LLMs across multiple trusted local devices to\\nmaintain original performance while only loading a fraction of the model weights. However, this method\\ncan incur latency due to varying device capabilities and resource scheduling challenges. To address these\\nissues, optimizing model assignment to align with device capabilities and minimizing data transmission\\nare effective strategies [451].\\n5 Generic and Domain-Specific SLMs\\nThis section investigates SLMs (with fewer than 7 billion parameters) in both general and specific\\ndomains. It details the methods of obtaining these SLMs the datasets, and the evaluation tasks,\\nexploring the techniques for acquiring SLMs through compression, fine-tuning, or training from\\nscratch. Additionally, we summarize the representative SLMs as detailed in Tables8 and 11.\\n5.1 Generic-Domain SLMs\\nOverview. SLMs, with fewer parameters than LLMs, enhance computational efficiency in pre-\\ntraining, fine-tuning, and inference, reducing memory and energy demands—crucial for resource-\\nlimited environments. Their compact, localized nature boosts privacy, personalization, and response\\ntimes, making them ideal for low-power edge devices. Therefore, SLMs are attracting increasing\\nattention, and various models are being developed. Table8 summarizes current representative\\ngeneric-domain 42 SLMs/SLM families. Although all chosen SLMs have similar architectures, they\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 35, 'page_label': '36'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:35\\nTable 8. High-Level Overview and Training Details of Generic-Domain SLMs\\nModel #ParamsDate Para-\\ndigm\\nDo-\\nmain\\nTraining Datasets Training Techniques\\nPhoneLM\\n[422]\\n0.5B; 1.5B2024.11Pre-trainGenericDCLM-baseline [195], Star-\\nCoderData [199]; OpenWeb-\\nMath[274],Dolma-algebraic\\nand Dolma-arXiv [322]\\nRoPE, MHA, Gated FFN, RMSNorm, ReLU,\\nFSDP, Flash Attention 2, ZeRO\\nLlama 3.2 [7] 1B; 3B 2024.9 Pre-trainGenericno release (9T tokens)GQA, SiLU, Multilingual Text and code,\\nShared embedding, Pruning, Distillation,\\nSFT, RLHF, RS, DPO\\nQwen 1 [24] 1.8B; 7B;> 2023.12Pre-trainGenericNo release MHA; RoPE; SwiGLU; RMSNorm\\nQwen 1.5\\n[24]\\n0.5B; 1.8B;\\n4B; 7B;>\\n2024.2 Pre-trainGenericNo release MHA; RoPE; SwiGLU; RMSNorm; Multi-\\nlingual support\\nQwen 2 [407] 0.5B;1.5B;\\n7B;>\\n2024.6 Pre-trainGenericNo release GQA; RoPE; SwiGLU; RMSNorm; Multilin-\\ngual support\\nQwen 2.5\\n[407]\\n0.5B; 1.5B;\\n3B; 7B;>\\n2024.9 Pre-trainGenericNo release GQA; RoPE; SwiGLU; RMSNorm; Multilin-\\ngual support; Larger corpus\\nGemma [338] 2B; 7B 2024.2 Pre-trainGenericUnknown MHA, RoPE, GELUtanh\\nGemma 2\\n[339]\\n2B;> 2024.7 Pre-trainGenericUnknown GQA; RoPE;GELUtanh; Alternating Local\\nand Global Attention; Logit Soft-Capping;\\nRMSNormforPre-andPost-Normalization\\nSmolLM [10] 135M;\\n360M; 1.7B\\n2024.7 Pre-trainGenericSmollm-corpus [28] GQA, trapezoidal LR scheduler\\nH2O-\\nDanube3\\n[281]\\n500M; 4B2024.7 Pre-trainGenericUnknown Three different training stages with differ-\\nent data mixes\\nFox-1 [340] 1.6B 2024.6 Pre-trainGenericUnknown (3T tokens)GQA; Deep architecture\\nRene [115] 1.3B 2024.5 Pre-trainGenericDolma-1.7 [322] Mamba-2 layers, sliding-window attention\\n(SWA)\\nMiniCPM\\n[146]\\n1.2B; 2.4B2024.4 Pre-trainGenericDolma [322]; C4 [291]; Pile\\n[87]; stack [177]; StarCoder\\n[199]; UltraChat [89]; Os-\\nsInstruct [378]; EvolInstruct\\n[397]\\nWSD learning rate scheduler\\nCT-LLM [94] 2B 2024.4 Pre-trainGenericMAP-CC Chinese, MHA, RoPE, SwiGLU, RMSNorm\\nPhi-1 [122] 1.3B 2023.6 Pre-trainCodingCodeTextBook [122]a MHA, GELUtanh, RoPE, FlashAttention\\nPhi-1.5 [204] 1.3B 2023.9 Pre-trainGenericCodeTextBook [122]; Syn-\\nthetic Datasets (20B)\\nMHA,GELUtanh, RoPE, FlashAttention,\\nDeep ZeRO Stage 2\\nPhi-2 [158] 2.7B 2023.12Pre-trainGenericCodeTextBook [122]; Syn-\\nthetic Datasets (20T)\\nMHA,GELUtanh, RoPE, FlashAttention,\\nDeep ZeRO Stage 2\\nPhi-3 [1] 3.8B; 7B;> 2024.4 Pre-trainGenericScaled-up dataset from phi-2MHA, SiLU, RoPE, FlashAttention, Deep\\nZeRO Stage 2\\nPhi-3.5 [1] 3.8B; 4.2B;\\n6.6B\\n2024.4 Pre-trainGenericmore multilingual and long-\\ntext data\\nMultilingual; Vision; MHA, SiLU, RoPE,\\nFlashAttention, ZeRO 2\\nOpenELM\\n[246]\\n270M;\\n450M; 1.1B;\\n3B\\n2024.4 Pre-trainGenericRefinedWeb [276], dedu-\\nplicated PILE [109], partial\\nRedPajama [70], partial\\nDolma v1.6 [322]\\nNo biases in FC layers; Pre-norm: RM-\\nSNorm; Pos encoding: RoPE; Attention:\\nGQA; FFN: SwiGLU; Tokenizer: LLaMA-\\nstyle\\nMobiLlama\\n[342]\\n0.5B; 0.8B2024.2 Pre-trainGenericLLM360 Amberb GQA; SwiGLU; Parameter sharing; multi-\\nmodal\\n(Continued)\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 36, 'page_label': '37'}, page_content='145:36 F. Wang et al.\\nTable 8. Continued\\nModel #ParamsDate Para-\\ndigm\\nDo-\\nmain\\nTraining Datasets Training Techniques\\nMobileLLM\\n[227]\\n125M; 350M2024.2 Pre-trainGenericUnknown (1\\u2009T tokens)SwiGLU FFN, deep and thin architectures,\\nembedding sharing, and GQA\\nOLMo [118] 1B; 7B 2024.2 Pre-trainGenericDolma [322]c SwiGLU; RoPE, Non-parameteric Layer\\nNorm\\nTinyLlama\\n[444]\\n1B 2024.1 Pre-trainGenericSlimPajama [321] and Star-\\nCoder [199]\\nGQA, SiLU, FSDP, Flash Attention [76],\\nxFormers [187]\\nStableLM\\n[346]\\n3B; 7B 2023.4 Pre-trainGenericRefinedWeb [276],\\nRedPajama [70], the Stack\\n[177], OpenWebText [116],\\nOpenWebMath [274], and\\npartial CulturaX [264]\\nMHA; SiLU; Fine-tuning; DPO; Self-\\nknowledge; RoPE; LayerNorm; no Biases\\nStableLM 2\\n[27]\\n1.6B 2024.2 Pre-trainGeneric\\nCerebras-\\nGPT [87]\\n111M;\\n256M;\\n590M; 1.3B;\\n2.7B; 6.7B;\\n>\\n2023.4 Pre-trainGenericPile [109] MHA; GELU; Maximal Update Parameteri-\\nzation\\nPythia [32] 14M; 70M;\\n160M;\\n410M; 1B;\\n1.4B; 2.8B;\\n6.9B;>\\n2023.4 Pre-trainGenericPile [109] MHA; GELU; Flash Attention [77]; RoPE\\n[324]; ZeRO [293]\\nBLOOM,\\nBLOOMZ\\n[184]\\n560M; 1.1B;\\n1.7B; 3B;\\n7.1B;>\\n2022.11Pre-trainGenericROOTS [183] and 13 pro-\\ngramming languages\\nMHA;GELUtanh; ALiBi Positional Embed-\\nding [284], Embedding LayerNorm [83]\\nGalactica\\n[336]\\n125M; 1.3B;\\n6.7B;>\\n2022.11Pre-trainScien-\\ntific\\nOpen-access scientific mate-\\nrials (106B tokens) but not\\nreleased\\nMHA; GeLU; Learned Positional Embed-\\ndings\\nOPT [445] 125M;\\n350M; 1.3B;\\n2.7B; 5.7B\\n2022.5 Pre-trainGenericPile [109] and PushShift.io\\nReddit [25]\\nMHA; ReLU\\nXGLM [214] 1.7B; 2.9B;\\n>\\n2021.12Pre-trainGenericCC100-XL –\\nGPT-Neo\\n[34]\\n125M;\\n350M; 1.3B;\\n2.7B\\n2021.5 Pre-trainGenericPile [109] –\\nMegatron-\\ngpt2 [317]\\n355M; 2.5B;\\n>\\n2019.9 Pre-trainGenericWikipedia [86], CC-Stories\\n[347], RealNews [435],\\nOpenWebtext\\n–\\nMINITRON\\n[259]\\n4B;> 2024.7 Distil-\\nlation;\\nPruning\\nGeneric8\\u2009T tokens in Nemotron-4\\n[273]\\nLR WSD Scheduler\\nOrca 2 [253] 7B 2023.11Distilla-\\ntion\\nGenericOrca 2 dataset LLaMA-2-7B based; prompt erasing\\nOrca [258] 13B 2023.6 Distilla-\\ntion\\nFLAN-v2 [229] From ChatGPT and GPT4, Explanation\\ntuning; Progressive Learning\\nMINIMA\\n[439]\\n3B 2023.11Distilla-\\ntion\\nGenericPile [109], Wudao [429],\\nGitHub [70]\\nFrom Llama-2-7B, Zero2, Flash Attention,\\nOptimal teacher size\\nDolly-v2 [71] 3B; 7B;> 2023.4 Instruc-\\ntion\\ntuning\\nGenericDatabricks-dolly-15k [71] From pythia\\n(Continued)\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 37, 'page_label': '38'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:37\\nTable 8. Continued\\nModel #ParamsDate Para-\\ndigm\\nDo-\\nmain\\nTraining Datasets Training Techniques\\nLaMini-LM\\n[174]\\n61M-7B 2023.4 Distilla-\\ntion\\nGenericLaMini instruction datasetA collection of SLMs distilled from\\nChatGPT-generated 2.58M instructions.\\nSpecialized\\nFlanT5 [107]\\n250M;\\n760M; 3B\\n2023.1 Instruc-\\ntion\\nTuning\\nGeneric\\n(math)\\nGSM8K Base model is FlanT5\\n#Params means parameter amounts. “>” indicates parameters larger than 7B.\\naCodeTextBook includes stack v1.2, code contests, synthetic python textbooks and exercises.\\nbLLM360 Amber includes Arxiv, Book, C4, Refined-Web, StarCoder, StackExchange, and Wikipedia.\\ncDolma includes Dolma’s CC, RefinedWeb, Star Coder, C4, PushShift API, Semantic Scholar, RedPajama, Flan, CC News,\\nOpenWebMath, MetaWika, Wikipedia.\\nFig. 19. Llama 3.2 1B model card.\\nvary in specific training datasets and techniques, with some datasets not being openly available.\\nTaking the latest Llama 3.2 1B models [7] in Figure19 as an example, its parameter size and use of\\nfiltered high-quality training data, pruning-based initialization, KD pre-training tasks, and training\\ntechniques such as SFT Rejection Sampling (RS), and DPO distinguish it from others.\\n5.1.1 Architecture Design. From Table8, we observe several trends in component choices for\\nSLMs:\\n(1) Recent SLMs frequently employ GQA in self-attention mechanisms because it can reduce\\ncomputational complexity. GQA achieves this by sharing query representations across mul-\\ntiple heads while keeping key and value representations separate. This approach aligns with\\nthe goals of SLM to enhance efficiency without compromising functionality.\\n(2) The choice of activation function should balance model capability and efficiency. ReLU,\\nknown for its efficiency, introduces greater sparsity to the model, which facilitates faster\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 38, 'page_label': '39'}, page_content='145:38 F. Wang et al.\\nTable 9. Zero-Shot Performance of Various Language Models from Small to Large on Common\\nBenchmarks\\nModel Size Range Model MMLU HellaSwag ARC (C) PIQA Winogrande\\n<1B\\nGPT-Neo 125M 26.0 30.3 23.0 62.5 51.8\\nTiny-Starcoder 170M 26.8 28.2 21.0 52.6 51.2\\nCerberas-GPT 256M 26.8 29.0 22.0 61.4 52.5\\nOPT 350M 26.0 36.7 23.6 64.7 52.6\\nMegatron-GPT2 345M 24.3 39.2 24.2 66.9 53.0\\nLiteLlama 26.2 38.5 24.9 67.7 49.9\\nGPT-SW3 356M 25.9 37.1 23.6 64.9 53.0\\nPythia 410M 27.3 40.9 26.2 67.2 53.1\\nXGLM 564M 25.2 34.6 24.6 64.9 53.0\\nLamini-GPT-LM 0.59B 25.5 31.6 24.2 63.9 47.8\\nMobiLlama 0.5B 26.5 52.5 29.5 72.0 57.5\\nMobiLlama 0.8B 26.9 54.1 30.2 73.2 57.5\\n1B–3B\\nStableLM 2 1.6B 41.8 68.2 43.8 74.0 64.9\\nPythia 1B 24.3 44.7 33.1 69.1 53.3\\nTinyLLaMA 1.1B 25.02 61.4 32.6 73.5 59.4\\nOLMo 1B 24.23 62.9 34.5 75.1 59.9\\nOLMo 1B (0724) 25.45 66.9 36.5 74.9 61.4\\nBoomer 1B 25.4 31.6 22.3 58 51.0\\nPythia-Dedup 1B 24.3 49.6 29.1 70.2 54.0\\nFalcon-RW 1B 25.4 63.1 35.1 74.1 61.9\\nCerebras-GPT 1.3B 26.7 38.5 26.1 66.8 53.6\\nLamini 1.3B 28.5 38.1 26.6 67.9 50.6\\nOPT 1.3B 24.6 54.5 29.6 72.5 59.7\\nGPT-Neo 1.3B 24.8 48.5 31.3 71.1 57.1\\nPythia-Deduped 1.4B 25.5 55.0 32.6 – 56.9\\nMobiLlama 1.2B 24.8 63.0 34.6 – 62.0\\nOpenELM 1.1B 25.3 64.8 32.3 75.6 61.7\\nOLMo-1B-0724-hf 1.2B 25.5 66.1 32.3 75.1 61.7\\nAMD-OLMo-1B 1.2B 24.9 63.6 33.7 75.6 61.6\\nGemma 2 2B 57.8 61.1 76.7 81.2 72.3\\nLlama 3.2 1B 49.3 41.2 59.4 – –\\nLlama 3.2 3B 63.4 69.8 78.6 – –\\n3B–7B\\nPhi-3.5-mini 3.8B 69.0 81.4 87.4 – –\\nPythia 6.9B – 63.8 44.1 75.2 60.9\\nFalcon 7B – 75.9 47.5 78.5 68.9\\nLLaMA 1 7B 35.1 76.2 44.5 77.2 70.5\\nLLaMA 2 7B 45.3 76.8 48.5 76.7 69.4\\nMPT 7B 30.2 77.6 46.5 77.3 69.9\\nRPJ-INCITE 7B 27.8 70.3 42.8 76.0 64.7\\nOLMo 7B 61.1 76.4 48.5 78.4 58.0\\n>7B\\nLLaMA 2 13B 55.6 80.7 48.8 80.8 72.9\\nLLaMA 2 70B 67.6 77.3 55.5 82.0 75.0\\nGemma 2 9B 70.6 87.3 89.5 86.1 78.8\\nGPT-3.5 86.4 90.1 87.4 89.3 82.7\\nClaude 2 89.3 92.1 88.0 91.5 85.5\\nGPT-4 90.6 93.5 94.0 92.7 87.2\\nData fromMobiLlama [342], OLMo/OLMoE [118, 257], and public evaluations of GPT-3.5, Claude 2, and GPT-4.\\ncoefficient calculations for inference acceleration. In contrast, SwiGLU’s parameters are\\nlearned during training, allowing the model to dynamically adapt to diverse tasks and\\ndatasets, thereby enhancing model capabilities and establishing it as a state-of-the-art option.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 39, 'page_label': '40'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:39\\nTable 10. Comparison of MobiLlama 0.5B with Larger Models (1.2B–180B) in Terms of Efficiency and\\nResource Consumption on Low-End Hardware Devices [ 342]\\nPlatform Model #Params Avg Tokens/s Memory (MB) Battery /1kT (mAh)\\nRTX 2080Ti\\nGPT-4 >100B N/A N/A N/A\\nClaude 2 >100B N/A N/A N/A\\nFalcon 180B <1.0 ∼3,20,000 N/A\\nLLaMA 2 70B ∼5.8a ∼1,28,000 N/A\\nMixtral 8×7B 46.7B (MoE) ∼10–20b ∼90,000 N/A\\nLLaMA 2 13B ∼20.0 ∼11,000 ∼85\\nGemma 2 9B ∼24.0 ∼8,000 ∼80\\nMistral 7.3B ∼30.0 ∼20,000 ∼68\\nPhi-2 2.7B 32.19 12,071 59.1\\nMobiLlama Large 1.2B 50.61 6,254 18.9\\nMobiLlama 0.5B 63.38 3,046 8.2\\nCPU i7\\nGPT-4 >100B N/A N/A N/A\\nClaude 2 >100B N/A N/A N/A\\nFalcon 180B ∼0.4–1.1c ∼7,00,000 N/A\\nLLaMA 2 70B ∼0.1–1.0 ∼2,56,000 N/A\\nMixtral 8× 7B 46.7B (MoE) ∼1–2 ∼90,000 N/A\\nLLaMA 2 13B ∼3.3 ∼8,400 ∼130\\nGemma 2 9B ∼4.5 ∼5,500 ∼97\\nMistral 7.3B ∼12.0 ∼3,500 ∼37\\nPhi-2 2.7B 22.14 1,972 27.4\\nMobiLlama Large 1.2B 29.23 1,163 10.8\\nMobiLlama 0.5B 36.32 799 4.9\\nSnapdragon\\nFalcon/GPT-4/Claude 2 >70B N/A N/A N/A\\nMixtral TurboSparse 47B (MoE) 11.7 24,000 N/A\\nLLaMA 2 13B N/A N/A N/A\\nGemma 2 9B N/A N/A N/A\\nPhi-2 2.7B 2.88 1,893 14.6\\nMobiLlama Large 1.2B 6.69 780 6.0\\nMobiLlama 0.5B 7.02 770 5.3\\na: PowerInfer with CPU–GPU offloading (i7-12700K\\xa0+\\xa0RTX 2080 Ti). b: Mixtral acts like a∼13B model in compute per\\ntoken; effective throughput depends on offloading. c: Falcon 180B 4–8\\u2009bit quantized on 256\\xa0GB server CPU (reddit).\\nSources: openreview.net, arxiv.org, reddit.com, news.ycombinator.com, mistral.Ai, anthropic.com, en.wikipedia.org.\\nSiLU, situated between these two, is favored for its balance of computational efficiency and\\nmodel performance.\\n(3) RMS normalization is commonly used than layer normalization due to its reduced computa-\\ntional demands.\\nA basic introduction to these options is provided in Section2. Apart from component choices, there\\nare notable innovations in architecture for SLMs:\\n—Mobilellm[227]highlightsthatdeepermodelsaremoreeffectivethanwideronesforimproving\\nperformance.\\n—Embedding sharing [445] is crucial, as embedding layers often constitute over 20% of a model’s\\nparameters—for example, with 512 dimensions and a 32k vocabulary, each layer holds 16M\\nparameters in a 125M-parameter model. Smaller models often reuse these weights for both\\ninput and output layers, enhancing efficiency and compactness.\\n—Layer sharing [227] increases hidden layers in small Transformer models without additional\\nstorage costs.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 40, 'page_label': '41'}, page_content='145:40 F. Wang et al.\\nTable 11. High-Level Overview and Training Details of Specific-Domain SLMs\\nModel #ParamsDate Base Models Domain Training DatasetsTrain Techniques\\nHippocrates [3] 7B 2024.4Instruction Tuning\\n(LLaMA2 [345],\\nMistral [163])\\nHealthcare Medical Guidelines,\\nPMC-Patients [456],\\nand PubMedQA-\\ncontexts [166]\\nContinual pre-training,\\ninstruction tuning, RLHF\\nBioMedLM [35] 2.7B 2024.3From scratch and\\nFine-tuning\\nHealthcare PubMed [109] FSDP\\nBioMistral [181] 7B 2024.2Mistral [163] BiomedicinePubMed [109] Continual pretraining\\nMentaLLaMA\\n[411]\\n7B; 13B 2023.9Instruction Tuning\\n(LLaMA2 [345])\\nHealthcare IMHI dataset RLHF; PEFT\\nAdaLM [419] 34M 2021.6Distillation (BERT\\n[86] or MiniLM [368])\\nHealthcare PubMed [109] Continual pretraining,\\nAdapt-and-Distill\\nRho-1 [215] 1B; 7B 2024.4TinyLlama-1.1B[444],\\nMistral-7B [163]\\nScience\\n(Mathemat-\\nics)\\nOpenWebMath [274] Continual pretraining\\nChemLLM [441] 7B 2024.4Instruction Tuning\\n(InternLM2)\\nScience\\n(Chemistry)\\nChemData Continual training and\\nfine-tuning\\nSciGLM [440] 6B 2024.3Instruction Tuning\\n(ChatGLM-6B)\\nScience SciInstruct Self-reflective instruc-\\ntion annotation\\nLlemma [23] 7B 2023.10Code Llama 7B Science\\n(Mathemat-\\nics)\\nProof-Pile-2 [23] Continual pre-training\\nOceanGPT [31] 2B; 7B;\\n14B\\n2023.10LLaMA2 [345] Science\\n(Ocean)\\nOpen-access litera-\\nture, DoINSTRUCT\\nContinual pre-training,\\nInstruction tuning\\nAstroLLaMA\\n[265]\\n7B 2023.9Tuning (LLaMA-2-7B)Science (As-\\ntronomy)\\narXiv abstracts from\\nKaggle\\nContinual training\\nDARWIN [393] 7B 2023.8LLaMa 7B Science\\n(physics,\\nchemistry,\\nand material)\\nSciQ [380], Scientific\\npaper [393], FAIR\\n[393]\\nFine-tuning\\nMindLLM [417] 1.3B; 3B 2023.10From-scratch and SFTLaw, FinancePile [109], Wudao\\n[429], CBooks\\nTrain on Bilingual Mix-\\nture Data, SFT\\n—Shared FFNs [342] make up about 65% of all trainable parameters, with attention mechanisms\\nand heads accounting for the rest. Sharing FFN parameters across all transformer layers of an\\nSLM is proposed to increase efficiency.\\n—Architecturesearchaheadofpre-training.PhoneLM[ 422]proposesaprincipleforconstructing\\non-deviceSLMs:searchingforaresource-efficientarchitectureonagivenhardwaretooptimize\\nthe speed-capacity trade-off before pretraining. This approach inspires the tailored selection of\\narchitectural components for on-device SLMs, based on specific compositional requirements\\nsuch as computing efficiency, model capability, and safety.\\nA detailed description of these architectural designs can be found in Section3.1.\\n5.1.2 Training Datasets. From Table8, we can observe a set of widely used training datasets in\\nSLM development. We provide the details below:\\n—Pile [109]: It comprises 22 smaller, high-quality, diverse corpora from various domains, such\\nas Pile-CC, PubMed Central, ArXiv, GitHub, and FreeLaw, designed to offer a comprehensive\\nfoundation for language model training. The dataset contains 207 billion tokens and totals\\n825\\xa0GB.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 41, 'page_label': '42'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:41\\n—C4 (Colossal Clean Crawled Corpus) [291]:Thisdatasetincludes350billiontokens,representing\\na cleaned version of the Common Crawl web corpus, intended to capture a wide snapshot of\\nthe internet.1\\n—The Stack [177]: It contains 6 trillion tokens of source code from over 300 programming\\nlanguages, useful for developing code-centric AI applications.Python-edu in smollm-corpus\\n[28] consists of Python files that are scored 4 or more by the educational code model and are\\nextracted from the stack-v2-train dataset.\\n—StarCoder [199]: It features 35 billion tokens, predominantly Python code, aimed at program-\\nming language understanding and generation.\\n—RedPajama [70]: This dataset encompasses 1.2 trillion tokens derived from over 100 billion\\ntext documents, processed using the CCNet pipeline to ensure a rich collection of web texts.\\n—RefinedWeb [276]: This dataset includes 5 trillion tokens of high-quality, extensively filtered\\nweb data, offering a valuable resource for training web-aware models.\\n—PushShift.io Reddit [25]: A resource around 5 billion tokens for social media data collection,\\nanalysis, and archiving, specifically of Reddit data, aiding research into social media dynamics.\\n—CulturaX [264]: It comprises 6.3 trillion tokens across 167 languages, supporting the develop-\\nment of models with extensive linguistic and cultural understanding.\\n—FineWeb [275]: A large-scale (15-trillion tokens, 44 TB disk space) dataset for LLM pretraining.\\nFineWeb is derived from 96 CommonCrawl snapshots.FineWeb-Edu is a subset of FineWeb\\nconstructed using scalable, automated, high-quality annotations for educational value.\\nFromtheanalysisofthesedatasets,wecanderiveseveralcriticalinsightsregardingthedevelopment\\nof SLMs: (i) Data quality is crucial for training effective SLMs, involving sophisticated filtering\\nlike removing duplicates or irrelevant content, often with another LLM’s help. For example, the\\nTinyStories corpus [98] is tailored for simplicity, ideal for training models to handle straightforward\\nnarratives. RedPajama-V2 [70] uses the CCNet pipeline to process 30B documents, providing quality\\nsignals and IDs for creating a 20B deduplicated dataset. (ii) Code Data: Source code constitutes a\\nsignificant component of valuable data for training models, particularly because of its structured\\nnature and logical content. Training on code data enhances a model’s reasoning capabilities and\\nsupports its ability to generalize across multiple natural languages, which is crucial for applications\\nrequiring robust problem-solving and interpretation skills in diverse coding environments [18, 106,\\n122, 241].\\n5.1.3 Training Algorithms. To enhance the alignment of SLMs with desirable properties such as\\nsafety and reasoning, training algorithms, particularly during the fine-tuning phase, are crucial in\\nevolving pre-trained SLMs.\\n—DPO [290] presents a simpler alternative to RLHF for optimizing language models based on\\nhuman preferences, preventing explicit reward modeling and reinforcement learning. Instead,\\nDPO modifies log probabilities of responses with a dynamic weighting mechanism to prevent\\nmodel degradation common in probability ratio-focused methods. The DPO loss function is:\\nL\\x19%$(c\\\\; cref) = −E(G,~F,~; )∼\\x19\\n[\\nlog f\\n(\\nVlog c\\\\(~F|G)\\ncref(~F|G) − Vlog c\\\\(~;|G)\\ncref(~;|G)\\n)]\\n,\\nwherec\\\\ isthepolicybeingoptimized, cref isthereferencepolicy, \\x19includestuples (G,~F,~;),\\nf is the sigmoid function, andV scales the log-ratios betweenc\\\\ and cref, guiding the model\\ntowards human-preferred outputs.\\n1Available athttps://commoncrawl.org.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 42, 'page_label': '43'}, page_content='145:42 F. Wang et al.\\n—Reinforcement Learning from Contrast Distillation (RLCD) [410] aims to calibrate gen-\\nerative SLMs/LLMs towards embodying harmless and beneficial characteristics. The process\\nstarts with an unaligned LM and initial prompts, which are modified into two variants,\\n?+ and ?−, intended to promote and suppress, respectively, attributes like helpfulness and\\nharmlessness. Upon inputting these prompts, the LM generates outputs>+ and>−, with>+\\nautomatically designated as the preferred response. This automation speeds up training by\\navoiding additional evaluative scoring. The training continues under the RLHF framework.\\n—Conditioned Reinforcement Learning Fine-Tuning (C-RLFT) , by OpenChat [362], en-\\nhances model performance by incorporating low-quality data during SFT. C-RLFT leverages\\nvaried data qualities with simple rewards (e.g., expert data at 1 credit, sub-optimal at 0.1),\\nusing distinct prompt tokens to condition data sources, eliminating costly human feedback.\\nSimilarly, Data Mix [281] trains on English text in three stages, reducing noisy web data\\nprogressively in each stage in favor of higher-quality data.\\n—Explanation Tuning, proposed by Orca [258], addresses the limitations of standard instruction-\\nbased fine-tuning, which often restricts SLMs to style imitation rather than reasoning. It uses\\nsystem prompts with instructions to direct GPT-4 to produce detailed explanations or perform\\nstep-by-step reasoning. The resulting instructions and the responses are used as a dataset for\\nfine-tuning SLMs to have a better ability to reason.\\n—Prompt Erasing , introduced by Orac 2 [253], is a distillation strategy designed to enhance\\nthe independent reasoning capabilities of student SLMs. In this approach, a more capable\\nteacher LLM is given intricate prompts intended to elicit specific strategic behaviors and more\\nprecise outcomes. During the training phase, the SLM is exposed only to the task instruction\\nand the resulting behavior, without access to the original intricate prompts that initiate such\\nresponses. This technique,knownas PromptErasing,positionsthe studentmodel asa cautious\\nreasoner because it not only learns to perform specific reasoning steps but also develops\\nstrategies for approaching tasks at a higher level.\\n—Progressive Learning, proposed by Orca [258], aims to bridge the capability gap between Orca\\nand the more capable GPT-4. It starts with training on five million data points from ChatGPT,\\nfollowed by one million from GPT-4. Research suggests that an intermediate-level teacher can\\nimprove distillation effects, enabling a stepwise learning approach where students start with\\nsimpler examples and gradually move to more complex ones, receiving improved reasoning\\nand explanations from a more advanced teacher.\\n—Maximal Update Parameterization (`P) optimizescontrolinitialization,layer-wiselearning\\nrates, and activation magnitudes to ensure stable training regardless of model layer widths.\\nThis method enhances training stability and allows the same optimizer settings, especially\\nlearning rates, to be used across different model scales. For instance, Cerebras-GPT [321]\\nemploys `P to train its models efficiently.\\n5.1.4 Model Performance. TocomparetheperformanceofSLMs,wehaveextractedexperimental\\nresults from two recent and concurrent studies published in June 2024,OLMo [118] andMobiLlama\\n[342], and the recently proposed edge-deviceLlama 3.2 1B and 3B in September 2024.2 The extracted\\nresults are merged and shown in Table9. From the table, we can find that the following evaluation\\nbenchmarks are commonly used:\\n(1) MMLU [137]: Evaluate broad knowledge across diverse fields such as humanities, science,\\ntechnology, engineering, and management. It includes multiple-choice questions covering\\n2https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 43, 'page_label': '44'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:43\\n57 tasks ranging from elementary mathematics to US history, computer science, law, and\\nbeyond, with a total of 14K test samples.\\n(2) HellaSwag [434]: Assesses the model’s ability to select the correct ending to scenarios from\\nmultiple options, testing common sense reasoning, including 10K test samples.\\n(3) ARC [68]: The AI2’s Reasoning Challenge (ARC) dataset features multiple-choice science\\nexam questions for grades 3 to 9, divided into Easy and Challenge partitions, with the latter\\ncontainingmorecomplexquestionsnecessitatingreasoning.Mostquestionsofferfouranswer\\nchoices. ARC includes a supporting knowledge base of 14.3M unstructured text passages,\\nwith 1.17K test samples in ARC_Challenge and 2.25K in ARC_Easy.\\n(4) PIQA [33]: A commonsense reasoning dataset designed to evaluate the physical knowledge\\nof NLP models. It presents questions (goals) that require physical commonsense for correct\\nresolution, alongside two detailed response options (sol1 and sol2). The dataset comprises\\n3,000 test samples.\\n(5) Winogrande [301]:adatasetstructuredasafill-in-the-blanktaskwithbinaryoptions,designed\\nto assess commonsense reasoning. The dataset includes 1,767 test samples by default splits.\\nAccuracy is used as the evaluation metric in the table.Open Language Model (OLMo)[118]\\nis publicly available with its training data and code.3 MobiLlama [342] is a general-purpose SLM\\ndesigned from scratch, available in 0.5B and 0.8B versions. It adopts a unique approach by using\\na shared FFN across all transformer blocks, enhancing efficiency.MobiLlama also shows high\\nefficiency on diverse hardware (Table10).\\nFrom Tables9 and 10, we can conclude that: (1) MobiLlama 0.5B and 0.8B demonstrate that a\\nshared FFN design can facilitate excellent performance in SLMs with fewer than 1B parameters,\\neven rivaling some models in the 1B-3B range. (2) The performance of MobiLlama 1.2B and OLMo\\n1.2B illustrates that advanced SLM architectures incorporating high-quality data, SwiGLU, non-\\nparametric layer normalization, RoPE, BPE tokenization, and a shared FFN design can achieve\\ncompetitive results among models with 1B-3B parameters. (3) Popular techniques such as pruning,\\nquantization, distillation, SFT, and DPO, utilized in Llama 3.2, have substantially enhanced SLM\\nperformance.(4)MobiLlamademonstratesthatSLMscansignificantlyreduceresourceconsumption\\non low-end hardware devices, achieving comparable performance while using a smaller proportion\\nof battery power and memory.\\nInsights: We draw several key insights from the development of generic-domain SLMs:\\n—Typical SLM architectures generally incorporate features such as GQA, gated FFN with SiLU activations,\\nRMS normalization, deep and thin architectures, embedding sharing, layer sharing, and shared FFNs.\\n—Although these components are widely used, current research has not yet thoroughly explored their\\nspecific contributions within SLMs.\\n—The importance of data quality in SLM research is increasingly emphasized, often considered more critical\\nthan the quantity of data and model architectural configurations.\\n—Post-pretraining, meticulous fine-tuning is often required to enhance the safety of SLMs, involving strate-\\ngies to distill capabilities from LLMs better. Common strategies include explanatory tuning, progressive\\nlearning, and prompt erasing.\\n5.2 Domain-Specific SLMs\\nOverview. The capability of LLMs to generate human-like text has significantly captured pub-\\nlic interest, highlighting their potential in the field of general artificial intelligence. However,\\n3https://allenai.org/olmo.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 44, 'page_label': '45'}, page_content='145:44 F. Wang et al.\\ninefficiencies persist when integrating LLMs into specialized applications due to resource con-\\nstraints. Unlike the need for extensive general knowledge and capabilities, domain-specific SLMs\\nshouldfocusonwell-definedtasksandexpertisepertinenttospecificfields.Forinstance,specialized\\nmodels can significantly impact biomedical research and healthcare by fine-tuning for interpretable\\nmentalhealthanalysisorassistinghumansinlegaldialoguesandfinancialtasksthroughinstruction\\ntuning, showcasing their potential transformative influence. Given the limited number of SLMs\\nspecialized in specific domains, we demonstrate some existing SLMs individually across healthcare,\\nscience, finance, and law domains.\\n5.2.1 SLMs for Healthcare. Hippocrates [3]isanopen-sourcemedicallanguagemodelframework\\nwith free access to its data, codebase, checkpoints, and protocols.4 It utilizes a medical pre-training\\ncorpus from Medical Guidelines, PMC-Patients [456], and PubMedQA-contexts [166], totaling about\\n300M tokens. The Hippo series, a 7B model, undergoes continuous pre-training, instruction tuning,\\nandRLHF.Fine-tunedonMistralandLlama-2,itrivals70Bmodelsinsomeevaluations.Forexample,\\nHippo-Mistral 7B scores 59.9% on MedQA, outperforming Meditron 70B [57] at 58.5%.BioMedLM\\n[35], a 2.7B GPT-style model trained on PubMed content [109], excels in biomedical QA after fine-\\ntuning, achieving 57.3% on MedMCQA (dev) and 69.0% on MMLU medical genetics exams. Available\\non Hugging Face Hub.5 AdaLM [419] enhances domain-specific SLMs by continuing training on a\\nmedical-focused SLM atop a general pre-trained model. It empirically validates that adaptation-\\nthen-distillation is the most effective way. AdaLM modified a BERT_base model (12 layers, 768\\nhidden size) [86] with a 16\\xa0GB PubMed6 abstracts corpus.MentalLLaMA [411] introduces the first\\nIMHI dataset for mental health analysis and the first open-source LM for explainable analysis\\non social media. The IMHI is compiled from ten sources, totaling 105K samples. Expert-designed\\nmental health analysis prompts are employed via ChatGPT for explanations. Based on Llama-2-7B,\\nMentalLLaMA is instruction-tuned on this data and matches top methods in accuracy on the IMHI\\ntest set. Project code is available athttps://github.com/SteveKGYang/MentaLLaMA.\\n5.2.2 SLMs for Science. SciGLM [440] is a collegiate-level scientific language model overcom-\\ning data scarcity with a self-reflective instruction annotation framework. Utilizing GPT-4 [2], it\\ngenerates detailed reasoning for unlabeled scientific problems through three steps with designed\\nprompts in Table12: (i) CoT prompt for step-by-step answers (Prompt 1), (ii) reflective prompt\\nfor correcting errors (Prompt 2), and (iii) integrating the correct answer for clarity (Prompt 3).\\nThe SciInstruct dataset spans physics, chemistry, math, and proofs, tuning ChatGLM-6B’s [95]\\nreasoning abilities. SciGLM boosts the base model’s (ChatGLM3-6B-Base) scientific QA accuracy\\nby 3.06% on benchmarks such as CEval-Hard [152], CEval-Sci [152], MMLU-Sci [137], SciEval\\n[325], and SciBench [369]. Llemma [23], an SLM derived from CodeLlama [299], specializes in\\nmathematical reasoning. By continual pre-training, its 7B model is evolved on 55B tokens from\\nthe newly created Proof-Pile-2 dataset, which includes scientific papers, math web content, and\\nmathematical code up until April 2023, to enhance few-shot capabilities. It excels in mathematical\\nbenchmarks like MATH [137], GSM8k [69], OCWCourses [189], MMLU-STEM [137], and SAT,\\nsurpassing all comparable open-weight models.ChemLLM [441] is a chemistry-focused language\\nmodelthatutilizesitsproposedChemData,adatasetdesignedforinstructiontuningthattransforms\\nchemical data into dialogue format for training. ChemLLM is based on InternLM2-Base-7B [41],\\ninitially enhancing its language skills with a multi-corpus of 1.7 million Q&A pairs from Hugging\\nFace, then fine-tuning using ChemData and the multi-corpus to maintain its general capabilities.\\n4https://cyberiada.github.io/Hippocrates/.\\n5https://huggingface.co/stanford-crfm/BioMedLM.\\n6https://pubmed.ncbi.nlm.nih.gov/.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 45, 'page_label': '46'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:45\\nTable 12. Prompts for Self-Reflective Instruction Annotation Framework\\nCoT [Prompt 1] The following input consists of a science problem. Please generate an elaborate step-by-step solution\\nto the problem.\\nReflective Generation [Prompt 2] The following input comprises a science problem and a corresponding solution. However, this\\nsolution is incorrect. Please reflect on its errors and then generate a correct step-by-step solution to the problem.\\nPrompt Answer [Prompt 3] The following input consists of a science problem, a corresponding solution, and the real answer.\\nThe given solution is incorrect. Please reflect on its errors and then generate a correct step-by-step solution to\\nthe problem based on the real answer.\\nChemLLM excels in interdisciplinary chemical tasks within the proposed ChemBench, achieving\\nresults comparable to GPT-4 [2] and outperforming GPT-3.5 with a score of 92.6 in Mol2caption,\\nslightly below that of GPT-4.AstroLLaMA [265] introduces an astronomy-focused language model.\\nBased on Llama-2-7B [345] and enhanced via continual pre-training, it has been developed using\\nover 300\\u2009K astronomy abstracts from arXiv.7 AstroLLaMA achieves 30% lower perplexity than\\nLlama-2-7B, indicating substantial improvements in domain adaptability. AstroLLaMA is avail-\\nable8 for tasks such as automated paper summarization and conversational agent development in\\nastronomy.\\n5.2.3 SLMs for Finance and Law. MindLLM [417] introduces a bilingual (Chinese and English)\\nSLM, pretrained on the Pile dataset [109] for English and WuDao [429], CBook, and various\\nChinese web content for Chinese. Bilingual training enhances capacity and prevents catastrophic\\nforgetting. It explores specific domains such as law and finance through SFT. In law, it utilizes\\npublicly available legal data, scenario-based Q&A from LaW-GPT [142], and NLP-based legal tasks\\nfrom DISC-LawLLM [432]. In finance, EastMoney9 is selected as the data source.\\nInsights: We draw several key insights from the development of domain-specific SLMs:\\n—Adapting SLMs to domain-specific data is a common practice for acquiring domain-specific SLMs,\\nprompting many to create their datasets [265, 411, 440, 441]. These datasets are often annotated\\nusing LLMs like GPT-4 and used to continual pre-train or fine-tune general models such as LLaMa-\\n2-7B [3, 35]. To ensure the data quality, specialized annotation frameworks are developed, such as\\nSciGLM [440].\\n—In domains with abundant corpora, training a general model from scratch and fine-tuning it using\\nSFT [417] is practical. Bilingual settings during training can prevent catastrophic forgetting.\\n—DistillinggeneralcapabilitiesfromLLMswhileintegratingdomain-specificknowledgefromcorpora\\nis another method for developing domain-specific SLMs [419].\\n6 SLMs for LLMs\\nIn this section, we provide a comprehensive review of how SLMs enhance LLMs. While LLMs are\\nrobust, they face challenges such as latency during inference, labor-intensive fine-tuning, noise\\nfiltration issues in retrieval, suboptimal zero-shot performance, copyright infringement risks, and\\nevaluation difficulties. SLMs can help LLMs to alleviate these issues. Research in this field can\\nbe categorized into five primary areas: (i) using SLMs for reliable LLM generation; (ii) extracting\\nprompts for LLMs using SLMs; (iii) fine-tuning LLMs with SLMs; (iv) applying SLMs in LLM\\napplications; (v) utilizing SLMs as guardians; and (vi) evaluating LLMs using SLMs. A summary\\n7https://www.kaggle.com/Cornell-University/arxiv.\\n8https://huggingface.co/universeTBD/astrollama.\\n9https://www.eastmoney.com/default.html.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 46, 'page_label': '47'}, page_content='145:46 F. Wang et al.\\nTable 13. SLMs Help LLMs in Different Aspects\\nAspect Representative Work Key Point\\nSLM for reliable\\nLLM generations\\nAPRICOT [349] Trains a small auxiliary model to predict LLM’s confidence using only\\ntextual inputs and outputs.\\nPOLAR [454] Using a BERT model to calibrate LLM responses.\\nHallucination Detector in NMT\\n[404]\\nUsing lightweight classifiers to detect hallucinations in Neural Machine\\nTranslation.\\nSAPLMA [22] UsingaBERTSLMasaclassifiertoassessthetruthfulnessofstatements\\naccurately.\\nQuestion Decomposer [388] Distilled SLM decomposes complex questions to aid reasoning.\\nSuperICL [398] SLM Plug-ins provide confidence and prediction for contextual exem-\\nplars to aid in-context learning.\\nSuperContext [413] Specific SLM enhances ICL by providing confidence and predictions to\\novercome out-of-domain challenges.\\nSelf-RAG [19] A proxy model labels special tokens during RAG data generation for\\nfine-tuning.\\nSKR [371] Training a small model to detect its self-knowledge for better use of\\nexternal knowledge.\\nSlimPLM [331] Detecting missing knowledge in LLMs with a slim proxy model, en-\\nhancing the LLM’s knowledge integration.\\nIn-Context RALM [294] Training a RoBERTa-based reranker for top-k BM25 documents using\\nLM signals to enhance LM gains.\\nCRAG [406] Training a lightweight evaluator to assess document quality and trigger\\nactions based on confidence levels.\\nGSR [151] Training a Generative Sub-graph Retriever (GSR) for relation chain in\\nRAG when retrieving from knowledge graphs.\\nSLM for\\nextracting LLM\\nprompts\\nPrompt Extraction [448] Small model trained to predict confidence of extracted system prompt\\nfrom adversarial prompts.\\nPrompt Stealing Attacks [304] Using small models fine-tuned as parameter extractors to facilitate\\nhierarchical prompt reconstruction.\\nOutput2prompt [438] Using a sparse encoder-decoder-based T5 small model to reverse-\\nengineer LLM inputs from outputs.\\nModel Purifying [201] Using SLMs to ensemble with LLMs, mitigating negative effects from\\nuncurated data.\\nSLM for\\nFine-tuning\\nLLMs\\nL P [247] Learning Percentage as a difficulty metric.\\nEmulated Fine-tuning [252] Emulating pre-training and fine-tuning at different scales by summing\\nbase log probabilities with behavior deltas.\\nCROSSLM [82] SLMs enhance LLMs by generating task-specific high-quality data.\\nWeak-to-Strong Search [457] Framing LLM alignment as a test-time greedy search to maximize the\\nlog-probability difference between tuned and untuned SLMs.\\nSLM for LLM\\napplications\\nSLCoLM [333] Using SLM predictions to guide the LLM generation process in Chinese\\nEntity Relation Extraction.\\nHEF [418] Using SLMs as plugins to improve LLM’s nuanced understanding.\\nContrastive decoding [202] Enhancing text quality by maximizing the difference between expert\\nand amateur log probabilities.\\nSLM for LLM\\nsafety\\nLlama Guard [156] An LLM-based input–output safeguard model geared towards Hu-\\nman–AI conversation use cases.\\nSLM as Guardian [180] AsmallerLLMforbothharmfulquerydetectionandsafeguardresponse\\ngeneration.\\nSLM for LLM\\nevaluation\\nSLIDE [453] Utilizing SLMs trained via contrast learning to distinguish and score\\nresponses in dialogue scenarios effectively.\\nKuhn et al. [178] An SLM is used as the natural language inference classifier.\\nSelfCheckGPT [245] An SLM is used to calculate BERTScore.\\nFactscore [250] An SLM functions as the natural language inference classifier.\\nof representative work in each category along with their key point is given in Table13. Next, we\\nintroduce each category in detail.\\n6.1 SLM for Reliable LLM Generation\\nAlthough LLMs generally produce fluent and convincing text, they can occasionally generate\\nerroneous responses [162, 355]. Additionally, LLMs are susceptible to privacy breaches from\\nuntrusted data collection, which can erode user trust or cause harm. To address these issues, recent\\nstudies have focused on using SLMs to calibrate LLM confidence, detect hallucinations, and improve\\nretrieval-augmented LLMs and their reasoning capabilities.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 47, 'page_label': '48'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:47\\nFig. 20. Architectures of Enhancing Calibration and Hallucination Detection of LLMs.\\nEnhancing Calibration and Hallucination Detection of LLMs. As illustrated in Figure20(a), to\\ncalibrate LLMs, an SLM processes both questions and LLM-generated answers to predict calibrated\\nconfidence. This training involves minimizing the discrepancy between estimated calibration\\nerror and predicted confidence score. For instance,APRICOT [349] uses an auxiliary DeBERTaV3\\nmodel [133] to assess LLM confidence in open-question scenarios, aiming to improve uncertainty\\nexpression and response adjustment. Similarly,POLAR [454] has developed a self-supervised\\napproach that generates risk scores for each response to calibrate LLM confidence, utilizing a small\\nBERT model [86] to synchronize LLM outputs with other weak supervision sources. As shown\\nin Figure20(b), for hallucination detection, an SLM analyzes LLM internal states to output the\\nlikelihood of hallucination. This process uses supervised data obtained by testing the knowledge\\nboundaries of the LLM. In neural machine translation, Xu et al. [404] develop a lightweight detector\\nthat analyzes token contributions to hallucinations, outperforming both model-free baselines and\\nquality estimation classifiers. Furthermore,SAPLMA [22] found that LLM internal states can signal\\nthe truthfulness of statements, with a small BERT classifier trained to differentiate correct from\\nincorrect predictions, achieving accuracies of 71–83%.\\nEnhancing RAG. Generally, as shown in Figure21, SLMs can also serve as proxy models to\\nevaluate the familiarity of LLMs with user queries, determining whether LLMs need to retrieve\\nadditional information or can respond directly. For example, SlimPLM [331] is a small proxy\\nFig. 21. Architecture of SLM as a Heuristic RAG prober.\\nmodel that assesses the necessity for LLM re-\\ntrieval by generating heuristic answers. High-\\nquality responses indicate that LLMs can han-\\ndle queries independently, whereas lower-\\nquality outputs require further retrieval. Ad-\\nditionally, Self-Knowledge Guided Retrieval\\n(SKR) [371] enables SLMs to autonomously de-\\ncide when LLMs should operate independently,\\nbased on their self-assessment of knowledge\\nlimitations. Further,SELF-RAG [19] improves the factual accuracy and quality of LLM outputs\\nthrough on-demand retrieval and self-reflection. This method employs a small critical language\\nmodel to issue reflective markers and make binary decisions regarding the need for further infor-\\nmation retrieval.Moreover, some studies utilize SLMs to evaluate the relevance of retrieved documents.\\nLongLLMLingua [165] employs SLMs to calculate the relevance of documents to a queryG@D4 using\\nperplexity, as formalized by the equation:\\nA: = − 1\\n#2\\nÕ\\n8\\nlog ?SLM(G@D4\\n8 |G3>2\\n: ), : ∈ { 1,2,..., }, (2)\\nwhereG@D4\\n8 is the8th token in the query sequence,G3>2\\n: is the retrieved document, and#2 is the\\ntotal number of tokens in the query.?SLM represents the probability generated by an SLM.CRAG\\n[406] employs SLMs as evaluators of document relevance in the same way.RA-ISF [224] trains a\\nSLM that checks the base LLM in self-knowledge, relevance judgment, and question decomposition.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 48, 'page_label': '49'}, page_content='145:48 F. Wang et al.\\nIn addition, some research employs SLMs as re-rankers to refine the order of documents provided\\nby initial retrieval efforts such as BM25 [298]. In-Context RALM [294] positions SLMs as rankers,\\noptimizing the document sequence with a fine-tuning process on RoBERTa [223] as defined by the\\nloss function:\\nmin\\nA0=:4A\\n:Õ\\n8=1\\n− log ?rank(38|G≤B9 ) · ?\\\\(~|38; G≤B9 ), (3)\\nwhere G≤B8 is a prefix sampled from the training data,~ = GB8 +1,...,G B8 +B represents the text to\\nbe generated in the next stride,?\\\\(~|38; G≤B8) denotes the probability of the LLM generating~\\ngiven38 and G≤B8, and?rank(38|G≤B9 ) is the ranking score of38. Lastly, some studies leverage SLMs to\\nretrieve subgraphs when utilizing knowledge graphs as external sources. Huang et al. [151] introduce\\nthe Generative Sub-graph Retriever (GSR), which employs SLMs to predict relation chains\\nfor answering questions, offering a cost-effective alternative to training LLMs. Specifically, it\\nuses customized T5 (220M, 770M, and 3B) [291] as retrievers to enhance LLM readers, including\\nLlama2-chat-7B [345] and Llama3-instruct-8B [96], on the WebQSP [423] and CWQ [330] datasets.\\nFig. 22. SLM transfers knowledge into ICL.\\nEnhancing Reasoning Capabilities of\\nLLMs. As illustrated in Figure22, SLMs\\nenhance LLMs’ reasoning by transferring\\ntask knowledge to in-context examples, ef-\\nfectively reducing hallucinations. While\\nIn-context Learning (ICL) generally han-\\ndles few-shot learning with 16 to 32 ex-\\namples, it struggles when faced with extensive supervised data. SLMs, specialized in task-specific\\ntraining, complement the broader domain knowledge of extensively pre-trained LLMs. For example,\\nSuperICL [398] incorporates SLMs as plugins for efficiently executing supervised tasks. It predicts\\nlabels for contextual examples and integrates these predictions with the input text and actual\\nlabels to enhance knowledge transfer, thereby boosting the understanding and responsiveness\\nof LLMs.SuperContext [413] tackles challenges that LLMs encounter with new tasks and out-of-\\ndistribution data in natural language understanding by synergizing SLM outputs with LLM prompts\\nduring inference. This integration merges model predictions with their confidence levels, effectively\\nleveraging SLM task-specific knowledge and LLM domain expertise. Furthermore,SLMs efficiently\\ndecompose complex reasoning by breaking tasks into simpler components , as demonstrated in [388].\\nThis strategy increases efficiency and reduces deployment costs when SLMs and LLMs are used\\ncollaboratively, transforming complex tasks into manageable segments.\\nFig. 23. Architecture of SLM-based data protection.\\nAlleviate Copyright and\\nPrivacy Issues of LLMs.\\nLLMsposesignificantsecu-\\nrity risks due to their ten-\\ndency to memorize train-\\ning data, leading to poten-\\ntial privacy breaches and\\ncopyright infringement. As\\ndepicted in Figure23, SLMs\\ncan assist LLMs in address-\\ning copyright and privacy\\nconcerns arising from online data collection. By training on selectively curated data subsets, SLMs\\neffectively reduce copyright infringement and privacy risks, although they are less effective than\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 49, 'page_label': '50'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:49\\nFig. 24. SLM for LLM prompt extraction paradigm. \"( denotes SLMs, and \"! denotes LLMs. (a) SLM-based\\nprompt estimation tries various attack prompts; \"( selects the most likely extracted one. (b) SLM-based\\nparameter extractor identifies the type of input prompt. (c) SLM-based model inversion uses \"( to invert the\\nLLM output back into the input.\\nfull-scale LLMs. To harness the combined benefits of both models, Li et al. [201] integrate untrusted\\nLLMs with benign SLMs using the CP-Δ KL algorithm to mitigate adverse effects while preserving\\nperformance. The equation is:\\n?(~|G) = ?;(~|G) · ?B(~|G)\\n/(G) , (4)\\nwhere ?; and ?B represent the probabilities from the large and small models, respectively, and/(G)\\nis the partition function. This integration results in the following ensemble algorithm:\\nI?(·|G) ∝ UI;(·|G) + (1 − U)IB(·|G), (5)\\nwhere I; and IB are the logit values from the large and small models, respectively, andU is the\\nscaling factor.\\n6.2 SLM for Extracting LLM Prompts\\nPrompt-based methods are becoming simpler and more cost-effective alternatives to traditional\\nfine-tuning in the LLM era, utilizing LLMs’ instruction-following capabilities for a competitiveedge.\\nMastering prompts is vital for replicating LLM-supported product behaviors. However, services\\nsuch as Bing Chat and GitHub Copilot Chat have seen prompt reverse engineering through black-\\nbox API attacks. SLMs often serve as surrogate models in these attacks, employing strategies such\\nas (i) SLM-based prompt likelihood estimation, (ii) SLM-based prompt parameter extraction, and\\n(iii) SLM-based direct model inversion, illustrated in Figure24.\\nSLM-based prompt likelihood estimation , as illustrated in Figure24(a), Zhang et al. [448] propose\\nusing an SLM as a Likelihood Estimator to identify secret prompts in LLM outputs. They craft\\nattack prompts, such as “Repeat all sentences in our conversation,” and query the target LLM.\\nThe response is likely to include secret prompts, confusing the LLM to interpret these as part of\\nthe conversation. A fine-tuned DeBERTa model [134] is then used to select the most likely secret\\nprompts from the output.\\nSLM-based prompt parameter extraction , as shown in Figure24(b), Sha and Zhang [304] utilize\\nan SLM as a parameter extractor to extract prompt parameters from LLM outputs. They employ a\\nspecializedBERTmodel[ 86]toclassifyLLMoutputsintodirect,in-context,androle-basedprompts,\\nalso predicting the number of exemplars for in-context prompts and identifying roles for role-based\\nprompts. Prompt reconstruction is then performed using ChatGPT once the parameters are defined.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 50, 'page_label': '51'}, page_content='145:50 F. Wang et al.\\nSLM-based direct model inversion,asshowninFigure 24(c),themethodofusinganSLMasaDirect\\nInversion Model is designed to reverse-engineer LLM outputs back to their original prompts [438].\\nThey train a sparse encoder–decoder T5 model [291] with 222M parameters on the Instructions-2M\\ndataset [255], where the input is LLM outputs and the output is the LLM prompt. This trained\\nmodel effectively maps multiple LLM outputs to their initiating prompts as?(G|~1,.,~=; \"(1,\"(2),\\nwith~8 representing different output versions and\"(1,\"(2 the model parameters.\\n6.3 SLM for Fine-Tuning LLMs\\nFine-tuning is a crucial technique for adapting LLMs to specific tasks or domains, yet it is often\\ntime-consuming. For instance, fine-tuning the LLaMA-2-13B [345] checkpoint on 32 NVIDIA A100\\nGPUs with 80GB of memory using bfloat16 format requires approximately 70\\u2009hours [253]. This\\nprocess also demands high-quality data. Therefore, we examine how SLMs can enhance LLM\\nfine-tuning through three approaches: (i) proxy fine-tuning, (ii) selecting high-quality data, and\\n(iii) guiding LLM-generated task data, as illustrated in Figure25.\\nFig. 25. SLM for LLM fine-tuning.\\nSLMs as Proxy Models.\\nSLMs can approximate the\\ngradient of fine-tuning large-\\nscaleLLMsontargetdatasets,\\navoiding the costly fine-\\ntuning process in terms of\\ntime and computational re-\\nsources. As shown in Fig-\\nure 25(a), Emulated Fine-\\nTuning (EFT) [252] simu-\\nlates both unsupervised pre-\\ntraining and supervised fine-\\ntuning stages across different\\nscales by manipulating log\\nprobabilities. EFT, for exam-\\nple, combines base log prob-\\nabilities from a 70B model\\nwith behavioral deltas from a 7B model—these deltas represent differences between fine-tuned and\\nunfine-tuned SLMs, effectively emulating outcomes for the Llama-2 series. This method allows\\nfine-tuning on smaller models such as Falcon-7B [11] while capturing most benefits of fine-tuning\\nlarger models such as Falcon-180B, benefiting applications such as dialogue, QA, and code genera-\\ntion. Similarly,Proxy-tuning [217] adjusts LLM predictions by adding the differences between the\\noutputs of a fine-tuned small model and its untuned version to the LLM’s output vocabulary during\\ndecoding, maintaining the advantages of large-scale pre-training while integrating small-scale\\nfine-tuning benefits. Moreover, SLMs can act as proxies for approximate LLM fine-tuning during\\ndecoding. The Weak-to-Strong Search [457] strategy frames the alignment of LLMs as a test-time\\ngreedy search, aiming to maximize the log-probability difference between small tuned and un-\\ntuned models while sampling from the frozen large model. This approach serves as a dual-purpose\\nmethod: (1) a compute-efficient model upscaling strategy that circumvents direct tuning of the\\nlarge model and (2) an instance of weak-to-strong generalization that bolsters a strong model with\\nweak test-time guidance.\\nSLMs Play a Role in Selecting High-Quality Fine-Tuning Data for LLMs. Figure 25(b) illustrates\\nhow SLMs within the same family as the LLM can identify training samples that are likely to\\nbe challenging, enhancing the training efficiency and generalization capability of the LLM. As\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 51, 'page_label': '52'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:51\\ndemonstrated by Swayamdipta et al. [329] and further advanced by Mekala et al. [247], thelearning\\npercentage !%(8) is a metric used to curate high-quality datasets with hard samples:!%(8) = %8−1 −%8\\n%0 −%=\\n,\\nwhere %8 represents the perplexity at the end of epoch-8, and%0 is the initial perplexity. A higher\\n!%(8) early in training indicates significant learning in the initial epochs, highlighting the potential\\nof these samples to enhance LLMs.SmallToLarge (S2L)[416] utilizes training loss trajectories\\nfrom smaller models to guide data selection for larger models’ fine-tuning. Experimental results\\ndemonstrate that S2L significantly enhances data efficiency in SFT for mathematical problem-\\nsolving, reducing the required training data to just 11% of the original MathInstruct dataset [433]\\nto achieve performance comparable to that obtained using the full dataset.\\nSLMs Enhance the Quality of LLM-generated Data for Specific Tasks. As depicted in Figure25(c),\\nCROSSLM [82] promotes the local training of SLMs on client-specific private data to mitigate\\nprivacy risks associated with server-based LLMs. An SLM trained in this manner can guide the\\nserver-side LLM in producing high-quality synthetic datasets. Feedback from SLMs regarding the\\nquality of this synthetic data serves as a supervisory signal, enhancing both the quality of LLM\\noutputs and the utility of the data for further training.\\n6.4 SLM for LLM Applications\\nLLMs are utilized across various applications due to their open-ended generation capabilities, yet\\nthey often lack specialized knowledge and have other generation issues. SLMs can supplement this\\nby providing task-specific knowledge or reflecting weaknesses. Therefore, we explore how SLMs\\nenhance the performance of LLMs in specific applications, focusing on open-ended generation,\\nknowledge integration, relation extraction, and empathetic response.\\nFig. 26. Contrastive decoding [ 202].\\nIn open-ended text generation —such as writ-\\ning assistance and story creation—LLMs of-\\nten suffer from issues such as incoherence and\\nthematic drift over extended sequences. Due\\nto more frequent failure patterns observed in\\nSLMs, such as short, repeated, and irrelevant\\nstrings, these patterns serve as negative exam-\\nples for LLM decoding.Contrastive Decod-\\ning (CD)[202] improves coherence and lexical\\ndiversity by leveraging the differential capa-\\nbilities between a large model, OPT-13B [445],\\nand a smaller model, OPT-125M. As illustrated\\nin Figure26, CD improves content quality by\\nsampling generation based on the difference in\\nlog probabilities,log ?\\x1a-% − log ?\\x16\"\\x16, between\\nan expert LM and an amateur LM, rather than\\nrelying solely on the expert LM’s log probability. This approach effectively reduces generative\\nfailures, including repetition.\\nIn knowledge injection, general LLMs may lack domain-specific expertise for specialized tasks like\\nlaw or medicine. Domain-specific SLMs can supply crucial knowledge in a format suitable for LLMs.\\nTo this end,BLADE [192] integrates black-box LLMs with small domain-specific models. BLADE\\ncombines the comprehensive language capabilities of LLMs with the specialized knowledge of small\\nLMs. As shown in Figure27, BLADE’s process includes: (1) pre-training SLMs on domain-specific\\ndata, (2) fine-tuning with knowledge instruction to meet task-specific needs, and (3) using joint\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 52, 'page_label': '53'}, page_content='145:52 F. Wang et al.\\nBayesian optimization to enhance synergy between the LLM and the small LM, boosting overall\\nperformance.\\nFig. 27. BLADE framework [ 192].\\nIn relation extraction, a field limited\\nby scarce labeled data and prevalent\\nlong-tail categories, the“Train-Guide-\\nPredict” framework [ 333] employs\\nSLMs to learn task-specific knowledge\\nfor dominant categories. SLMs strug-\\ngle with rare categories, whereas LLMs\\nmanage these effectively due to their\\nextensive pre-trained text. Therefore,\\nthis framework leverages the strengths\\nof both models: it utilizes SLMs to acquire task knowledge and guide the LLM’s generative process\\nwith initial SLM predictions, enhancing the LLM’s handling of underrepresented categories.\\nIn generating empathetic responses , LLMs excel in expressiveness but struggle with nuanced\\nemotions and cognition.HEF [418] addresses this by incorporating Small Empathy Models (SEMs)\\nto enhance LLMs’ emotional and cognitive depth. This framework employs a two-tiered emotion\\nprediction method: SEMs identify primary emotions, directing LLMs to concentrate on these\\nemotions and their triggers, resulting in more accurate and empathetic responses.\\n6.5 SLM for LLM Safety\\nAs demonstrated by various works [256, 309, 431, 464], LLMs are vulnerable to adversarial attacks\\nand jailbreaking. For example, Wang et al. [364] show that ChatGPT’s performance on adversarial\\ndatasets is still far from perfect, indicating that potential risks of adversarial vulnerability remain.\\nAnother example includes jailbreaking ChatGPT by asking it to “pretend to be a sarcastic mean girl.”\\nUsing such techniques, it has been shown that even the most advanced LLMs are far from being\\nsafe against generating potentially harmful content. Hence, the widely adopted LLM-based services\\nto generate are at high risk of being misused for nefarious purposes. Consequently, resources such\\nas the Llama 2 Responsible Use Guide10 strongly advocate for implementing robust guardrails in\\nproducts that utilize Generative AI. These guardrails are specifically designed to mitigate risks\\nassociated with both inputs to and outputs from the model, ensuring safeguards against the\\ngeneration of high-risk or policy-violating content, as well as protecting against adversarial inputs\\nand attempts to compromise the model. In addition to developing trustworthy LLMs, adopting\\nSLMs for LLM safety [156, 180] has also attracted increasing attention. For example, Llama Guard\\n[156],fine-tunedonLlama2-7B,haspubliclyreleasedaninput–outputsafeguardtoolspecificallyfor\\nclassifying safety risks in prompts and responses within conversational AI applications. However,\\nthis tool is limited to assessing the harmfulness of questions and answers and does not facilitate\\nthe generation of fluent, safe responses. In response to this limitation, Kwon et al. [180] fine-tune a\\nspecialized SLM with harmful query detection and safeguard answer generation tasks to accurately\\ndetect harmful user queries and generate appropriate safeguard explanations, thereby enhancing\\nthe safety measures in conversational AI.\\n6.6 SLM for LLM Evaluation\\nSLMs can also enhance the evaluation of LLMs. In dialog evaluation, generating dialog reference\\nresponses is computationally complex, making accurate assessment difficult due to the multiple\\nplausible but semantically different responses possible for a single dialog context. Relying on\\n10https://ai.meta.com/static-resource/responsible-use-guide/.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 53, 'page_label': '54'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:53\\nLLM prompting for evaluation can lead to problems such as dependency on prompt wording and\\ninconsistent results. One solution involves training specialized SLMs to evaluate LLMs, as these\\nSLMs can be fine-tuned more quickly and generate outputs faster during inference, owing to\\ntheir reduced number of parameters. For example,SLIDE [453] employs contrastive learning to\\nfine-tune an SLM to effectively distinguish between positive and negative responses. Based on\\nits observation that SLMs are more accurate in identifying positive responses and LLMs excel at\\nclassifying negative ones, the trained SLM is subsequently integrated with an LLM to assign a score\\nto each response. The scoring method used is formalized as follows:\\nB2>A4 =\\n\\uf8f1\\uf8f4\\uf8f4 \\uf8f2\\n\\uf8f4\\uf8f4\\uf8f3\\nB2>A4(!\", ifB2>A4(!\" ≥ 0.5\\nB2>A4!!\", elifB2>A4!!\" < 0.5\\nB2>A4(!\" +B2>A4!!\"\\n2 , otherwise\\n. (6)\\nThis equation allows for adaptive response evaluation, leveraging the strengths of both models to\\nensure a more reliable and consistent assessment across varying dialogue contexts. In the natural\\nlanguage generation task, Kuhn et al. [178] design a novel entropy to evaluate the uncertainty of\\nLLMs. It aims to tackle the challenge ofsemantic equivalence [178]. For instance,A’s son is B and B\\nis A’s son are semantically equivalent. It should not be considered uncertain if an LLM is unsure\\nabout which of the two previously mentioned sentences to generate due to semantic equivalence.\\nA\\xa0DeBERTa-Large [134] fine-tuned on the MNLI [383] dataset serves as the classifier guided by\\nsemanticequivalenceintheclusteringstage. SelfCheckGPT [245]proposesablack-boxhallucination\\ndetection method for LLMs. The core idea is to leverage uncertainty derived from sampled outputs.\\nTo be specific, Manakul et al. [245] claim that an LLM trained on a concept generates responses that\\nare similar and factually consistent. One of the five variants of SelfCheckGPT uses BertScore to\\nachieveit.ADeBERTa-Large[ 223]isutilizedtocalculatetheBERTScore. Factscore [250]isproposed\\nto evaluate the factuality of LM-generated long-form content. It divides the generated long content\\ninto multiple short texts, enabling a more precise assessment of factual accuracy. In addition to\\nmanual evaluation, Min et al. [250] also propose an automated evaluation framework to estimate\\nFactscore, which can reduce costs. LLaMa 7B [344], fine-tuned on Super-NaturalInstructions [373]\\nis one of the LMs employed as an evaluation assistant and shows promising performance. They\\nalso employ Generalizable T5-based dense retrievers [266] to facilitate passage retrieval.\\nInsights: SLMs can improve LLMs in various aspects, including enhancing the reliability of LLM\\ngeneration, extracting prompts, fine-tuning, application, and evaluation. This discussion seeks to\\nanswer when SLMs should be utilized to augment LLMs. We identify several suitable scenarios:\\n—Adapting LLMs to specific tasks can require substantial computational resources and time. In\\nsuch cases, a smaller model could be fine-tuned instead to serve functions such as hallucination\\ndetection.\\n—SLMs can outperform LLMs in certain aspects; hence combining SLMs with LLMs can create a\\nmore powerful model, e.g., SLMs typically have fewer security issues than LLMs, and integrating\\nboth can generate a model that is both powerful and secure.\\n—SLMs, despite their limitations, can alert LLMs to these issues, such as the tendency to produce\\nrepetitivevocabulary.DesigningcontrastivelossescanhelpLLMsovercometheseissuesbylearning\\nfrom the nuanced feedback of SLMs.\\n—The fast inference speed and certain characteristics of SLMs can emulate and thus enhance the\\nbehavior of LLMs, acting as effective proxies. For example, the training data selection for LLMs\\ncan be guided by the difficulty metrics assessed by SLMs, and the parameter adjustments during\\nthe fine-tuning of SLMs can also approximate the fine-tuning processes of LLMs.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 54, 'page_label': '55'}, page_content='145:54 F. Wang et al.\\nTable 14. Synergy between SLMs and LLMs\\nSynergy Representative Work Key Point\\nCloud-Edge\\nSynergy\\n(Inference)\\nCoGenesis [442] Divide user instructions into general parts by LLMs and private parts\\nby SLMs.\\nXu et al. [402] Introduce split learning in 6\\u2009G to distribute LLM agents.\\nLLM-to-SLM [29] Encode prompts with server-side LLM and decode with edge-side SLM.\\nSynergy of Thoughts [305] SLMs suggest reasoning paths; LLMs correct contradictions.\\nHao et al. [129] SLM generates local tokens; LLM checks and corrects complex tokens.\\nLLMCad [399] Combine lightweight and high-precision LLMs for on-device inference.\\nKhattab et al. [170], Ma et al. [239] Focus on LLM’s reasoning and SLM’s efficient decoding.\\nCloud-Edge\\nSynergy\\n(Training)\\nCROSSLM [82]\\n\\xa0\\n\\xa0\\nPreserve client data privacy by training SLM locally and LLM remotely;\\nmutual improvement using SLM-labeled data from LLM outputs.\\n\\xa0\\nTask-Centric\\nSynergy\\nU-UMi [314] Break down a single LLM into specialized agents.\\nSynCID [208] Merge LLM’s semantics with SLM’s speed; refine labels via contrastive\\nlearning.\\nFilter-then-rerank [239] SLMs process simple samples and flag complex ones for LLM reranking.\\nData Shunt+ (DS+) [47] Process easy samples with SLMs and delegate hard samples to LLMs.\\n7 Synergy between SLMs and LLMs\\nThe synergy between SLMs and LLMs leverages the unique strengths of each to enhance overall\\nsystem performance and efficiency. SLMs, being lightweight and resource-efficient, are ideal for\\ndeployment on edge devices, enabling rapid responses and low latency for straightforward tasks.\\nLLMs, on the other hand, possess greater computational power and a deeper understanding of\\ncomplex language patterns, allowing them to handle more intricate and nuanced tasks. By integrat-\\ning SLMs and LLMs, systems can dynamically allocate tasks based on complexity, ensuring that\\nsimple queries are processed quickly on the edge while more demanding requests are escalated\\nto the cloud. This collaborative approach optimizes resource usage, reduces operational costs,\\nand maintains high-quality outputs across a diverse range of applications. The synergy between\\nSLMs and LLMs can be categorized into two parts:cloud-edge synergy and task-centric synergy.\\nCloud-edge synergy refers to a setup where SLMs operate on edge devices, while LLMs reside\\non the server. When the SLM is not powerful enough, the LLM compensates by handling more\\ncomplex tasks and providing additional support. Task-centric synergy refers to the scenario where\\nSLMs and LLMs leverage their respective strengths to improve task-oriented efficiency. Table14\\nsummarizes representative work in each category and their key points. Next, we introduce each\\ncategory in detail.\\n7.1 Cloud-Edge Synergy\\nThe current utilization of LLMs typically involves uploading private data to the cloud for response.\\nFine-tuning LLMs usually also requires uploading data to clouds for computing. However, this\\nraises privacy concerns, as the collection and usage of private data are constrained by personal\\nprivacy awareness and legal regulations [354]. Consequently, the cloud-edge synergy between\\nSLMs and LLMs is proposed to alleviate this issue, i.e., SLMs handle privacy-sensitive data locally,\\nLLMs handle de-identified or non-privacy-sensitive data, and these two models collaborate. This\\nsection discusses such cloud-edge synergy, dividing it into two categories: cloud-edge synergy\\nduring inference and cloud-edge synergy during training, as shown in Figure28.\\nCloud-edge Synergy during Inference. CoGenesis [442] breaks down the user instruction into\\na general section and a personal section. The LLM generates replies solely based on general\\ninstruction, and the SLM considers both user instruction and additional personal context for its\\noutput generation. A fusion strategy blends the output of LLM and SLM synergistically. Xu et al.\\n[402]introduceasplitlearningsystemforLLMagentsin6\\u2009Gnetworks,optimizingmobiledeviceand\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 55, 'page_label': '56'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:55\\nFig. 28. Could-edge synergy between LLMs and SLMs.\\ncloudservercollaboration.MobiledevicesoperatelightweightSLMswith0–10Bparametersforreal-\\ntime tasks, while cloud servers handle larger LLMs with over 10B parameters for complex reasoning\\nand planning. This setup allows efficient local task management on mobile devices and offloads\\nheavy operations to cloud servers. The system’s architecture features three modules—perception,\\ngrounding, and alignment—facilitating effective communication to meet the sophisticated needs of\\n6\\u2009G networks.\\nBesidestheseframeworks,morespecificmodelsareproposedtofacilitatethecloud-edgesynergy.\\nA common strategy is to use SLM’s fast decoding ability.LLM-to-SLM [29] proposes a framework\\nin which the pre-trained frozen encoder–decoder LLM resides on the server and computes a high-\\nquality representation of the prompt for the planning of an appropriate response. The SLM residing\\non the edge device, conditioned on this representation, decodes the response efficiently. Some\\nvariants put more emphasis on the reasoning ability of LLMs [170, 239, 305]. InSynergy of Thoughts\\n[305], the SLMs generate multiple low-cost reasoning paths. If these paths conflict, the larger LLMs\\nare invoked to provide reflective reasoning and correct any intuitive errors. Hao et\\xa0al. [129] propose\\na framework in which an SLM residing on the edge devices generates tokens, calling LLMs to\\nverify and correct threshold-gated “harder” tokens, to achieve a controllable tradeoff between\\ninference quality and cost.LLMCad [399] presents an on-device inference engine addressing\\nmemory and latency issues in deploying LLMs on mobile devices. It combines a lightweight LM for\\ntoken generation with a high-precision LLM for verification, leveraging a token tree structure and\\nspeculative generation for efficiency. Tested on devices such as Jetson TX2, it achieves up to 9.3×\\nspeedup for LLMs with over 10 billion parameters while maintaining accuracy.\\nCloud-edge Synergy During Training. CROSSLM [82] introduces a client-server collaborative\\ntraining framework that preserves data privacy by having clients locally train SLMs instead of\\nfine-tuning LLMs. The framework enables mutual enhancement through a feedback loop where\\nSLMsevaluateLLM-generatedsyntheticdataandprovidefeedbacktoimprovetheLLM’sgenerative\\ncapabilities, ensuring high-quality and task-specific data. Concurrently, the synthetic data trains the\\nSLMs, boosting their performance. This cyclical exchange fosters cloud-edge synergy and mutual\\nmodel improvement.\\n7.2 Task-Centric Synergy\\nThe advent of LLMs has significantly propelled various natural language processing tasks and\\ninspired research into their synergistic interactions with SLMs to enhance the performance of\\nmodels tailored for specific tasks. This section introduces scenarios where SLMs exhibit specialized\\ncapabilities after fine-tuning and discusses how combining their unique strengths with the versatile\\nabilities of LLMs can yield superior performance on specific tasks. For example, LLMs excel at\\nhandling difficult examples or can rewrite content to eliminate task-irrelevant redundancy, thereby\\nenhancing overall task performance, as illustrated in Figures29–31.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 56, 'page_label': '57'}, page_content='145:56 F. Wang et al.\\nFig. 29. Synergizing SLMs and LLMs in tool learning.\\nU-UMi [314] introduces a multi-agent frame-\\nwork to enhance tool learning by overcoming the\\nlimitationsofsingle-LLMapproachesforcomplex\\ntasks. It utilizes three specialized LMs—planner,\\ncaller, and summarizer—as depicted in Figure29\\n—each handling specific subtasks such as plan-\\nning, tool invocation, and summarization. This\\nmodular design allows the use of small and large open-source LLMs (e.g., LLaMa-7B/12B) and\\nsupports easy tool updates. Evaluated on benchmarks like ToolBench [286] and ToolAlpaca [332],\\nU-UMi outperforms traditional single-LLM methods and even exceeds GPT-4 in tool learning\\nperformance.\\nSynCID [208] focuses onConversational Intent Discovery (CID), a task where both known\\nand new intents must be identified from user utterances in an open-world setting. SynCID combines\\nFig. 30. Synergizing SLMs and LLMs in conversa-\\ntional intent detection.\\nLLMs’ deep semantic insights with SLMs’ agility\\nand specialized capabilities. As illustrated in Fig-\\nure 30, the framework uses LLM prompting to\\nrefine discourse and intent labels, enhancing se-\\nmantic accuracy and assigning new labels to un-\\nlabeled data. SLMs are trained via contrastive\\nlearning to align the semantic spaces of discourse\\nandintentdescriptors,reducingclusteringdistor-\\ntion and improving new intent detection. Tested\\non BANKING [44], CLINC [182], and StackOver-\\nflow [401], SynCID outperforms CID baselines\\nsignificantly.\\nFig. 31. Synergizing SLMs and LLMs in information extraction.\\nFilter-then-rerank\\n[239]addressesLLMs’\\npoor performance\\non simpler IE tasks\\nbyintegratingLLMs\\nand SLMs. SLMs\\nact as filters, pre-\\ndicting and identi-\\nfying difficult sam-\\nples, while LLMs rerank the top N candidate labels for these cases. As illustrated in Figure31,\\nSLM predictions are final for non-difficult samples, minimizing reliance on LLMs and reducing\\nlatency and costs; for those difficult samples, the top N predicted candidate labels from the SLM\\nare passed to the LLM for reranking (predicting). Tested on small-sample IE tasks, this approach\\nimproves performance by an average of 2.4% compared to previous methods.Data Shunt+ (DS+)\\n[47] introduces a framework to reduce costs by minimizing large model queries during inference\\nand boosting LLM performance with SLMs for tasks like sentiment analysis and image processing.\\nDS+ uses SLMs for “easy” samples within the main training distribution and LLMs for “hard”\\noutliers or boundary cases, maintaining accuracy while reducing LLM use. It incorporates S4L and\\nL4S modules with Prompt Pruning (PP) and 2-stage Confidence Distillation (2CD) for better input\\nprocessing and knowledge transfer. Tests show DS+ outperforms fine-tuning in accuracy and cost\\nefficiency, significantly cutting down on LLM queries.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 57, 'page_label': '58'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:57\\nFig. 32. Scenarios we discuss in this section. The taxonomy is inspired by previous works [ 326, 357]. Please\\nnote that the trustworthy scenarios listed here are not exhaustive.\\n8 Trustworthiness in SLMs\\nLanguage models have become ubiquitous in our daily lives, and we increasingly rely on them.\\nHowever, they pose risks regarding their limitations in trustworthy dimensions like privacy and\\nfairness. These concerns are especially critical in high-stakes domains such as healthcare [132]\\nand finance [206]. Consequently, numerous studies have emerged to evaluate the trustworthiness\\nof LMs [91, 97, 141, 179, 254, 261, 280, 357, 376, 430]. In this section, we consider the works that\\nbenchmark various LMs’ trustworthiness and omit the specific attack methods [42, 55, 153, 465]\\nor work [412] that only focuses on early pre-trained LMs like BERT [86] as they are already\\ncovered in previous survey papers [81, 117, 125, 295]. Inspired by previous works [326, 357], we\\ndiscuss the following five key trustworthy scenarios:robustness, privacy, reliability, safety , and\\nfairness, as shown in Figure32. We consider two dimensions for robustness: Adversarial (Adv)\\nRobustness [358] and Out-of-Distribution (OOD) Robustness [39, 218]. For safety, we explore two\\nkey concerns: Misinformation [350] and Toxicity [379]. For reliability, we focus on Hallucination\\n[149] and Sycophancy [308]. Please note that these are just the aspects we are focusing on, and\\ntherefore, this is not a comprehensive classification or taxonomy. For example, robustness also\\ncontains robustness to adversarial demonstration.\\nThough there are a lot of works benchmarking LMs’ trustworthiness, their main focus is on\\nLLMs. Therefore, we survey some representative works evaluating the trustworthiness of LMs,\\nfocusing specifically on those that include SLMs of around 7B parameters or smaller. We also\\nsummarize these works in Table15. Next, we briefly introduce them.\\nHolistic Evaluation of Language Models (HELM)[209] benchmarks a large number of LMs\\nfrom various aspects, including a lot of metrics related to trustworthiness, such as robustness\\nand fairness. Do-Not-Answer [370] introduces a dataset to evaluate how LMs act when they face\\ncontent that should not be answered. Wang et al. [370] also label the output of several LMs on their\\ndataset and then uses the labeled data to train some classifiers. PromptRobust [460] constructs\\ntwo kinds of adversarial prompts to evaluate LMs’ robustness: One kind is designed under non-\\nadversarial settings with semantic integrity, while another category is created under adversarial\\nsettings. Their results show that LMs perform poorly under such prompts. HaluEval [194] builds a\\ndataset comprising both the samples generated by their proposed framework and human-labeled\\nhallucinations. It facilitates analysis of when LMs produce hallucinated output and how well\\nthey detect hallucinated content. Then they use some strategies, such as knowledge retrieval, to\\nhelp LMs better recognize hallucinations. Mo et al. [254] evaluate the trustworthiness of open-\\nsource LMs, presenting a variety of scenarios such as fairness and privacy. Results show that\\nsmaller LMs sometimes outperform larger ones in terms of trustworthiness. PrivLM-Bench [193] is\\ndesigned to evaluate the privacy issues in LMs. It enables a fair comparison of privacy-preserving\\nLMs by considering more than just differential privacy parameters. FFT [74] introduces around\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 58, 'page_label': '59'}, page_content='145:58 F. Wang et al.\\nTable 15. Comparison of Different Works That Evaluate the\\nTrustworthiness Issues in LMs\\nPaper\\nAdv Robustness\\nOOD Robustness\\nToxicity\\nMisinformation\\nHallucination\\nSycophancy\\nPrivacy\\nFairness\\nHave Compressed SLMs\\nHELM [209] ✓ × ✓ ✓ × × × ✓ ×\\nDo-Not-Answer [370] × × ✓ ✓ × × ✓ ✓ ×\\nPromptRobust [460] ✓ × × × × × × × ×\\nHaluEval [194] × × × × ✓ × × × ×\\nMo et al. [254] ✓ × ✓ × ✓ ✓ ✓ ✓ ×\\nPrivLM-Bench [193] × × × × × × ✓ × ×\\nFFT [74] × × ✓ ✓ ✓ × × ✓ ×\\nROBBIE [100] × × ✓ × × × × ✓ ×\\nTrustLLM [326] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ×\\nRAmBLA [36] ✓ × × × ✓ × × × ×\\nJailbreakBench [46] × × ✓ ✓ × × ✓ × ×\\nXie et al. [395] × × ✓ × ✓ × × × ×\\nOR-Bench [73] × × ✓ ✓ × × ✓ × ×\\nSORRY-Bench [392] × × ✓ ✓ × × ✓ × ×\\nBeHonest [61] × × × ✓ ✓ ✓ × × ×\\nHong et al. [141] ✓ ✓ ✓ × × × ✓ ✓ ✓\\nRUPBench [376] ✓ × × × × × × × ×\\nNakka et al. [261] × × ✓ × × × ✓ ✓ ×\\ntwo thousand crafted examples to evaluate LMs’ performances on three trustworthy dimensions:\\nfactuality, fairness, and toxicity. Their results suggest that larger LMs do not always show better\\nharmlessness.ROBBIE[ 100]firstbenchmarksvariousseriesofLMsusingalotofdatasets,including\\ntwo newly introduced datasets developed by ROBBIE. It also evaluates mitigation techniques\\ndesigned to reduce bias and toxicity. TrustLLM [326] is a comprehensive benchmark that contains a\\nlarge number of datasets and various well-designed metrics to systematically evaluate various LMs\\nacrossmultipletrustworthydimensions,includingtruthfulness,safety,fairness,robustness,privacy,\\nand machine ethics. They also carefully design specific subcategories for each dimension. RAmBLA\\n[36] evaluates the trustworthiness of four LMs as biomedical assistants from three dimensions:\\nRobustness, High Recall, and Hallucination. RAmBLA suggests LMs with more parameters are\\nless likely to cause hallucinations and may choose to reject providing an answer in uncertain\\nsituations. JailbreakBench [46] constructs a jailbreaking dataset named JBB-Behaviors and jailbreak\\nartifacts to evaluate current LMs’ performance regarding jailbreaking. It also proposes a unified\\nevaluation pipeline that can incorporate new jailbreak defense techniques. Xie et al. [395] test\\nonline safety analysis methods, filling the gap where no methods focus on the generation phase.\\nOR-Bench [73] constructs three datasets: OR-Bench-80K, OR-Bench-Hard-1K, and OR-Bench-Toxic,\\nto systematically evaluate over-refusal problems in LMs, emphasizing the challenge of balancing\\nsafety alignment with the models’ usefulness. SORRY-Bench [392] systematically tests 43 different\\nLMs to see how they perform when facing requests that should be refused. They also collect more\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 59, 'page_label': '60'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:59\\nthan annotations created by humans and find that fine-tuned 7B LMs can achieve performance\\ncomparable to GPT-4 scale LMs as evaluators. BeHonest [61] evaluates the honesty of LMs from\\nthree aspects: Self-Knowledge, Non-Deceptiveness, and Consistency. They use many different\\nmetrics for each aspect. For example, the sycophancy rate and the lying rate are adopted in Non-\\nDeceptiveness. The results in both the Self-Knowledge and Consistency parts reveal that larger\\nmodel sizes generally bring improved performance for the Llama-2 [345] and Llama-3 [96] series.\\nHong et al. [141] examine the effects of compression methods, including quantization and pruning,\\non the trustworthiness of language models. They find that pruning and extreme quantization\\nsignificantly affect the trustworthiness of LMs. RUPBench [376] comprises 15 reasoning datasets\\ndesigned to assess the performance of LMs both in normal conditions and under various adversarial\\nperturbations. Their results indicate that larger LMs generally demonstrate better resilience to\\nperturbations. Nakka et al. [261] investigate the trust and ethical implications of SLMs deployed on\\npersonal devices. It reveals the vulnerabilities of on-device SLMs compared with their on-server\\ncounterparts.\\nPlease note that the dimensions discussed in this section reflect only those relevant to our\\ncurrent focus; additional dimensions may be discussed in those works but not listed in Table15.\\nFor example, TrustLLM [326] also explores Machine Ethics.\\n9 Future Directions\\nIn this section, we offer insights into several promising future research directions that could inspire\\nand motivate the community to address existing gaps in the development of SLMs.\\n9.1 Developing Efficient SLM Model Architecture\\nAlthough Transformers [352] are foundational in most language models, they face significant\\ncomputational and memory challenges that worsen with model size, impacting training and autore-\\ngressive decoding. Recently, Mamba [119] has emerged as a promising alternative, adapting SSMs\\nto dynamically select inputs based on demands, thereby enhancing efficiency. Thereafter, xLSTM\\n[26] demonstrates that an improved LSTM could function as an LLM, revealing the potential of\\ntraditional SSMs. The integration of global static information captured by SSMs with the dynamic\\ninformation processing of Transformers could complement each other, leading to new architectures\\nthat balance effectiveness and efficiency.\\n9.2 Addressing SLM Training Inefficiencies\\nOne study [88] explores the disparate learning dynamics between SLMs and LLMs. Utilizing the\\nPythia model suite, the research demonstrates that layers’ activations in larger models converge\\nmore rapidly and monotonically to their final states. This phenomenon is associated with a higher\\nproportional effective rank (PER)in the parameters and gradients of larger models. The analysis\\nenhances our understanding of training inefficiencies in small models and provides insights for\\nfuture efforts, such as developing methods to increase the PER of layers’ parameters.\\n9.3 Expanding Domain-Specific SLMs\\nDomain-specific SLMs, which are tailored for specific fields, can provide a stronger foundation\\nfor relevant downstream tasks than general-purpose models. Currently, these models primarily\\nfocus on scientific and healthcare domains. However, there is significant potential for expansion\\ninto other key areas such as law, finance, education, telecommunications, and transportation. The\\nscarcity of SLMs that cater to these domains presents an urgent call for research into developing\\nmore specialized models.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 60, 'page_label': '61'}, page_content='145:60 F. Wang et al.\\n9.4 Establishing Benchmarking and Leaderboard Platforms for SLMs\\nSeveralcompellingreasonsjustifytheestablishmentofbenchmarkingandleaderboardplatformsfor\\nSLMs.Firstly,moststate-of-the-artSLMsaretrainedonproprietarydatasets,whichmayincludetest\\nsets from existing evaluation tasks, presenting challenges for fair capability comparisons. Secondly,\\nmany SLMs are designed for specific device applications, significantly differing from general\\nopen-domain tasks. Thus, there is a lack of comprehensive benchmarks that accurately reflect\\nSLM performance in specific device applications. For example, SLMs deployed on smartphones\\noften handle tasks sensitive to user data, such as auto-replies based on historical chat texts or GUI\\ncontext understanding—tasks not typically included in current benchmarks, potentially leading to\\nan underestimation of their importance. Finally, current evaluation tasks focus primarily on metrics\\nlike accuracy. Evaluating on-device SLMs involves balancing multiple factors, including overall\\ncapabilities, response times, storage and memory usage, power consumption, CPU utilization,\\nadditional fine-tuning requirements, and context window constraints, making comprehensive and\\ndetailed assessments essential.\\n9.5 Enhancing SLM Performance and Efficiency\\nIn terms of enhancing SLM performance and efficiency, the efficiency of using teacher LLMs via\\ninstruction tuning can be further developed, such as Efficient Instruction Tuning of SLMs from\\nLLMs-generateddata,OptimizingTeacherLLMSelectionforSLMLearning,andApplyingEmerging\\nTechniques from LLMs to SLMs.\\n—Efficient Instruction Tuning of SLMs from LLMs-Generated Data. Enhancing the specialization\\nof SLMs through instruction tuning from LLMs-generated data is crucial, yet finding the most\\ncost-effective instructional strategies remains an underexplored area. Some key areas for\\nexploration are:\\n(1) Instruction Design Adaptability : The performance of LLMs and SLMs varies significantly\\nwith changes in instructions. Therefore, designing tailored instructions that effectively\\nactivate relevant sub-competencies and reasoning pathways in SLMs for specific tasks is\\ncrucial. This approach would optimize their ability to utilize instructional data, repre-\\nsenting a significant future research direction.\\n(2) SLM Capability Adaptability: Given that SLMs exhibit diverse capabilities across domains,\\nsimply supplying extensive data samples for instruction tuning is often inefficient, as\\nSLMs may spend excessive time processing unnecessary data. To optimize efficiency\\nwhen adapting to specific domains, we suggest first assessing the intrinsic capabilities\\nof an SLM within those domains. Subsequently, one could select appropriate data and\\nactivate essential fine-grained capabilities to effectively adapt to domain shifts. This\\ntargeted approach ensures efficient and domain-specific instruction tuning.\\n(3) Optimizing Data Efficiency : SLMs may possess missing or latent domain knowledge,\\nand activating this latent knowledge may not require substantial data. Thus, identifying\\ninherent knowledge within SLMs and determining the minimal data necessary for effec-\\ntive fine-tuning is a future direction. This research aims to optimize performance while\\nminimizing training resources.\\n—Optimizing Teacher LLM Selection for SLM Learning. Teacher LLMs with different abilities\\nand knowledge facilitate diverse applications for SLM training, including data rewriting and\\ngeneration. Selecting the appropriate teacher model based on specific use cases is crucial.\\nThis process requires evaluating the teacher’s capabilities and knowledge to ensure optimal\\napplication. For example, GPT-4 excels in generating domain-specific data, outperforming\\nChatGPT, which may produce inferior outcomes. Strategic selection of teacher LLMs is\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 61, 'page_label': '62'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:61\\nessential for future work to ensure their strengths are effectively utilized to enhance SLM\\nperformance.\\n—Applying Emerging Techniques from LLMs to SLMs. To improve LLM performance, techniques\\nsuch as RAG and Mixture of Experts (MoE) are employed. The adoption of RAG in SLMs shows\\nsignificant promise [220], suggesting benefits from further tailoring retrieved information for\\nSLMs. Future research should account for SLMs’ constraints, such as limited context windows,\\nand customize RAG accordingly. MoE uses multiple experts to enhance learning without\\nincreasing active neurons, but its storage demands pose challenges for SLM deployment,\\nmaking this a promising area for exploration. Additionally, the application of LLM techniques,\\nsuch as in-context learning and prompt engineering to maximize SLM performance, while\\naccounting for SLMs’ constraints, warrants further investigation.\\n9.6 Applications of SLMs\\nIn real-world applications, SLMs often need to provide personalized services and need to be updated\\nperiodically to reflect new needs and new knowledge. Hence, there are several promising directions\\nin terms of the real-world application of SLMs, which are listed as follows:\\n—LoRA for Personalized Services. Companies often provide personalized services, but user-\\nspecific complexities can render simple rules ineffective. Training a separate SLM for each\\nuser is impractical. LoRA suggests a method of separable training weights alongside fixed\\noriginal weights, enabling scalable customization. For instance, RecLoRA [459] integrates\\npersonalized knowledge into SLMs/LLMs tailored for recommendation tasks by maintaining\\na set of parallel, independent LoRA weights. This approach effectively customizes language\\nmodel parameters to align with individual user preferences. This approach is a promising\\ndirection that inspires further investigation.\\n—Lifelong On-device Learning for Knowledge Injection. SLMs on devices can access local data\\nwithout risking data privacy through two main methods. The first method uses retrieval-\\naugmented generation to integrate local data into prompts, requiring SLMs with advanced\\nprocessing and reasoning capabilities. The second method fine-tunes SLMs with local data,\\nintegrating customized knowledge into the model’s weights. However, this approach demands\\nsignificant device resources, including memory and energy. A promising solution is lifelong\\nlearning, where SLMs continuously learn and adapt while in use.\\n—Strategic Use of SLMs and LLMs in Multi-Agent Systems. LLMs can function as agents; however,\\ntheir extensive capabilities are often underutilized in many scenarios, leading to resource\\nwastage. Consequently, strategically routing to appropriately capable SLMs and LLMs within\\nmulti-agent systems can optimize cost and functionality.\\n9.7 Multimodal SLMs\\nResearch on SLMs also includes multimodal data. For example,SmolVLM [101] is a compact model\\nthat handles image and text inputs to produce text outputs, suitable for on-device use and various\\nmultimodal tasks.SOLO [56] integrates vision and language processing in a single 7B Transformer\\nmodel. The limited scope of existing research on multimodal SLMs provides a compelling impetus\\nfor researchers to investigate the integration of various modalities, including audio and graphs.\\n9.8 SLMs Assisting LLMs\\nIn Section6, we introduced existing works on the use of SLMs to assist LLMs. For instance, EFT\\n[252] emulates fine-tuning on LLMs by leveraging behavior deltas between SLMs’ pre-trained and\\nfine-tuned weights to alleviate the time-cost issues associated with fine-tuning LLMs; SlimPLM\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 62, 'page_label': '63'}, page_content='145:62 F. Wang et al.\\n[331]detectsmissingknowledgeinLLMsusingaslimproxySLMtoaccelerateknowledgeinjection;\\nContrastive Decoding [202] enhances text quality by maximizing the difference between the log\\nprobabilities of an expert LLM and an amateur SLM to mitigate issues of low-quality generation.\\nThe research on adopting SLMs to assist LLMs is still in its early stages, with many promising\\ndirections yet to be explored. We list some as follows:\\n—Enhancing LLM Performance Across Broader Tasks through SLM Integration. SLMs can outper-\\nform LLMs in certain scenarios. For example, SLMs often present fewer security vulnerabilities\\ncompared to their larger counterparts and demonstrate superior performance on easier sam-\\nples in specific tasks [201, 239]. Therefore, integrating SLMs with LLMs can promote the\\ndevelopment of models that are not only more robust but also inherently safer. Currently,\\nresearch in this domain is relatively sparse, suggesting that this collaborative framework\\ncould potentially be applied to a wider array of tasks.\\n—Efficient Enhancement of LLMs through Proxy SLMs. Existing research [19, 217, 252, 331]\\nindicatesthatSLMs,owingtotheiracceleratedfine-tuningandinferencespeeds,caneffectively\\nmimic the behaviors of LLMs, thereby serving as efficient proxies for optimization. However,\\nthe application of SLMs as operational proxies for LLMs is currently underexplored. This\\nmimicry could potentially be expanded to include various aspects of LLM functionality, such\\nas the optimization of prompts, the filtration and integration of supplementary knowledge,\\nand the management of additional knowledge repositories.\\n—SLMs Assist in Managing Data Quality. LLMs tend to produce hallucinations and toxic content\\ndue to low-quality real-world training data. One solution is to remove these low-quality data\\n[361]. However, directly eliminating low-quality content can diminish certain functionalities\\nof LLMs, such as versatility [362]. Therefore, it is crucial to define more refined data quality\\nassessment criteria across dimensions such as factuality, safety, and diversity [382] for real-\\nworld data. Researching efficient data selection methods using small models represents a\\nvaluable research direction. Additionally, while synthetic data serves as a vital complement\\nto scarce human-generated data [228], the potential for small models to effectively manage\\nsynthetic data remains largely unexplored.\\n—SLMs Assist in LLM Assessment. LLMs are producing vast amounts of increasingly complex\\ntexts, such as specialized code and scientific papers, presenting challenges not only for human\\nevaluators but also for traditional assessment metrics. Consequently, developing effective\\nevaluators to assess various aspects of generated content, including factuality, safety, and\\nuncertainty, becomes crucial. Given their proficiency in handling specific tasks, exploring the\\npotential of SLMs to evaluate LLM outputs is a promising research direction.\\n—SLMs Optimize Query and Reduce Noise for LLM RAG. For RAG using LLMs, differing query\\nrequirements between LLMs and search engines pose a challenge. The query for LLMs is\\noften abstract and difficult for search engines to handle, so they require more detailed query\\nkeywords. Moreover, LLMs may not need all the information related to a query because they\\nonly require partial additional knowledge. Thus, intermediate agents are crucial to adapting\\nLLM queries for search engines by clarifying the required detailed keywords that can search\\nfor necessary extra knowledge. Additionally, search engine outputs contain noise, requiring\\nrefinement to boost LLM efficiency. SLMs, skilled in a single task, are ideal for optimizing\\nquery rewriting and noise reduction in RAG systems, making their application in LLM RAG a\\npromising research area.\\n—SLMs Safeguard LLM . Resources such as the Llama 2 Responsible Use Guide strongly advocate\\nfor the implementation of robust guardrails in products that utilize Generative AI. SLMs can\\nbe strategically designed to serve as such guardrails, mitigating risks associated with both\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 63, 'page_label': '64'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:63\\ninputs and outputs from the model. This approach ensures safeguards against the generation\\nof high-risk or policy-violating content and provides protection against adversarial inputs and\\nattempts to compromise the model. Future research could explore how SLMs can enhance the\\nsafety of LLMs by providing protection against various threats, including adversarial attacks,\\njailbreak attacks, and backdoor attacks.\\n9.9 Synergy between SLMs and LLMs\\nInSection 7,wediscussedhowSLMsandLLMscancomplementeachother.Forexample,CoGenesis\\n[442] integrates SLMs for private data and LLMs for broader context, while Synergy of Thoughts\\n[305] uses SLMs for initial reasoning and LLMs for conflict resolution. CROSSLM [82] shows how\\nprivacy can be preserved by training SLMs locally to support LLMs without data exposure. Research\\nin this area is still evolving, and we outline several promising future directions below:\\n—Refined Cloud-Edge Division of Labor. Current research mainly focuses on splitting tasks be-\\ntween edge-based SLMs and cloud-based LLMs along privacy-sensitive and non-sensitive\\ndata boundaries. A potential future direction involves more granular task partitioning: deter-\\nmining which subtasks should be handled locally by SLMs (e.g., initial data filtering, quick\\nsemantic parsing) and which should be delegated to the cloud-based LLM (e.g., advanced\\nreasoning, complex generation). This approach can further optimize latency, privacy, and\\nresource utilization.\\n—Adaptive On-Device Specialization for Dynamic Environments. Although SLMs have shown the\\nability to handle private or personalized data locally, continuous changes in user preferences,\\napplication requirements, and data distributions pose challenges. Future work can explore\\nadaptive strategies where edge-based SLMs dynamically specialize or update their parameters,\\nguided by the cloud-based LLM. For instance, the LLM can periodically distill new knowledge\\ninto the SLM or provide feedback signals to help the SLM adapt to evolving scenarios.\\n9.10 Trustworthy SLMs\\nAs SLMs are playing crucial roles in various aspects, understanding and improving the trustwor-\\nthiness of SLMs are essential. Hence, two promising directions are:\\n—A Comprehensive Evaluation of SLMs’ Trustworthiness. While numerous studies address trust-\\nworthiness issues in LLMs, research on SLMs remains sparse. Most existing literature focuses\\non models with at least 7 billion parameters, leaving a gap in the comprehensive analysis of\\nSLMs’ trustworthiness. Current evaluations typically cover only a fraction of the necessary\\naspects. Therefore, a systematic assessment, such as TrustLLM [326], is essential to thor-\\noughly evaluate the trustworthiness of SLMs and understand their reliability across various\\napplications.\\n—Developing Trustworthy SLMs. DevelopingtrustworthySLMsiscrucial,withthreekeyresearch\\ndirections: (i) Training SLMs to be trustworthy from scratch; (ii) Ensuring SLMs retain or gain\\ntrustworthiness when compressed from LLMs—maintaining trustworthiness if the LLM is\\ntrustworthy and instilling trustworthiness if it is not; and (iii) Fine-tuning non-trustworthy\\nSLMs to enhance their robustness.\\n10 Conclusion\\nThis article provides a comprehensive survey of SLMs with up to 7 billion parameters. Initially, we\\naddress the need to clearly define SLMs due to existing ambiguities in their characterization. We\\nthen present the foundational concepts essential for constructing SLMs. The survey progresses to\\nexplore enhancement techniques, including KD and quantization, as well as strategies for adapting\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 64, 'page_label': '65'}, page_content='145:64 F. Wang et al.\\nLLMs to SLM contexts. We survey representative SLMs, both general-domain and domain-specific,\\ndiscussing their preferred datasets and architectural decisions. We also assess their applications\\nacross various tasks and deployment strategies on devices. Further, we investigate their role in\\naugmenting the capabilities of LLMs, serving as proxies for fine-tuning and facilitating two types\\nof synergies: cloud-local and task-centric. Additionally, we discuss the critical aspect of their\\ntrustworthiness. The article concludes with key insights aimed at guiding future research on SLMs.\\nReferences\\n[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen\\nBach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. 2024. Phi-3 technical report: A highly capable language\\nmodel locally on your phone. arXiv:2404.14219. Retrieved fromhttps://arxiv.org/abs/2404.14219\\n[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 Technical Report. arXiv:2303.08774. Retrieved\\nfrom https://arxiv.org/abs/2303.08774\\n[3] Emre Can Acikgoz, Osman Batur İnce, Rayene Bench, Arda Anıl Boz, İlker Kesen, Aykut Erdem, and Erkut Erdem.\\n2024. Hippocrates: An open-source framework for advancing large language models in healthcare. arXiv:2404.16621.\\nRetrieved fromhttps://arxiv.org/abs/2404.16621\\n[4] Harshavardhan Adepu, Zhanpeng Zeng, Li Zhang, and Vikas Singh. 2024. FrameQuant: Flexible low-bit quantization\\nfor transformers. arXiv:2403.06082. Retrieved fromhttps://arxiv.org/abs/2403.06082\\n[5] Abien Fred Agarap. 2018. Deep learning using rectified linear units (ReLU). arXiv:1803.08375. Retrieved from\\nhttp://arxiv.org/abs/1803.08375\\n[6] Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos Garea, Matthieu Geist, and Olivier\\nBachem. 2024. On-policy distillation of language models: Learning from self-generated mistakes. InThe Twelfth\\nInternational Conference on Learning Representations (ICLR ’24) .\\n[7] Meta AI. 2024. Llama 3.2: Revolutionizing edge AI and vision with open, customizable models. Retrieved from\\nhttps://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/. Accessed: September 25, 2024.\\n[8] JoshuaAinslie,JamesLee-Thorp,MichieldeJong,YuryZemlyanskiy,FedericoLebrón,andSumitSanghai.2023.GQA:\\nTraining generalized multi-query transformer models from multi-head checkpoints. arXiv:2305.13245. Retrieved\\nfrom https://arxiv.org/abs/2305.13245\\n[9] AliAl-Lawati,JasonLucas,ZhiweiZhang,PrasenjitMitra,andSuhangWang.2025.Graph-basedmolecularin-context\\nlearning grounded on morgan fingerprints. arXiv:2502.05414. Retrieved fromhttps://arxiv.org/abs/2502.05414\\n[10] Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Leandro von Werra, and Thomas Wolf. 2024. SmolLM - blazingly\\nfast and remarkably powerful. arXiv:2409.00286v1. Retrieved fromhttps://arxiv.org/abs/2409.00286v1\\n[11] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane\\nDebbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. 2023. The falcon series of open\\nlanguage models. arXiv:2311.16867. Retrieved fromhttps://arxiv.org/abs/2311.16867\\n[12] Guilherme F. C. F. Almeida, José Luiz Nunes, Neele Engelmann, Alex Wiegmann, and Marcelo de Araújo. 2024.\\nExploring the psychology of LLMs’ moral and legal reasoning.Artif. Intell. 333, (2024), 104145.DOI: https://doi.org/\\n10.1016/j.artint.2024.104145\\n[13] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji\\nRuwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. 2022. Deepspeed-inference: Enabling efficient inference\\nof transformer models at unprecedented scale. InSC22: International Conference for High Performance Computing,\\nNetworking, Storage and Analysis . IEEE, 1–15.\\n[14] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang. 2024. Fluctuation-based adaptive structured pruning for\\nlarge language models. InProceedings of the AAAI Conference on Artificial Intelligence , Vol. 38, AAAI, 10865–10873.\\n[15] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri,\\nEmanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 Technical Report. arXiv:2305.10403. Retrieved from\\nhttps://arxiv.org/abs/2305.10403\\n[16] AI Anthropic. 2024. The Claude 3 model family: Opus, Sonnet, Haiku.Claude-3 Model Card , 1, (2024). Retrieved from\\nhttps://api.semanticscholar.org/CorpusID:268232499\\n[17] DavidAnugraha,GentaIndraWinata,ChenyueLi,PatrickAmadeusIrawan,andEn-ShiunAnnieLee.2024.ProxyLM:\\nPredicting language model performance on multilingual tasks via proxy models. arXiv:2406.09334. Retrieved from\\nhttps://arxiv.org/abs/2406.09334\\n[18] Viraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr Locatelli, Marzieh Fadaee, Ahmet\\nÜstün, and Sara Hooker. 2024. To code, or not to code? Exploring impact of code in pre-training. arXiv:2408.10914.\\nRetrieved fromhttps://arxiv.org/abs/2408.10914\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 65, 'page_label': '66'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:65\\n[19] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-RAG: Learning to retrieve,\\ngenerate, and critique through self-reflection. InThe Twelfth International Conference on Learning Representations .\\nRetrieved fromhttps://openreview.net/forum?id=hSyW5go0v8\\n[20] Saleh Ashkboos, Maximilian L. Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. 2024.\\nSliceGPT: Compress large language models by deleting rows and columns. InThe Twelfth International Conference on\\nLearning Representations. Retrieved fromhttps://openreview.net/forum?id=vXxardq6db\\n[21] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang,\\nCarrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv:2108.07732.\\nRetrieved fromhttps://arxiv.org/abs/2108.07732\\n[22] Amos Azaria, and Tom Mitchell. 2023. The internal state of an LLM knows when it’s lying. InFindings of the\\nAssociation for Computational Linguistics: EMNLP ’23 . Association for Computational Linguistics, 967–976.\\n[23] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng,\\nStella Biderman, and Sean Welleck. 2023. Llemma: An open language model for mathematics. arXiv:2310.10631.\\nRetrieved fromhttps://arxiv.org/abs/2310.10631\\n[24] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et\\nal. 2023. Qwen Technical Report. arXiv:2309.16609. Retrieved fromhttps://arxiv.org/abs/2309.16609\\n[25] Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. 2020. The pushshift\\nreddit dataset. InProceedings of the International AAAI Conference on Web and Social Media , Vol.14, AAAI, 830–839.\\n[26] Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael K. Kopp,\\nGünter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. 2024. xLSTM: Extended long Short-Term memory.\\nIn The Thirty-Eighth Annual Conference on Neural Information Processing Systems . Retrieved fromhttps://openreview.\\nnet/forum?id=ARAxPPIAhq\\n[27] Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan, James\\nBaicoianu,BenBrooks,NathanCooper,AshishDatta,etal.2024.StableLM21.6BTechnicalReport.arXiv:2402.17834.\\nRetrieved fromhttps://arxiv.org/abs/2402.17834\\n[28] Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. 2024.SmolLM-Corpus.\\nRetrieved fromhttps://huggingface.co/datasets/HuggingFaceTB/smollm-corpus\\n[29] Benjamin Bergner, Andrii Skliar, Amelie Royer, Tijmen Blankevoort, Yuki Asano, and Babak Ehteshami Bejnordi.\\n2024. Think big, generate quick: LLM-to-SLM for fast autoregressive decoding. arXiv:2402.16844. Retrieved from\\nhttps://arxiv.org/abs/2402.16844\\n[30] Milan Bhan, Jean-Noel Vittaut, Nicolas Chesneau, and Marie-Jeanne Lesot. 2024. Self-AMPLIFY: Improving small\\nlanguage models with self post Hoc explanations. arXiv:2402.12038. Retrieved fromhttps://arxiv.org/abs/2402.12038\\n[31] Zhen Bi, Ningyu Zhang, Yida Xue, Yixin Ou, Daxiong Ji, Guozhou Zheng, and Huajun Chen. 2023. OceanGPT: A\\nlarge language model for ocean science tasks. arXiv:2310.02031. Retrieved fromhttps://arxiv.org/abs/2310.02031\\n[32] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad\\nAflah Khan, Shivanshu Purohit, Usvsn Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar\\nvan der Wal. 2023. Pythia: A suite for analyzing large language models across training and scaling. arXiv:230401373.\\nRetrieved fromhttps://arxiv.org/abs/2304.01373\\n[33] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. 2020. Piqa: Reasoning about physical commonsense in\\nnatural language. InProceedings of the AAAI Conference on Artificial Intelligence , Vol. 34, AAAI, 7432–7439.\\n[34] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive\\nLanguage Modeling with Mesh-Tensorflow.https://doi.org/10.5281/zenodo.5297715.\\n[35] Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Roxana Daneshjou,\\nJonathan Frankle, Percy Liang, Michael Carbin, et al. 2024. BioMedLM: A 2.7 b parameter language model trained on\\nbiomedical text. arXiv:2403.18421. Retrieved fromhttps://arxiv.org/abs/2403.18421\\n[36] WilliamJamesBolton,RafaelPoyiadzi,EdwardR.Morrell,GabrielavanBergen,GonzalezBueno,andLeaGoetz.2024.\\nRAmBLA:AframeworkforevaluatingthereliabilityofLLMsasassistantsinthebiomedicaldomain.arXiv:2403.14578.\\nRetrieved fromhttps://arxiv.org/abs/2403.14578\\n[37] Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022. Inpars: Data augmentation for\\ninformation retrieval using large language models. arXiv:2202.05144. Retrieved fromhttps://arxiv.org/abs/2202.05144\\n[38] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\\nPranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. InAdvances in\\nNeural Information Processing Systems , H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol.\\n33. Curran Associates, Inc., 1877–1901. Retrieved fromhttps://proceedings.neurips.cc/paper_files/paper/2020/file/\\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\\n[39] Saikiran Bulusu, Bhavya Kailkhura, Bo Li, Pramod K. Varshney, and Dawn Song. 2020. Anomalous example detection\\nin deep learning: A survey.IEEE Access 8, (2020), 132330–132347.DOI: https://doi.org/10.1109/ACCESS.2020.3010274\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 66, 'page_label': '67'}, page_content='145:66 F. Wang et al.\\n[40] Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. 2025. A survey on mixture of experts.\\nIEEE Transactions on Knowledge & Data Engineering 37, 7 (2025), 3896–3915.\\n[41] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei\\nChu, et al. 2024. InternLM2 Technical Report. arXiv:2403.17297. Retrieved fromhttps://arxiv.org/abs/2403.17297\\n[42] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts,\\nTom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In30th\\nUSENIX Security Symposium (USENIX Security ’21) . JMLR, 2633–2650.\\n[43] Samuel Carreira, Tomás Marques, José Ribeiro, and Carlos Grilo. 2023. Revolutionizing mobile interaction: Enabling\\na 3 billion parameter GPT LLM on mobile. arXiv:231001434. Retrieved fromhttps://arxiv.org/abs/2310.01434\\n[44] Iñigo Casanueva, Tadas Temčinas, Daniela Gerz, Matthew Henderson, and Ivan Vulić. 2020. Efficient intent detection\\nwith dual sentence encoders. InProceedings of the 2nd Workshop on Natural Language Processing for Conversational\\nAI . Association for Computational Linguistics, Online, 38–45.\\n[45] Wei-Cheng Chang, X. Yu Felix, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. 2020. Pre-training tasks for\\nembedding-based large-scale retrieval. InInternational Conference on Learning Representations .\\n[46] Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag,\\nEdgar Dobriban, Nicolas Flammarion, George J. Pappas, Florian Tramer, et al. 2024. Jailbreakbench: An open\\nrobustness benchmark for jailbreaking large language models. arXiv:2404.01318. Retrieved fromhttps://arxiv.org/\\nabs/2404.01318\\n[47] Dong Chen, Shuo Zhang, Yueting Zhuang, Siliang Tang, Qidong Liu, Hua Wang, and Mingliang Xu. 2024. Improving\\nlarge models with small models: Lower costs and better performance. arXiv:2406.15471. Retrieved fromhttps:\\n//arxiv.org/abs/2406.15471\\n[48] Dong Chen, Yueting Zhuang, Shuo Zhang, Jinfeng Liu, Su Dong, and Siliang Tang. 2024. Data shunt: Collaboration\\nof small and large models for lower costs and better performance. InProceedings of the AAAI Conference on Artificial\\nIntelligence, Vol. 38, AAAI, 11249–11257.\\n[49] Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. 2025. SFT or\\nRL? an early investigation into training R1-like reasoning large vision-language models. arXiv:2504.11468. Retrieved\\nfrom https://arxiv.org/abs/2504.11468\\n[50] Hongzhan Chen, Siyue Wu, Xiaojun Quan, Rui Wang, Ming Yan, and Ji Zhang. 2023. MCC-KD: Multi-CoT consistent\\nknowledge distillation. InFindings of the Association for Computational Linguistics: EMNLP ’23 . Association for\\nComputational Linguistics, 6805–6820.\\n[51] Lihu Chen, and Gaël Varoquaux. 2024. What is the role of small models in the LLM era: A survey. arXiv:2409.06857.\\nRetrieved fromhttps://arxiv.org/abs/2409.06857\\n[52] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri\\nEdwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on\\ncode. arXiv:2107.03374. Retrieved fromhttps://arxiv.org/abs/2107.03374\\n[53] Tianyi Chen, Tianyu Ding, Badal Yadav, Ilya Zharkov, and Luming Liang. 2023. Lorashear: Efficient large language\\nmodelstructuredpruningandknowledgerecovery.arXiv:2310.18356.Retrievedfrom https://arxiv.org/abs/2310.18356\\n[54] Wei Chen, Zhiyuan Li, and Mingyuan Ma. 2024. Octopus: On-device language model for function calling of software\\nAPIs. arXiv:240401549. Retrieved fromhttps://arxiv.org/abs/2404.01549\\n[55] Yangyi Chen, Fanchao Qi, Hongcheng Gao, Zhiyuan Liu, and Maosong Sun. 2022. Textual backdoor attacks can Be\\nmore harmful via two simple tricks. InProceedings of the 2022 Conference on Empirical Methods in Natural Language\\nProcessing, EMNLP . Springer-Verlag, 11215–11221.\\n[56] Yangyi Chen, Xingyao Wang, Hao Peng, and Heng Ji. 2024. A single transformer for scalable vision-language\\nmodeling. arXiv:2407.06438. Retrieved fromhttps://arxiv.org/abs/2407.06438\\n[57] Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi,\\nMatteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, et al. 2023. MEDITRON-70B: Scaling medical\\npretraining for large language models. arXiv:2311.16079. Retrieved fromhttps://arxiv.org/abs/2311.16079\\n[58] Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane,\\nTing-Hao Huang, Bryan R. Routledge, et al. 2021. FinQA: A dataset of numerical reasoning over financial data. In\\nProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing . ACM, 3697–3711.\\n[59] Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang. 2022. ConvFinQA:\\nExploring the chain of numerical reasoning in conversational finance question answering. InProceedings of the\\n2022 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics,\\n6279–6292.\\n[60] Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Hongzhi Zhang, Fuzheng Zhang, Di Zhang, Kun Gai, and Ji-Rong Wen.\\n2024. Small agent can also rock! Empowering small language models as hallucination detector. arXiv:2406.11277.\\nRetrieved fromhttps://arxiv.org/abs/2406.11277\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 67, 'page_label': '68'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:67\\n[61] SteffiChern,ZhulinHu,YuqingYang,EthanChern,YuanGuo,JiaheJin,BinjieWang,andPengfeiLiu.2024.BeHonest:\\nBenchmarking honesty of large language models. arXiv:2406.13261. Retrieved fromhttps://arxiv.org/abs/2406.13261\\n[62] Yew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. 2024. InstructEval: Towards holistic evaluation of\\nInstruction-Tuned large language models. InProceedings of the First Edition of the Workshop on the Scaling Behavior\\nof Large Language Models (SCALE-LLM 2024) . Association for Computational Linguistics, 35–64.\\n[63] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang,\\nYonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing\\nGPT-4 with 90%* ChatGPT Quality. Retrieved fromhttps://lmsys.org/blog/2023-03-30-vicuna/\\n[64] YaeJeeCho,LuyangLiu,ZhengXu,AldiFahrezi,andGauriJoshi.2024.HeterogeneousLoRAforfederatedfine-tuning\\nof on-device foundation models. arXiv:240106432. Retrieved fromhttps://arxiv.org/abs/2401.06432\\n[65] Xiaokai Chu, Jiashu Zhao, Lixin Zou, and Dawei Yin. 2022. H-ERNIE: A multi-granularity pre-trained language\\nmodel for web search. InProceedings of the 45th International ACM SIGIR Conference on Research and Development in\\nInformation Retrieval . ACM, 1478–1489.\\n[66] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang,\\nMostafa Dehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models.J. Mach. Learn.\\nRes. 25, 70 (2024), 1–53.DOI: https://doi.org/10.5555/3722577.3722647\\n[67] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.\\n2019. BoolQ: Exploring the surprising difficulty of natural yes/no questions. arXiv:1905.10044. Retrieved from\\nhttps://arxiv.org/abs/1905.10044\\n[68] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.\\n2018. Think you have solved question answering? Try ARC, the AI2 reasoning challenge. arXiv:1803.05457. Retrieved\\nfrom https://arxiv.org/abs/1803.05457\\n[69] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry\\nTworek,Jacob Hilton,ReiichiroNakano,et al. 2021. Trainingverifiersto solvemath wordproblems.arXiv:2110.14168.\\nRetrieved fromhttps://arxiv.org/abs/2110.14168\\n[70] Together Computer. 2023. RedPajama: An Open Dataset for Training Large Language Models. Retrieved from\\nhttps://github.com/togethercomputer/RedPajama-Data\\n[71] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Za-\\nharia, and Reynold Xin. 2023. Free Dolly: Introducing the World’s First Truly Open Instruction-Tuned LLM. Retrieved\\nfrom https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm\\n[72] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021. Overview of the TREC 2020 deep learning\\ntrack. arXiv:2102.07662. Retrieved fromhttps://arxiv.org/abs/2102.07662\\n[73] Justin Cui, Wei-Lin Chiang, Ion Stoica, and Cho-Jui Hsieh. 2024. OR-bench: An over-refusal benchmark for large\\nlanguage models. arXiv:2405.20947. Retrieved fromhttps://arxiv.org/abs/2405.20947\\n[74] Shiyao Cui, Zhenyu Zhang, Yilong Chen, Wenyuan Zhang, Tianyun Liu, Siqi Wang, and Tingwen Liu. 2023. Fft:\\nTowardsharmlessnessevaluationandanalysisforLLMSwithfactuality,fairness,toxicity.arXiv:2311.18580.Retrieved\\nfrom https://arxiv.org/abs/2311.18580\\n[75] Luigi Daniele, and Suphava Deeprasit. 2023. Amplify-Instruct: Synthetically generated diverse multi-turn conversa-\\ntions for efficient LLM training. arXiv Preprint. Retrieved fromhttps://huggingface.co/datasets/LDJnr/Capybara\\n[76] Tri Dao. 2024. FlashAttention-2: Faster attention with better parallelism and work partitioning. InThe Twelfth\\nInternational Conference on Learning Representations .\\n[77] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. Flashattention: Fast and memory-efficient\\nexact attention with io-awareness.Adv. Neural Inf. Process. Syst . 35, (2022), 16344–16359.\\n[78] TriDao,andAlbertGu.2024.TransformersareSSMs:Generalizedmodelsandefficientalgorithmsthroughstructured\\nstate space duality. arXiv:2405.21060. Retrieved fromhttps://arxiv.org/abs/2405.21060\\n[79] Rocktim Jyoti Das, Liqun Ma, and Zhiqiang Shen. 2023. Beyond size: How gradients shape pruning decisions in\\nlarge language models. arXiv:2311.04902. Retrieved fromhttps://arxiv.org/abs/2311.04902\\n[80] Anirban Dasgupta, Ravi Kumar, and Tamás Sarlós. 2011. Fast locality-sensitive hashing. InProceedings of the 17th\\nACM SIGKDD International Conference on Knowledge Discovery and Data Mining . ACM, 1073–1081.\\n[81] Pieter Delobelle, Ewoenam Kwaku Tokpo, Toon Calders, and Bettina Berendt. 2022. Measuring fairness with biased\\nrulers: A comparative study on bias metrics for pre-trained language models. InProceedings of the 2022 Conference\\nof the North American Chapter of the Association for Computational Linguistics . Association for Computational\\nLinguistics, 1693–1706.\\n[82] Yongheng Deng, Ziqing Qiao, Ju Ren, Yang Liu, and Yaoxue Zhang. 2023. Mutual enhancement of large and small\\nlanguage models with cross-silo knowledge transfer. arXiv:2312.05842. Retrieved fromhttps://arxiv.org/abs/2312.\\n05842\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 68, 'page_label': '69'}, page_content='145:68 F. Wang et al.\\n[83] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. GPT3.int8(): 8-bit Matrix Multiplication for\\nTransformers at Scale. InAdvances in Neural Information Processing Systems , Alice H. Oh, Alekh Agarwal, Danielle\\nBelgrave, and Kyunghyun Cho (Eds.). Retrieved fromhttps://openreview.net/forum?id=dXiGWqBoxaD.\\n[84] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2024. Qlora: Efficient finetuning of quantized\\nllms. Adv. Neural Inf. Process. Syst . 36, (2024).DOI: https://doi.org/10.48550/arXiv.2305.14314\\n[85] Tim Dettmers, and Luke Zettlemoyer. 2023. The case for 4-bit precision: K -bit inference scaling laws. InInternational\\nConference on Machine Learning . PMLR, 7750–7774.\\n[86] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional\\ntransformers for language understanding. InProceedings of the 2019 Conference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human Language Technologies , Vol. 1 (Long and Short Papers). Association\\nfor Computational Linguistics, 4171–4186.\\n[87] Nolan Dey, Gurpreet Gosal, Zhiming Chen, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, and\\nJoel Hestness. 2023. Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale\\nCluster. CoRR abs/2304.03208.\\n[88] Richard Diehl Martinez, Pietro Lesci, and Paula Buttery. 2024. Tending Towards Stability: Convergence Challenges\\nin Small Language Models. InFindings of the Association for Computational Linguistics: EMNLP 2024, Yaser Al-\\nOnaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, 3275–3286.https:\\n//doi.org/10.18653/v1/2024.findings-emnlp.187\\n[89] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen\\nZhou. 2023. Enhancing chat language models by scaling high-quality instructional conversations. arXiv:2305.14233.\\nRetrieved fromhttps://arxiv.org/abs/2305.14233\\n[90] Tinghe Ding. 2024. MobileAgent: Enhancing mobile control via human-machine interaction and SOP integration.\\narXiv:240104124. Retrieved fromhttps://arxiv.org/abs/2401.04124\\n[91] Ricardo Dominguez-Olmedo, Moritz Hardt, and Celestine Mendler-Dünner. 2023. Questioning the survey responses\\nof large language models. arXiv:2306.07951. Retrieved fromhttps://arxiv.org/abs/2306.07951\\n[92] Qian Dong, Yiding Liu, Qingyao Ai, Haitao Li, Shuaiqiang Wang, Yiqun Liu, Dawei Yin, and Shaoping Ma. 2023. I3\\nretriever: Incorporating implicit interaction in pre-trained language models for passage retrieval. InProceedings of\\nthe 32nd ACM International Conference on Information and Knowledge Management . ACM, 441–451.\\n[93] Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Shih-Yang Liu,\\nMatthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, et al. 2024. Hymba: A hybrid-head architecture for small\\nlanguage models. arXiv:2411.13676. Retrieved fromhttps://arxiv.org/abs/2411.13676\\n[94] Xinrun Du, Zhouliang Yu, Songyang Gao, Ding Pan, Yuyang Cheng, Ziyang Ma, Ruibin Yuan, Xingwei Qu, Jiaheng\\nLiu, Tianyu Zheng, Xinchen Luo, Guorui Zhou, Wenhu Chen, and Ge Zhang. 2024. Chinese tiny LLM: Pretraining a\\nChinese-centric large language model. arXiv:240404167. Retrieved fromhttps://arxiv.org/abs/2404.04167\\n[95] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General\\nlanguage model pretraining with autoregressive blank infilling. InProceedings of the 60th Annual Meeting of the\\nAssociation for Computational Linguistics Vol. 1 Long Papers, Association for Computational Linguistics, 320–335.\\n[96] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil\\nMathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The Llama 3 herd of models. arXiv:2407.21783. Retrieved\\nfrom https://arxiv.org/abs/2407.21783\\n[97] Kazuki Egashira, Mark Vero, Robin Staab, Jingxuan He, and Martin Vechev. 2024. Exploiting LLM quantization.\\narXiv:2405.18137. Retrieved fromhttps://arxiv.org/abs/2405.18137\\n[98] Ronen Eldan, and Yuanzhi Li. 2023. Tinystories: How small can language models be and still speak coherent English?.\\narXiv:2305.07759. Retrieved fromhttps://arxiv.org/abs/2305.07759\\n[99] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. 2018. Sigmoid-weighted linear units for neural network function\\napproximation in reinforcement learning.Neural Netw.: Off. J. Int. Neural Netw. Soc . 107, (2018), 3–11.DOI: https:\\n//doi.org/10.1016/j.neunet.2017.12.012\\n[100] DavidEsiobu,XiaoqingTan,SagharHosseini,MeganUng,YuchenZhang,JudeFernandes,JaneDwivedi-Yu,Eleonora\\nPresani, Adina Williams, and Eric Smith. 2023. ROBBIE: Robust bias evaluation of large generative language models.\\nIn Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , 3764–3814. Retrieved\\nfrom https://aclanthology.org/2023.emnlp-main.230.\\n[101] Hugging Face. 2024.SmolVLM - small yet mighty Vision Language Model . Retrieved fromhttps://huggingface.co/\\nblog/smolvlm. Accessed: November 26, 2024.\\n[102] William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with\\nsimple and efficient sparsity.J. Mach. Learn. Res . 23, 120 (2022), 1–39.DOI: https://doi.org/10.48550/arXiv.2101.03961\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 69, 'page_label': '70'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:69\\n[103] Shangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov. 2023. Knowledge\\ncard: Filling LLMs’ knowledge gaps with plug-in specialized language models. arXiv:2305.09955. Retrieved from\\nhttps://arxiv.org/abs/2305.09955\\n[104] Elias Frantar, and Dan Alistarh. 2023. SparseGPT: Massive language models can be accurately pruned in one-shot. In\\nInternational Conference on Machine Learning . PMLR, 10323–10337.\\n[105] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2023. GPTQ: Accurate post-training quantization\\nfor generative pre-trained transformers. InThe Eleventh International Conference on Learning Representations .\\n[106] Hao Fu, Yao Peng, and Tushar Khot. 2022. How does GPT Obtain its Ability? Tracing Emergent Abilities of Language\\nModels to their Sources.Yao Fu’s Notion (Dec. 2022) Retrieved fromhttps://yaofu.notion.site/How-does-GPT-Obtain-\\nits-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1\\n[107] Yao Fu, Hao Peng, and Litu Ou. Ashish sabharwal, and tushar khot. 2023. Specializing smaller language models\\ntowards multi-step reasoning. InInternational Conference on Machine Learning . PMLR, 10421–10430.\\n[108] Chongming Gao, Shiqi Wang, Shijun Li, Jiawei Chen, Xiangnan He, Wenqiang Lei, Biao Li, Yuan Zhang, and Peng\\nJiang. 2023. CIRS: Bursting filter bubbles by counterfactual interactive recommender system.ACM Trans. Inf. Syst .\\n42, 1 (2023), 1–27.DOI: https://doi.org/10.1145/3594871\\n[109] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish\\nThite, Noa Nabeshima, et al. 2020. The Pile: An 800GB dataset of diverse text for language modeling. arXiv:2101.00027.\\nRetrieved fromhttps://arxiv.org/abs/2101.00027\\n[110] Luyu Gao, and Jamie Callan. 2022. Unsupervised corpus aware language model pre-training for dense passage\\nretrieval. InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics , Vol. 1 Long\\nPapers, Association for Computational Linguistics, 2843–2853.\\n[111] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding,\\nJeffrey Hsu, Alain Le Noac’h, et al. 2024. The Language Model Evaluation Harness.DOI: https://doi.org/10.5281/\\nzenodo.12608602\\n[112] Shangqian Gao, Chi-Heng Lin, Ting Hua, Zheng Tang, Yilin Shen, Hongxia Jin, and Yen-Chang Hsu. 2024. DISP-LLM:\\nDimension-Independent structural pruning for large language models. InThe Thirty-Eighth Annual Conference on\\nNeural Information Processing Systems . Retrieved fromhttps://openreview.net/forum?id=YxaY6tHgg0\\n[113] SuyuGe,YunanZhang,LiyuanLiu,MinjiaZhang,JiaweiHan,andJianfengGao.2024.Modeltellsyouwhattodiscard:\\nAdaptive KV cache compression for LLMs. InThe Twelfth International Conference on Learning Representations .\\nRetrieved fromhttps://openreview.net/forum?id=uNrFpDPMyo\\n[114] Alex Gichamba, Tewodros Kederalah Idris, Brian Ebiyau, Eric Nyberg, and Teruko Mitamura. 2024. ColBERT\\nretrieval and ensemble response scoring for language model question answering. arXiv:240810808. Retrieved from\\nhttps://arxiv.org/abs/2408.10808\\n[115] Karan Goel. 2024. The OnDevice Intelligence Update. Retrieved fromhttps://www.cartesia.ai/blog/on-device\\n[116] Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. 2019. Openwebtext corpus.\\n[117] Shreya Goyal, Sumanth Doddapaneni, Mitesh M. Khapra, and Balaraman Ravindran. 2023. A survey of adversarial\\ndefenses and robustness in nlp.Comput. Surveys 55, 14s (2023), 1–39.\\n[118] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha,\\nHamish Ivison, Ian Magnusson, Yizhong Wang, et al. 2024. OLMo: Accelerating the science of language models.\\narXiv:2402.00838. Retrieved fromhttps://arxiv.org/abs/2402.00838\\n[119] Albert Gu, and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. arXiv:2312.00752.\\nRetrieved fromhttps://arxiv.org/abs/2312.00752\\n[120] Naibin Gu, Peng Fu, Xiyu Liu, Bowen Shen, Zheng Lin, and Weiping Wang. 2024. Light-PEFT: Lightening Parameter-\\nEfficient Fine-Tuning via Early Pruning. InFindings of the Association for Computational Linguistics: ACL 2024 ,\\nLun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, 7528–7541.DOI:\\nhttps://doi.org/10.18653/v1/2024.findings-acl.447\\n[121] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2024. MiniLLM: Knowledge distillation of large language models.\\nIn The Twelfth International Conference on Learning Representations .\\n[122] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan\\nJavaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang,\\nSébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. 2023. Textbooks are all you need.\\narXiv:230611644. Retrieved fromhttps://arxiv.org/abs/2306.11644\\n[123] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, Y. K. Li,\\net al. 2024. DeepSeek-Coder: When the Large Language Model Meets Programming–The Rise of Code Intelligence.\\narXiv preprint arXiv:2401.14196 .\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 70, 'page_label': '71'}, page_content='145:70 F. Wang et al.\\n[124] Jinyang Guo, Jianyu Wu, Zining Wang, Jiaheng Liu, Ge Yang, Yifu Ding, Ruihao Gong, Haotong Qin, and Xianglong\\nLiu. 2024. Compressing large language models by joint sparsification and quantization. InForty-First International\\nConference on Machine Learning .\\n[125] Shangwei Guo, Chunlong Xie, Jiwei Li, Lingjuan Lyu, and Tianwei Zhang. 2022. Threats to pre-trained language\\nmodels: Survey and taxonomy.arXiv preprint arXiv:2202.06862 .\\n[126] Song Guo, Jiahang Xu, Li Lyna Zhang, and Mao Yang. 2023. Compresso: Structured pruning with collaborative\\nprompting learns compact large language models.arXiv preprint arXiv:2310.05015 .\\n[127] Zhen Guo, Peiqi Wang, Yanwei Wang, and Shangdi Yu. 2023. Improving small language models on PubMedQA via\\ngenerative data augmentation. arXiv:230507804. Retrieved fromhttps://arxiv.org/abs/2305.07804\\n[128] Song Han, Jeff Pool, John Tran, and William Dally. 2015. Learning both weights and connections for efficient neural\\nnetwork. Adv. Neural Inf. Process. Syst. , 28 (2015), 1135–1143.\\n[129] Zixu Hao, Huiqiang Jiang, Shiqi Jiang, Ju Ren, and Ting Cao. 2024. Hybrid SLM and LLM for edge-cloud collaborative\\ninference. InProceedings of the Workshop on Edge and Mobile Foundation Models . ACM, 36–41.\\n[130] Tim Hartill, Diana Benavides-Prado, Michael Witbrock, and Patricia J. Riddle. 2023. Answering unseen questions\\nwith smaller language models using rationale generation and dense retrieval. arXiv:230804711. Retrieved from\\nhttps://arxiv.org/abs/2308.04711\\n[131] TimHartill,NesetTan,MichaelWitbrock,andPatriciaJ.Riddle.2023.Teachingsmallerlanguagemodelstogeneralise\\nto unseen compositional questions. arXiv:230800946. Retrieved fromhttps://arxiv.org/abs/2308.00946\\n[132] Kai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng, and Erik Cambria. 2023. A survey of large lan-\\nguage models for healthcare: From data, technology, and applications to accountability and ethics. arXiv:2310.05694.\\nRetrieved fromhttps://arxiv.org/abs/2310.05694\\n[133] Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023. DeBERTaV3: Improving DeBERTa using ELECTRA-Style\\nPre-Training with Gradient-Disentangled embedding sharing. InThe Eleventh International Conference on Learning\\nRepresentations. Retrieved fromhttps://openreview.net/forum?id=sE7-XhLxHA\\n[134] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. Deberta: Decoding-enhanced bert with\\ndisentangled attention. arXiv:2006.03654. Retrieved fromhttps://arxiv.org/abs/2006.03654\\n[135] Narges Heidari, Parham Moradi, and Abbas Koochari. 2022. An attention-based deep learning method for solving\\nthe cold-start and sparsity issues of recommender systems.Knowl.-Based Syst . 256 (2022), 109835.DOI: https:\\n//doi.org/10.1016/j.knosys.2022.109835\\n[136] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020.\\nMeasuring massive multitask language understanding. arXiv:2009.03300. Retrieved fromhttps://arxiv.org/abs/2009.\\n03300\\n[137] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021.\\nMeasuring massive multitask language understanding. InInternational Conference on Learning Representations .\\nRetrieved fromhttps://openreview.net/forum?id=d7KBjmI3GmQ\\n[138] Dan Hendrycks, and Kevin Gimpel. 2016. Gaussian error linear units (gelus). arXiv:1606.08415. Retrieved from\\nhttps://arxiv.org/abs/1606.08415\\n[139] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv:1503.02531.\\nRetrieved fromhttps://arxiv.org/abs/1503.02531\\n[140] Sepp Hochreiter, and Jürgen Schmidhuber. 1996. LSTM can solve hard long time lag problems.Adv. Neural Inf.\\nProcess. Syst . 9, (1996), 473–479.DOI: https://doi.org/10.5555/2998981.2999048\\n[141] Junyuan Hong, Jinhao Duan, Chenhui Zhang, Zhangheng Li, Chulin Xie, Kelsey Lieberman, James Diffenderfer, Brian\\nR. Bartoldson, Ajay Kumar Jaiswal, Kaidi Xu, et al. 2024. Decoding compressed trust: Scrutinizing the trustworthiness\\nof efficient LLMs under compression. InProceedings of the Forty-First International Conference on Machine Learning ,\\nICML. Retrieved fromhttps://openreview.net/forum?id=e3Dpq3WdMv\\n[142] Yutong Meng Yuhao Wang Hongcheng Liu, and Yusheng Liao. 2023. XieZhi: Chinese law large language model.\\nRetrieved fromhttps://github.com/LiuHC0428/LAW_GPT\\n[143] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo,\\nMona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. InInternational Conference\\non Machine Learning . PMLR, 2790–2799.\\n[144] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna,\\nChen-Yu Lee, and Tomas Pfister. 2023. Distilling step-by-step! outperforming larger language models with less\\ntrainingdataandsmallermodelsizes.In Findings of the Association for Computational Linguistics: ACL ’23.Association\\nfor Computational Linguistics, 8003–8017.\\n[145] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\n2021. Lora: Low-rank adaptation of large language models. arXiv:2106.09685. Retrieved fromhttps://arxiv.org/abs/\\n2106.09685\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 71, 'page_label': '72'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:71\\n[146] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang,\\nWeilin Zhao, et al. 2024. Minicpm: Unveiling the potential of small language models with scalable training strategies.\\narXiv:2404.06395. Retrieved fromhttps://arxiv.org/abs/2404.06395\\n[147] Xing Hu, Yuan Chen, Dawei Yang, Sifan Zhou, Zhihang Yuan, Jiangyong Yu, and Chen Xu. 2024. I-LLM: Efficient\\ninteger-only inference for fully-quantized low-bit large language models. arXiv:2405.17849. Retrieved fromhttps:\\n//arxiv.org/abs/2405.17849\\n[148] Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2023. Large\\nlanguage models can Self-Improve. InThe 2023 Conference on Empirical Methods in Natural Language Processing .\\n[149] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng,\\nXiaocheng Feng, Bing Qin, et al. 2023. A survey on hallucination in large language models: Principles, taxonomy,\\nchallenges, and open questions. arXiv:2311.05232. Retrieved fromhttps://arxiv.org/abs/2311.05232\\n[150] Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong Liu, Michele Magno, and Xiaojuan\\nQi. 2024. Billm: Pushing the limit of post-training quantization for LLMS. arXiv:2402.04291. Retrieved fromhttps:\\n//arxiv.org/abs/2402.04291\\n[151] Wenyu Huang, Guancheng Zhou, Hongru Wang, Pavlos Vougiouklis, Mirella Lapata, and Jeff Z. Pan. 2024. Less is\\nMore: Making Smaller Language Models Competent Subgraph Retrievers for Multi-hop KGQA. InFindings of the\\nAssociation for Computational Linguistics: EMNLP 2024 , Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.).\\nAssociation for Computational Linguistics, 15787–15803.https://doi.org/10.18653/v1/2024.findings-emnlp.927\\n[152] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv,\\nYikai Zhang, Yao Fu, et al. 2024. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models.\\nAdv. Neural Inf. Process. Syst . 36 (2024), 62991–63010.\\n[153] Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. 2023. Catastrophic jailbreak of open-source\\nLLMs via exploiting generation. arXiv:2310.06987. Retrieved fromhttps://arxiv.org/abs/2310.06987\\n[154] Yuheng Huang, Jiayang Song, Zhijie Wang, Shengming Zhao, Huaming Chen, Felix Juefei-Xu, and Lei Ma. 2023. Look\\nbefore you leap: An exploratory study of uncertainty measurement for large language models. arXiv:2307.10236.\\nRetrieved fromhttps://arxiv.org/abs/2307.10236\\n[155] Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. 2020. Poly-encoders: Architectures and\\npre-training strategies for fast and accurate multi-sentence scoring. InInternational Conference on Learning Repre-\\nsentations.\\n[156] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing\\nHu, Brian Fuller, Davide Testuggine, et al. 2023. Llama guard: LLM-based input-output safeguard for human-AI\\nconversations. arXiv:2312.06674. Retrieved fromhttps://arxiv.org/abs/2312.06674\\n[157] Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. 1991. Adaptive mixtures of local\\nexperts. Neural Comput . 3, 1 (1991), 79–87.\\n[158] Mojan Javaheripi, Sébastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio César Teodoro Mendes,\\nWeizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth Gopi, et al. 2023. Phi-2: The surprising power of small\\nlanguage models. InMicrosoft Research Blog .\\n[159] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Hwang, and Jong C. Park. 2023. Test-Time Self-Adaptive small\\nlanguage models for question answering. InFindings of the Association for Computational Linguistics: EMNLP ’23 .\\nAssociation for Computational Linguistics, 15459–15469.\\n[160] Ananya Harsh Jha, Tom Sherborne, Evan Pete Walsh, Dirk Groeneveld, Emma Strubell, and Iz Beltagy. 2024. Just\\nCHOP: Embarrassingly simple LLM compression. arXiv:230514864. Retrieved fromhttps://arxiv.org/abs/2305.14864\\n[161] Yixin Ji, Yang Xiang, Juntao Li, Wei Chen, Zhongyi Liu, Kehai Chen, and Min Zhang. 2024. Feature-based low-\\nrank compression of large language models via Bayesian optimization. arXiv:2405.10616. Retrieved fromhttps:\\n//arxiv.org/abs/2405.10616\\n[162] Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. 2023. Towards mitigating LLM halluci-\\nnation via self reflection. InFindings of the Association for Computational Linguistics: EMNLP ’23 . Association for\\nComputational Linguistics, 1827–1843.\\n[163] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las\\nCasas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7B. arXiv:2310.06825.\\nRetrieved fromhttps://arxiv.org/abs/2310.06825\\n[164] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, De-\\nvendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts.\\narXiv:2401.04088. Retrieved fromhttps://arxiv.org/abs/2401.04088\\n[165] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2024. LongLLM-\\nLingua: Accelerating and enhancing LLMs in long context scenarios via prompt compression. InICLR 2024 Workshop\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 72, 'page_label': '73'}, page_content='145:72 F. Wang et al.\\non Mathematical and Empirical Understanding of Foundation Models . Retrieved fromhttps://openreview.net/forum?\\nid=9YvfRrpmyw\\n[166] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. PubMedQA: A dataset for\\nbiomedical research question answering. InProceedings of the 2019 Conference on Empirical Methods in Natural\\nLanguage Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) ,\\n2567–2577.\\n[167] Rudolph Emil Kalman. 1960. A new approach to linear filtering and prediction problems.Trans. ASME, D 82 (1960),\\n35–44.\\n[168] Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, and Tuo Zhao. 2024. Gear:\\nAn efficient KV cache compression recipefor near-lossless generative inference of LLM. arXiv:2403.05527. Retrieved\\nfrom https://arxiv.org/abs/2403.05527\\n[169] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec\\nRadford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv:2001.08361. Retrieved\\nfrom https://arxiv.org/abs/2001.08361\\n[170] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful\\nHaq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, et al. 2023. DSPy: Compiling declarative language model\\ncalls into self-improving pipelines. arXiv:2310.03714. Retrieved fromhttps://arxiv.org/abs/2310.03714\\n[171] Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, and Dongsoo Lee.\\n2023. Memory-efficient fine-tuning of compressed large language models via Sub-4-bit integer quantization.Adv.\\nNeural Inf. Process. Syst . 36 (2023), 36187–36207.\\n[172] Minsoo Kim, Sihwa Lee, Janghwan Lee, Sukjin Hong, Du-Seong Chang, Wonyong Sung, and Jungwook Choi. 2023.\\nToken-scaled logit distillation for ternary weight generative language models.Adv. Neural Inf. Process. Syst . 36 (2023),\\n42097–42118.\\n[173] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W. Mahoney, and Kurt\\nKeutzer. 2023. Squeezellm: Dense-and-sparse quantization. arXiv:2306.07629. Retrieved fromhttps://arxiv.org/abs/\\n2306.07629\\n[174] Yoon Kim, and Alexander M. Rush. 2016. Sequence-Level knowledge distillation. InProceedings of the 2016 Conference\\non Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 1317–1327.\\n[175] YoungJinKim,RaffyFahim,andHanyHassanAwadalla.2023.Mixtureofquantizedexperts(MoQE):Complementary\\neffect of low-bit quantization and robustness. arXiv:2310.02410. Retrieved fromhttps://arxiv.org/abs/2310.02410\\n[176] Jongwoo Ko, Sungnyun Kim, Tianyi Chen, and Se-Young Yun. 2024. DistiLLM: Towards streamlined distillation for\\nlarge language models. arXiv:2402.03898. Retrieved fromhttps://arxiv.org/abs/2402.03898\\n[177] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite,\\nMargaret Mitchell, Sean Hughes, Thomas Wolf, et al. 2022. The stack: 3 TB of permissively licensed source code.\\narXiv:2211.15533. Retrieved fromhttps://arxiv.org/abs/2211.15533\\n[178] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for uncertainty\\nestimation in natural language generation. InProceedings of the Eleventh International Conference on Learning\\nRepresentations. Retrieved fromhttps://openreview.net/forum?id=VD-AYtP0dve\\n[179] Divyanshu Kumar, Anurakt Kumar, Sahil Agarwal, and Prashanth Harshangi. 2024. Fine-tuning, quantization, and\\nLLMs: Navigating unintended outcomes. arXiv:2404.04392. Retrieved fromhttps://arxiv.org/abs/2404.04392\\n[180] Ohjoon Kwon, Donghyeon Jeon, Nayoung Choi, Gyu-Hwung Cho, Hwiyeol Jo, Changbong Kim, Hyunwoo Lee,\\nInho Kang, Sun Kim, and Taiwoo Park. 2024. SLM as guardian: Pioneering AI safety with small language model.\\nIn Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track , Franck\\nDernoncourt, Daniel Preoţiuc-Pietro, and Anastasia Shimorina (Eds.). Association for Computational Linguistics,\\n1333–1350. DOI: https://doi.org/10.18653/v1/2024.emnlp-industry.99\\n[181] YanisLabrak,AdrienBazoge,EmmanuelMorin,Pierre-AntoineGourraud,MickaelRouvier,andRichardDufour.2024.\\nBiomistral: A collection of open-source pretrained large language models for medical domains. arXiv:2402.10373.\\nRetrieved fromhttps://arxiv.org/abs/2402.10373\\n[182] Stefan Larson, Anish Mahendran, Joseph J. Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan K. Kum-\\nmerfeld, Kevin Leach, Michael A. Laurenzano, Lingjia Tang, and Jason Mars. 2019. An Evaluation Dataset for Intent\\nClassification and Out-of-Scope Prediction. InEmnlp-Ijcnlp 2019 , Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun\\nWan (Eds.). Association for Computational Linguistics, 1311–1316.DOI: https://doi.org/10.18653/v1/D19-1131\\n[183] Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao,\\nLeandro Von Werra, Chenghao Mou, Eduardo González Ponferrada, Huu Nguyen, et al. 2022. The bigscience roots\\ncorpus: A 1.6 tb composite multilingual dataset.Adv. Neural Inf. Process. Syst . 35 (2022), 31809–31826.\\n[184] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra\\nSashaLuccioni,FrançoisYvon,MatthiasGallé,etal.2023.Bloom:A176b-parameteropen-accessmultilinguallanguage\\nmodel. arxiv:2211.05100. Retrieved fromhttps://arxiv.org/abs/2211.05100\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 73, 'page_label': '74'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:73\\n[185] Hojae Lee, Junho Kim, and SangKeun Lee. 2024. Mentor-KD: Making small language models better multi-step\\nreasoners. InProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing , Yaser Al-\\nOnaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, 17643–17658.DOI:\\nhttps://doi.org/10.18653/v1/2024.emnlp-main.977\\n[186] Jooyoung Lee, Fan Yang, Thanh Tran, Qian Hu, Emre Barut, and Kai-Wei Chang. 2024. Can small language models\\nhelp large language models reason better?: LM-Guided chain-of-thought. InProceedings of the 2024 Joint International\\nConference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING ’24) , 2835–2843.\\n[187] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu,\\nJieru Hu, Marta Tintore, Susan Zhang, et al. 2022. xFormers: A modular and hackable transformer modelling library.\\nRetrieved fromhttps://github.com/facebookresearch/xformers\\n[188] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer normalization. arXiv:1607.06450. Retrieved\\nfrom https://arxiv.org/abs/1607.06450v1\\n[189] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose\\nSlone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with\\nlanguage models.Adv. Neural Inf. Process. Syst . 35, (2022), 3843–3857.\\n[190] Chenglin Li, Qianglong Chen, Liangyue Li, Caiyu Wang, Yicheng Li, Zulong Chen, and Yin Zhang. 2023. Mixed\\ndistillation helps smaller language model better reasoning. arXiv:2312.10730. Retrieved fromhttps://arxiv.org/abs/\\n2312.10730\\n[191] Guangyan Li, Yongqiang Tang, and Wensheng Zhang. 2024. LoRAP: Transformer sub-layers deserve differentiated\\nstructured compression for large language models. arXiv:2404.09695. Retrieved fromhttps://arxiv.org/abs/2404.09695\\n[192] Haitao Li, Qingyao Ai, Jia Chen, Qian Dong, Zhijing Wu, Yiqun Liu, Chong Chen, and Qi Tian. 2024. BLADE:\\nEnhancing black-box large language models with small domain-specific models. arXiv:2403.18365. Retrieved from\\nhttps://arxiv.org/abs/2403.18365\\n[193] Haoran Li, Dadi Guo, Donghao Li, Wei Fan, Qi Hu, Xin Liu, Chunkit Chan, Duanyi Yao, Yuan Yao, and Yangqiu Song.\\n2024. PrivLM-Bench: A multi-level privacy evaluation benchmark for language models. InProceedings of the 62nd\\nAnnual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, 54–73.\\n[194] Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. HaluEval: A Large-Scale hallucination\\nevaluation benchmark for large language models. InProceedings of the 2023 Conference on Empirical Methods in\\nNatural Language Processing . Association for Computational Linguistics, 6449–6464.DOI: https://doi.org/10.18653/\\nv1/2023.emnlp-main.397\\n[195] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick\\nKeh, Kushal Arora, et al. 2024. DataComp-LM: In search of the next generation of training sets for language models.\\narXiv:2406.11794. Retrieved fromhttps://arxiv.org/abs/2406.11794\\n[196] Luchang Li, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, and Qin Xie. 2024. Transformer-lite: high-efficiency deploy-\\nment of large language models on mobile phone GPUs. arXiv:2403.20041. Retrieved fromhttps://arxiv.org/abs/2403.\\n20041\\n[197] Pingzhi Li, Xiaolong Jin, Yu Cheng, and Tianlong Chen. 2024. Examining post-training quantization for mixture-of-\\nexperts: A benchmark. arXiv:2406.08155. Retrieved fromhttps://arxiv.org/abs/2406.08155\\n[198] Quan Li, Tianxiang Zhao, Lingwei Chen, Junjie Xu, and Suhang Wang. 2024. Enhancing graph neural networks with\\nlimited labeled data by actively distilling knowledge from large language models. arXiv:2407.13989. Retrieved from\\nhttps://arxiv.org/abs/2407.13989\\n[199] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone,\\nChristopher Akiki, Jia Li, Jenny Chim, et al. 2023. StarCoder: May the source be with you! arXiv:2305.06161. Retrieved\\nfrom https://arxiv.org/abs/2305.06161\\n[200] Shengrui Li, Xueting Han, and Jing Bai. 2024. Nuteprune: Efficient progressive pruning with numerous teachers for\\nlarge language models. arXiv:2402.09773. Retrieved fromhttps://arxiv.org/abs/2402.09773\\n[201] Tianlin Li, Qian Liu, Tianyu Pang, Chao Du, Qing Guo, Yang Liu, and Min Lin. 2024. Purifying large language models\\nby ensembling a small language model. arXiv:2402.14845. Retrieved fromhttps://arxiv.org/abs/2402.14845\\n[202] Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori B. Hashimoto, Luke Zettlemoyer,\\nand Mike Lewis. 2023. Contrastive decoding: Open-ended text generation as optimization. InProceedings of the 61st\\nAnnual Meeting of the Association for Computational Linguistics , Vol. 1 Long Papers, Association for Computational\\nLinguistics, 12286–12312.\\n[203] Xiang Lisa Li, and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. arXiv:2101.00190.\\nRetrieved fromhttps://arxiv.org/abs/2101.00190\\n[204] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023. Textbooks\\nare all you need II: Phi-1.5 Technical Report. arXiv:2309.05463. Retrieved fromhttps://arxiv.org/abs/2309.05463\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 74, 'page_label': '75'}, page_content='145:74 F. Wang et al.\\n[205] Yun Li, Lin Niu, Xipeng Zhang, KaiLiu, Jianchen Zhu, and Zhanhui Kang. 2023. E-Sparse: Boosting the large language\\nmodel inference through entropy-based N:M sparsity. arXiv:2310.15929. Retrieved fromhttps://arxiv.org/abs/2310.\\n15929\\n[206] YinhengLi,ShaofeiWang,HanDing,andHangChen.2023.Largelanguagemodelsinfinance:Asurvey. In Proceedings\\nof the Fourth ACM International Conference on AI in Finance , 374–382.\\n[207] Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet, and Vong, Teknium. 2023.\\nSlimOrca: An Open Dataset of GPT-4 Augmented FLAN Reasoning Traces, with Verification. Retrieved from\\nhttps://https://huggingface.co/Open-Orca/SlimOrca\\n[208] Jinggui Liang, Lizi Liao, Hao Fei, and Jing Jiang. 2024. Synergizing large language models and Pre-Trained smaller\\nmodels for conversational intent discovery. InFindings of the Association for Computational Linguistics ACL ’24 .\\nAssociation for Computational Linguistics, 14133–14147.\\n[209] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak\\nNarayanan, Yuhuai Wu, Ananya Kumar, et al. 2023. Holistic evaluation of language models. In TMLR’23. Retrieved\\nfrom https://openreview.net/forum?id=iO4LZibEqW\\n[210] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom,\\nYonatan Belinkov, Shai Shalev-Shwartz, et al. 2024. Jamba: A hybrid transformer-Mamba language model.\\narXiv:2403.19887. Retrieved fromhttps://arxiv.org/abs/2403.19887\\n[211] JianghaoLin,RongShan,ChenxuZhu,KounianhuaDu,BoChen,ShigangQuan,RuimingTang,YongYu,andWeinan\\nZhang. 2024. Rella: Retrieval-enhanced large language models for lifelong sequential behavior comprehension in\\nrecommendation. InProceedings of the ACM on Web Conference 2024 , 3497–3508.\\n[212] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang,\\nChuang Gan, and Song Han. 2024. AWQ: Activation-aware weight quantization for on-device LLM compression and\\nacceleration. Proc. Mach. Learn. Syst . 6 (2024), 87–100.\\n[213] Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring how models mimic human falsehoods.\\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics , Vol. 1 Long Papers,\\nAssociation for Computational Linguistics, 3214–3252.\\n[214] Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman\\nGoyal, Shruti Bhosale, Jingfei Du, et al. 2022. Few-shot learning with multilingual generative language models.\\nIn EMNLP-Main 2022 , Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational\\nLinguistics, 9019–9052.DOI: https://doi.org/10.18653/v1/2022.emnlp-main.616\\n[215] Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan\\nDuan, et al. 2024. Rho-1: Not all tokens are what you need. arXiv:2404.07965. Retrieved fromhttps://arxiv.org/abs/\\n2404.07965\\n[216] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai\\nDai, Daya Guo, et al. 2024. DeepSeek-v2: A strong, economical, and efficient mixture-of-experts language model.\\narXiv:2405.04434. Retrieved fromhttps://arxiv.org/abs/2405.04434\\n[217] Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, and Noah A. Smith. 2024. Tuning language\\nmodels by proxy. arXiv:2401.08565. Retrieved fromhttps://arxiv.org/abs/2401.08565\\n[218] Jiashuo Liu, Zheyan Shen, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. 2021. Towards out-of-\\ndistribution generalization: A survey. arXiv:2108.13624. Retrieved fromhttps://arxiv.org/abs/2108.13624\\n[219] Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. 2024. Once: Boosting content-based recommendation\\nwith both open-and closed-source large language models. InProceedings of the 17th ACM International Conference on\\nWeb Search and Data Mining . ACM, 452–461.\\n[220] Suqing Liu, Zezhu Yu, Feiran Huang, Yousef Bulbulia, Andreas Bergen, and Michael Liut. 2024. Can small language\\nmodels with retrieval-augmented generation replace large language models when learning computer science?. In\\nProceedings of the 2024 on Innovation and Technology in Computer Science Education , Vol. 1, ACM, 388–393.\\n[221] Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. 2023. What makes good data for alignment? a\\ncomprehensive study of automatic data selection in instruction tuning. arXiv:2312.15685. Retrieved fromhttps:\\n//arxiv.org/abs/2312.15685\\n[222] Yinpeng Liu, Jiawei Liu, Xiang Shi, Qikai Cheng, and Wei Lu. 2024. Let’s learn step by step: Enhancing in-context\\nlearning ability with curriculum learning. arXiv:2402.10738. Retrieved fromhttps://arxiv.org/abs/2402.10738\\n[223] YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,OmerLevy,MikeLewis,LukeZettlemoyer,\\nand Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. arXiv:1907.11692. Retrieved\\nfrom https://arxiv.org/abs/1907.11692\\n[224] Yanming Liu, Xinyue Peng, Xuhong Zhang, Weihao Liu, Jianwei Yin, Jiannan Cao, and Tianyu Du. 2024. RA-ISF:\\nLearning to answer and understand from retrieval augmentation via iterative self-feedback. arXiv:2403.06840.\\nRetrieved fromhttps://arxiv.org/abs/2403.06840\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 75, 'page_label': '76'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:75\\n[225] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and\\nAnshumali Shrivastava. 2023. Scissorhands: Exploiting the persistence of importance hypothesis for LLM KV cache\\ncompression at test time.Adv. Neural Inf. Process. Syst . 36 (2023), 52342–52364.\\n[226] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman\\nKrishnamoorthi, and Vikas Chandra. 2023. LLM-QAT: Data-free quantization aware training for large language\\nmodels. arXiv:2305.17888. Retrieved fromhttps://arxiv.org/abs/2305.17888\\n[227] ZechunLiu,ChangshengZhao,ForrestIandola,ChenLai,YuandongTian,IgorFedorov,YunyangXiong,ErnieChang,\\nYangyang Shi, Raghuraman Krishnamoorthi, et al. 2024. Mobilellm: Optimizing sub-billion parameter language\\nmodels for on-device use cases. arXiv:2402.14905. Retrieved fromhttps://arxiv.org/abs/2402.14905\\n[228] Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao Ding, Gang Chen, and Haobo Wang. 2024. On LLMs-Driven\\nsynthetic data generation, curation, and evaluation: A survey. InFindings of the Association for Computational\\nLinguistics ACL ’24 . Association for Computational Linguistics, 11065–11082.\\n[229] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret\\nZoph, Jason Wei, et al. 2023. The flan collection: Designing data and methods for effective instruction tuning. In\\nInternational Conference on Machine Learning . PMLR, 22631–22648.\\n[230] Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William Brannon, Niklas\\nMuennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, et al. 2023. The data provenance initiative: A large scale\\naudit of dataset licensing & attribution in AI. arXiv:2310.16787. Retrieved fromhttps://arxiv.org/abs/2310.16787\\n[231] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang,\\nDmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. 2024. StarCoder 2 and the stack v2: The next generation.\\narXiv:2402.19173. Retrieved fromhttps://arxiv.org/abs/2402.19173\\n[232] Wenhao Lu, Jian Jiao, and Ruofei Zhang. 2020. Twinbert: Distilling knowledge to twin-structured compressed bert\\nmodels for large-scale retrieval. InProceedings of the 29th ACM International Conference on Information & Knowledge\\nManagement. ACM, 2645–2652.\\n[233] Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas D. Lane, and Mengwei\\nXu. 2024. Small language models: Survey, measurements, and insights. arXiv:2409.15790. Retrieved fromhttps:\\n//arxiv.org/abs/2409.15790\\n[234] HaitongLuo,XuyingMeng,SuhangWang,TianxiangZhao,FaliWang,HanyunCao,andYujunZhang.2024.Enhance\\ngraph alignment for large language models. arXiv:2410.11370. Retrieved fromhttps://arxiv.org/abs/2410.11370\\n[235] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. 2022. BioGPT: Generative\\npre-trained transformer for biomedical text generation and mining.Briefings in Bioinformatics 23, 6 (2022). bbac409.\\n[236] Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang,\\nJilong Xue, and Furu Wei. 2024. The era of 1-bit LLMS: All large language models are in 1.58 bits. arXiv:2402.17764.\\nRetrieved fromhttps://arxiv.org/abs/2402.17764\\n[237] Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. Llm-pruner: On the structural pruning of large language models.\\nAdv. Neural Inf. Process. Syst . 36 (2023), 21702–21720.\\n[238] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query rewriting in Retrieval-Augmented\\nlarge language models. InProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing .\\nAssociation for Computational Linguistics, 5303–5315.\\n[239] Yubo Ma, Yixin Cao,YongChing Hong, and Aixin Sun. 2023. Large language model is not a good few-shotinformation\\nextractor, but a good reranker for hard samples! arXiv:2303.08559. Retrieved fromhttps://arxiv.org/abs/2303.08559\\n[240] Yuhan Ma, Chenyou Fan, and Haiqi Jiang. 2023. Sci-cot: Leveraging large language models for enhanced knowledge\\ndistillation in small models for scientific QA. In2023 9th International Conference on Computer and Communications\\n(ICCC). IEEE, 2394–2398.\\n[241] Yingwei Ma, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, and Shanshan Li. 2024. At which training\\nstage does code data help LLMs reasoning? InThe Twelfth International Conference on Learning Representations .\\nRetrieved fromhttps://openreview.net/forum?id=KIPJKST4gw\\n[242] Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya Harsh Jha, Oyvind Tafjord, Dustin\\nSchwenk, Evan Pete Walsh, Yanai Elazar, Kyle Lo, et al. 2023. Paloma: A benchmark for evaluating language model\\nfit. arXiv:2312.10523. Retrieved fromhttps://arxiv.org/abs/2312.10523\\n[243] Dakota Mahan, Ryan Carlow, Louis Castricato, Nathan Cooper, and Christian Laforte. [n.\\u2009d.] Stable beluga models.\\nRetrieved fromhttps://huggingface.co/stabilityai/StableBeluga2\\n[244] VladimirMalinovskii,DenisMazur,IvanIlin,DenisKuznedelev,KonstantinBurlachenko,KaiYi,DanAlistarh,andPe-\\nter Richtarik. 2024. PV-tuning: Beyond straight-through estimation for extreme LLM compression. arXiv:2405.14852.\\nRetrieved fromhttps://arxiv.org/abs/2405.14852\\n[245] Potsawee Manakul, Adian Liusie, and Mark Gales. 2023. SelfCheckGPT: Zero-Resource Black-Box hallucination\\ndetection for generative large language models. InProceedings of the 2023 Conference on Empirical Methods in Natural\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 76, 'page_label': '77'}, page_content='145:76 F. Wang et al.\\nLanguage Processing , Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics,\\n9004–9017. DOI: https://doi.org/10.18653/v1/2023.emnlp-main.557\\n[246] Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Seyed Iman\\nMirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, et al. 2024. OpenELM: An efficient language model\\nfamily with open training and inference framework. InWorkshop on Efficient Systems for Foundation Models II (ICML\\n’24).\\n[247] Dheeraj Mekala, Alex Nguyen, and Jingbo Shang. 2024. Smaller language models are capable of selecting instruction-\\ntuning training data for larger language models. arXiv:2402.10430. Retrieved fromhttps://arxiv.org/abs/2402.10430\\n[248] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen.\\n2024. ShortGPT: Layers in large language models are more redundant than you expect. arXiv:2403.03853. Retrieved\\nfrom https://arxiv.org/abs/2403.03853\\n[249] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models.\\narXiv:1609.07843. Retrieved fromhttps://arxiv.org/abs/1609.07843\\n[250] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-Tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer,\\nand Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text\\ngeneration. InProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , Houda\\nBouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 12076–12100.https://doi.org/\\n10.18653/v1/2023.emnlp-main.741\\n[251] Go Min-Su. 2024. Deep Learning Bible - 8. Large Language Models. WikiDocs. Retrieved fromhttps://wikidocs.net/\\n237419\\n[252] Eric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn, and Christopher D. Manning. 2024. An emulator for\\nfine-tuning large language models using small language models. InThe Twelfth International Conference on Learning\\nRepresentations. Retrieved fromhttps://openreview.net/forum?id=Eo7kv0sllr\\n[253] Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi Chen,\\nAnastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, et al. 2023. Orca 2: Teaching small language models how to\\nreason. arXiv:2311.11045. Retrieved fromhttps://arxiv.org/abs/2311.11045\\n[254] LingboMo,BoshiWang,MuhaoChen,andHuanSun.2024.HowtrustworthyareOpen-SourceLLMs?Anassessment\\nunder malicious demonstrations shows their vulnerabilities. InProceedings of the 2024 Conference of the North\\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies , Vol. 1 Long Papers,\\nAssociation for Computational Linguistics, 2775–2792.\\n[255] John Xavier Morris, Wenting Zhao, Justin T. Chiu, Vitaly Shmatikov, and Alexander M. Rush. 2024. Language model\\ninversion. InThe Twelfth International Conference on Learning Representations . Retrieved fromhttps://openreview.\\nnet/forum?id=t9dWHpGkPj\\n[256] Maximilian Mozes, Xuanli He, Bennett Kleinberg, and Lewis D. Griffin. 2023. Use of LLMS for illicit purposes: Threats,\\nprevention measures, and vulnerabilities. arXiv:2308.12833. Retrieved fromhttps://arxiv.org/abs/2308.12833\\n[257] Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Evan Pete\\nWalsh, Oyvind Tafjord, Nathan Lambert, et al. 2025. OLMoE: Open mixture-of-experts language models. InThe\\nThirteenth International Conference on Learning Representations . Retrieved fromhttps://openreview.net/forum?id=\\nxXTkbTBmqq\\n[258] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah.\\n2023. Orca: Progressive learning from complex explanation traces of GPT-4. arXiv:2306.02707. Retrieved from\\nhttps://arxiv.org/abs/2306.02707\\n[259] Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Moham-\\nmad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. 2024. Compact language models via pruning and\\nknowledge distillation. arXiv:2407.14679. Retrieved fromhttps://arxiv.org/abs/2407.14679\\n[260] Rithesh Murthy, Liangwei Yang, Juntao Tan, Tulika Manoj Awalgaonkar, Yilun Zhou, Shelby Heinecke, Sachin\\nDesai, Jason Wu, Ran Xu, Sarah Tan, Jianguo Zhang, Zhiwei Liu, Shirley Kokane, Zuxin Liu, Ming Zhu, Huan Wang,\\nCaiming Xiong, and Silvio Savarese. 2024. MobileAIBench: Benchmarking LLMs and LMMs for on-device use cases.\\narXiv:240610290. Retrieved fromhttps://arxiv.org/abs/2406.10290\\n[261] Kalyan Nakka, Jimmy Dani, and Nitesh Saxena. 2024. Is on-device AI broken and exploitable? Assessing the trust\\nand ethics in small language models. arXiv:2406.05364. Retrieved fromhttps://arxiv.org/abs/2406.05364\\n[262] Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, and Brad Myers. 2024. Using an LLM to help\\nwith code understanding. InProceedings of the IEEE/ACM 46th International Conference on Software Engineering .\\nACM, 1–13.\\n[263] Piotr Nawrot, Adrian Łańcucki, Marcin Chochowski, David Tarjan, and Edoardo Ponti. 2024. Dynamic memory\\ncompression: Retrofitting LLMs for accelerated inference. InForty-First International Conference on Machine Learning .\\nRetrieved fromhttps://openreview.net/forum?id=tDRYrAkOB7\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 77, 'page_label': '78'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:77\\n[264] Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi,\\nand Thien Huu Nguyen. 2024. CulturaX: A cleaned, enormous, and multilingual dataset for large language models\\nin 167 languages. InProceedings of the 2024 Joint International Conference on Computational Linguistics, Language\\nResources and Evaluation (LREC-COLING ’24) . Association for Computational Linguistics, 4226–4237.\\n[265] Tuan Dung Nguyen, Yuan-Sen Ting, Ioana Ciuca, Charles O’Neill, Ze-Chang Sun, Maja Jabłońska, Sandor Kruk,\\nErnest Perkowski, Jack Miller, Jason Jason Jingsh Li, et al. 2023. AstroLLaMA: Towards specialized foundation\\nmodels in astronomy. InProceedings of the Second Workshop on Information Extraction from Scientific Publications .\\nAssociation for Computational Linguistics, 49–55.\\n[266] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall,\\nMing-Wei Chang, and Yinfei Yang. 2022. Large dual encoders are generalizable retrievers. InProceedings of the\\n2022 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics,\\n9844–9855. DOI: https://doi.org/10.18653/v1/2022.emnlp-main.669\\n[267] Rodrigo Nogueira, and Kyunghyun Cho. 2019. Passage re-ranking with BERT. arXiv:1901.04085. Retrieved from\\nhttps://arxiv.org/abs/1901.04085\\n[268] A. Noorian. 2024. A BERT-based sequential POI recommender system in social media.Comput. Stand. Interf . 87\\n(2024), 103766.DOI: https://doi.org/10.1016/j.csi.2023.103766\\n[269] OpenAI. 2024.GPT-4o mini: Advancing cost-efficient intelligence . Retrieved fromhttps://openai.com/index/gpt-4o-\\nmini-advancing-cost-efficient-intelligence/. Accessed: July 18, 2024.\\n[270] OpenAI. 2024.Hello GPT-4o. Retrieved fromhttps://openai.com/index/hello-gpt-4o/. Accessed: May 13, 2024.\\n[271] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\\nAgarwal,KatarinaSlama,AlexRay,etal.2022.Traininglanguagemodelstofollowinstructionswithhumanfeedback.\\nAdv. Neural Inf. Process. Syst . 35, (2022), 27730–27744.\\n[272] ShankarPadmanabhan,YasumasaOnoe,MichaelZhang,GregDurrett,andEunsolChoi.2023.Propagatingknowledge\\nupdates to lms through distillation.Adv. Neural Inf. Process. Syst . 36 (2023), 47124–47142.\\n[273] Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subramanian, Dan Su, Chen\\nZhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush Dattagupta, et al. 2024. Nemotron-4 15B Technical Report.\\narXiv:2402.16819. Retrieved fromhttps://arxiv.org/abs/2402.16819\\n[274] Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. 2024. OpenWebMath: An open dataset of\\nHigh-Quality mathematical web text. InThe Twelfth International Conference on Learning Representations . Retrieved\\nfrom https://openreview.net/forum?id=jKHmjlpViu\\n[275] Guilherme Penedo, Hynek Kydlíček, Loubna Ben Allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro\\nVon Werra, and Thomas Wolf. 2024. The fineweb datasets: Decanting the web for the finest text data at scale.\\narXiv:2406.17557. Retrieved fromhttps://arxiv.org/abs/2406.17557\\n[276] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobei-\\ndli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb dataset for Falcon LLM:\\nOutperforming curated corpora with web data, and web data only. arXiv:2306.01116. Retrieved fromhttps:\\n//arxiv.org/abs/2306.01116\\n[277] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng,\\nMichael Chung, Leon Derczynski, et al. 2023. RWKV: Reinventing RNNs for the Transformer Era. InFindings of the\\nAssociation for Computational Linguistics: EMNLP 2023,HoudaBouamor,JuanPino,andKalikaBali(Eds.).Association\\nfor Computational Linguistics, 14048–14077.DOI: https://doi.org/10.18653/v1/2023.findings-emnlp.936\\n[278] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with GPT-4.\\narXiv:2304.03277. Retrieved fromhttps://arxiv.org/abs/2304.03277\\n[279] Zhiyuan Peng, Xuyang Wu, Qifan Wang, and Yi Fang. 2023. Soft prompt tuning for augmenting dense retrieval with\\nlarge language models. arXiv:2307.08303. Retrieved fromhttps://arxiv.org/abs/2307.08303\\n[280] Ethan Perez, Sam Ringer, Kamile Lukosiute, Karina Nguyen, Edwin Chenandet al. 2023. Discovering language\\nmodel behaviors with Model-Written evaluations. InFindings of ACL ’23 . Association for Computational Linguistics,\\n13387–13434.\\n[281] Pascal Pfeiffer, Philipp Singer, and Yauhen Babakhin, Gabor Fodor, Nischay Dhankhar, and Sri Satish Ambati. 2024.\\nH2O-Danube3 Technical Report. arXiv:2407.09276. Retrieved fromhttps://arxiv.org/abs/2407.09276\\n[282] Karmvir Singh Phogat, Sai Akhil Puranam, Sridhar Dasaratha, Chetan Harsha, and Shashishekar Ramakrishna.\\n2024. Fine-tuning Smaller Language Models for Question Answering over Financial Documents. InFindings of the\\nAssociation for Computational Linguistics: EMNLP 2024 , Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.).\\nAssociation for Computational Linguistics, 10528–10548.https://doi.org/10.18653/v1/2024.findings-emnlp.617\\n[283] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao,\\nShivani Agrawal, and Jeff Dean. 2023. Efficiently scaling transformer inference.Proc. Mach. Learn. Syst . 5, 606–624.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 78, 'page_label': '79'}, page_content='145:78 F. Wang et al.\\n[284] Ofir Press, Noah Smith, and Mike Lewis. 2022. Train short, test long: Attention with linear biases enables input\\nlength extrapolation. InInternational Conference on Learning Representations . Retrieved fromhttps://openreview.\\nnet/forum?id=R8sQPpGCv0\\n[285] Ruiyang Qin, Jun Xia, Zhenge Jia, Meng Jiang, Ahmed Abbasi, Peipei Zhou, Jingtong Hu, and Yiyu Shi. 2023. Enabling\\non-device large language model personalization with self-supervised data selection and synthesis. arXiv:2311.12275.\\nRetrieved fromhttps://arxiv.org/abs/2311.12275\\n[286] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et\\nal. 2024. ToolLLM: Facilitating large language models to master 16000+ real-world APIs. InThe Twelfth International\\nConference on Learning Representations .\\n[287] Haohao Qu, Liangbo Ning, Rui An, Wenqi Fan, Tyler Derr, Hui Liu, Xin Xu, and Qing Li. 2024. A survey of Mamba.\\narXiv:2408.01129. Retrieved fromhttps://arxiv.org/abs/2408.01129\\n[288] Haohao Qu, Yifeng Zhang, Liangbo Ning, Wenqi Fan, and Qing Li. 2024. SSD4Rec: A structured state space duality\\nmodel for efficient sequential recommendation. arXiv:2409.01192. Retrieved fromhttps://arxiv.org/abs/2409.01192\\n[289] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are\\nunsupervised multitask learners.OpenAI Blog 1, 8 (2019), 9.\\n[290] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. 2023.\\nDirect preference optimization: Your language model is secretly a reward model.Adv. Neural Inf. Process. Syst . 36,\\n(2023), 53728–53741.\\n[291] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\\nPeter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer.J. Mach. Learn.\\nRes. 21, 140 (2020), 1–67.DOI: https://doi.org/10.5555/3455716.3455856\\n[292] Mohammad Wali Ur Rahman, Murad Mehrab Abrar, Hunter Gibbons Copening, Salim Hariri, Sicong Shao,\\nPratik Satam, and Soheil Salehi. 2023. Quantized transformer language model implementations on edge devices.\\narXiv:2310.03971. Retrieved fromhttps://arxiv.org/abs/2310.03971\\n[293] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward\\ntraining trillion parameter models. InSC20: International Conference for High Performance Computing, Networking,\\nStorage and Analysis. IEEE, 1–16.\\n[294] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.\\n2023. In-Context Retrieval-Augmented language models.Trans. Assoc. Comput. Linguist . 11, (2023), 1316–1331.DOI:\\nhttps://doi.org/10.1162/tacl_a_00605\\n[295] Krithika Ramesh, Arnav Chavan, Shrey Pandit, and Sunayana Sitaram. 2023. A comparative study on the impact\\nof model compression techniques on fairness in language models. InProceedings of the 61st Annual Meeting of the\\nAssociation for Computational Linguistics,Vol.1LongPapers,AssociationforComputationalLinguistics,15762–15782.\\nRetrieved fromhttps://aclanthology.org/2023.acl-long.878\\n[296] Al Mamunur Rashid, George Karypis, and John Riedl. 2008. Learning preferences of new users in recommender\\nsystems: An information theoretic approach.ACM SIGKDD Explor. Newsl . 10, 2 (2008), 90–100.DOI: https://doi.org/\\n10.1145/1540276.1540302\\n[297] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. 2024. Androidinthewild: A\\nlarge-scale dataset for android device control.Adv. Neural Inf. Process. Syst . 36,. 2024.\\n[298] Stephen Robertson, and Hugo Zaragoza. 2009. The probabilistic relevance framework: BM25 and beyond.Found.\\nTrends Inf. Ret . 3, 4 (2009), 333–389.DOI: https://doi.org/10.1561/1500000019\\n[299] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal\\nRemez, Jérémy Rapin, et al. 2023. Code Llama: Open foundation models for code. arXiv:2308.12950. Retrieved from\\nhttps://arxiv.org/abs/2308.12950\\n[300] Caitlin Sadowski, and Greg Levin. 2007.Simhash: Hash-Based Similarity Detection . Technical report, Google.\\n[301] KeisukeSakaguchi,RonanLeBras,ChandraBhagavatula,andYejinChoi.2021.Winogrande:Anadversarialwinograd\\nschema challenge at scale.Commun. ACM 64, 9 (2021), 99–106.DOI: https://doi.org/10.1145/3474381\\n[302] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer,\\nNicola Cancedda, and Thomas Scialom. 2024. Toolformer: Language models can teach themselves to use tools.Adv.\\nNeural Inf. Process. Syst . 36, (2024).DOI: https://doi.org/10.18653/v1/2025.realm-1.14\\n[303] Rico Sennrich, Jannis Vamvas, and Alireza Mohammadshahi. 2023. Mitigating hallucinations and off-target machine\\ntranslation with source-contrastive and language-contrastive decoding. arXiv:2309.07098. Retrieved fromhttps:\\n//arxiv.org/abs/2309.07098\\n[304] Zeyang Sha, and Yang Zhang. 2024. Prompt stealing attacks against large language models. arXiv:2402.12959.\\nRetrieved fromhttps://arxiv.org/abs/2402.12959\\n[305] Yu Shang, Yu Li, Fengli Xu, and Yong Li. 2024. Synergy-of-thoughts: Eliciting efficient reasoning in hybrid language\\nmodels. arXiv:2402.02563. Retrieved fromhttps://arxiv.org/abs/2402.02563\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 79, 'page_label': '80'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:79\\n[306] Yuzhang Shang, Zhihang Yuan, Qiang Wu, and Zhen Dong. 2023. PB-LLM: Partially binarized large language models.\\narXiv:231000034. Retrieved fromhttps://arxiv.org/abs/2310.00034\\n[307] Hang Shao, Bei Liu, and Yanmin Qian. 2024. One-shot sensitivity-aware mixed sparsity pruning for large language\\nmodels. InICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE,\\n11296–11300.\\n[308] Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng,\\nEsin Durmus, Zac Hatfield-Dodds, Scott R. Johnston, et al. 2023. Towards understanding sycophancy in language\\nmodels. arXiv:2310.13548. Retrieved fromhttps://arxiv.org/abs/2310.13548\\n[309] Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, and Nael Abu-Ghazaleh. 2023. Survey of\\nvulnerabilities in large language models revealed by adversarial attacks. arXiv:2310.10844. Retrieved fromhttps:\\n//arxiv.org/abs/2310.10844\\n[310] Noam Shazeer. 2019. Fast transformer decoding: One write-head is all you need. arXiv:1911.02150. Retrieved from\\nhttps://arxiv.org/abs/1911.02150\\n[311] Noam Shazeer. 2020. Glu variants improve transformer. arXiv:2002.05202. Retrieved fromhttps://arxiv.org/abs/2002.\\n05202\\n[312] Bowen Shen, Zheng Lin, Yuanxin Liu, Zhengxiao Liu, Lei Wang, and Weiping Wang. 2022. COST-EFF: Collaborative\\noptimization of spatial and temporal efficiency with slenderized multi-exit language models. InProceedings of the 2022\\nConference on Empirical Methods in Natural Language Processing , Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang\\n(Eds.). Association for Computational Linguistics, 1719–1730.https://doi.org/10.18653/v1/2022.emnlp-main.112\\n[313] BowenShen,ZhengLin,DarenZha,WeiLiu,JianLuan,BinWang,andWeipingWang.2024.PruningLargeLanguage\\nModels to Intra-module Low-rank Architecture with Transitional Activations. InFindings of the Association for\\nComputational Linguistics: ACL 2024 , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for\\nComputational Linguistics, 9781–9793.https://doi.org/10.18653/v1/2024.findings-acl.582\\n[314] Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen, Ji Zhang, and Fei Huang.\\n2024. Small LLMs are weak tool learners: A Multi-LLM agent. InProceedings of the 2024 Conference on Empirical\\nMethods in Natural Language Processing . Association for Computational Linguistics, 16658–16680.https://doi.org/10.\\n18653/v1/2024.emnlp-main.929\\n[315] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré,\\nIon Stoica, and Ce Zhang. 2023. Flexgen: High-throughput generative inference of large language models with a\\nsingle gpu. InInternational Conference on Machine Learning . PMLR, 31094–31116.\\n[316] Wentao Shi, Xiangnan He, Yang Zhang, Chongming Gao, Xinyue Li, Jizhi Zhang, Qifan Wang, and Fuli Feng. 2024.\\nLarge language models are learnable planners for long-term recommendation. InProceedings of the 47th International\\nACM SIGIR Conference on Research and Development in Information Retrieval . ACM, 1893–1903.\\n[317] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019.\\nMegatron-LM: Training multi-billion parameter language models using model parallelism. arXiv:1909.08053. Re-\\ntrieved fromhttps://arxiv.org/abs/1909.08053\\n[318] Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, and Chandan K. Reddy. 2024. LLM-SR:\\nScientific equation discovery via programming with large language models. arXiv:2404.18400. Retrieved from\\nhttps://arxiv.org/abs/2404.18400\\n[319] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. 2022.\\nTest-time prompt tuning for zero-shot generalization in vision-language models.Adv. Neural Inf. Process. Syst . 35,\\n(2022), 14274–14289.DOI: https://doi.org/10.5555/3600270.3601308\\n[320] Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay\\nTanwani, Heather Cole-Lewis, Stephen Pfohl, et al. 2023. Large language models encode clinical knowledge.Nature\\n620, 7972 (2023), 172–180.DOI: https://doi.org/10.1038/s41586-023-06291-2\\n[321] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R. Steeves, Joel Hestness, and Nolan Dey. 2023. SlimPajama:\\nA 627B token cleaned and deduplicated version of RedPajama. Retrieved fromhttps://cerebras.ai/blog/slimpajama-a-\\n627b-token-cleaned-and-deduplicated-version-of-redpajama; https://huggingface.co/datasets/cerebras/SlimPajama-\\n627B\\n[322] Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi\\nChandu, Jennifer Dumas, Yanai Elazar, et al. 2024. Dolma: An Open Corpus of Three Trillion Tokens for Language\\nModel Pretraining Research. arXiv:2402.00159. Retrieved fromhttps://arxiv.org/abs/2402.00159\\n[323] Sofia Eleni Spatharioti, David M. Rothschild, Daniel G. Goldstein, and Jake M. Hofman. 2023. Comparing traditional\\nand LLM-based search for consumer choice: A randomized experiment. arXiv:2307.03744. Retrieved fromhttps:\\n//arxiv.org/abs/2307.03744\\n[324] JianlinSu,MurtadhaAhmed,YuLu,ShengfengPan,WenBo,andYunfengLiu.2024.Roformer:Enhancedtransformer\\nwith rotary position embedding.Neurocomputing 568, (2024), 127063.DOI: https://doi.org/10.1016/j.neucom.2023.\\n127063\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 80, 'page_label': '81'}, page_content='145:80 F. Wang et al.\\n[325] Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. 2024. Scieval: A\\nmulti-level large language model evaluation benchmark for scientific research. InProceedings of the AAAI Conference\\non Artificial Intelligence, Vol. 38, 19053–19061.\\n[326] Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan\\nZhang, Xiner Li, et al. 2024. TrustLLM: Trustworthiness in large language models. arXiv:2401.05561. Retrieved from\\nhttps://arxiv.org/abs/2401.05561\\n[327] Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. 2024. A simple and effective pruning approach for large\\nlanguage models. InProceedings of the Twelfth International Conference on Learning Representations . ICLR.\\n[328] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. 2020. Mobilebert: A compact\\ntask-agnostic BERT for resource-limited devices. arXiv:2004.02984. Retrieved fromhttps://arxiv.org/abs/2004.02984\\n[329] Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A. Smith, and\\nYejin Choi. 2020. Dataset cartography: Mapping and diagnosing datasets with training dynamics. InProceedings of\\nthe 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , 9275–9293.\\n[330] Alon Talmor, and Jonathan Berant. 2018. The web as a Knowledge-Base for answering complex questions. In\\nProceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies, Volume 1 (Long Papers),MarilynWalker,HengJi,andAmandaStent(Eds.).Association\\nfor Computational Linguistics, 641–651.DOI: https://doi.org/10.18653/v1/N18-1059\\n[331] Jiejun Tan, Zhicheng Dou, Yutao Zhu, Peidong Guo, Kun Fang, and Ji-Rong Wen. 2024. Small models, big insights:\\nleveraging slim proxy models to decide when and what to retrieve for LLMs. arXiv:2402.12052. Retrieved from\\nhttps://arxiv.org/abs/2402.12052\\n[332] QiaoyuTang,ZiliangDeng,HongyuLin,XianpeiHan,QiaoLiang,BoxiCao,andLeSun.2023.Toolalpaca:Generalized\\ntool learning for language models with 3000 simulated cases. arXiv:2306.05301. Retrieved fromhttps://arxiv.org/abs/\\n2306.05301\\n[333] Xuemei Tang, Jun Wang, and Qi Su. 2024. Small language model is a good guide for large language model in Chinese\\nentity relation extraction. arXiv:2402.14373. Retrieved fromhttps://arxiv.org/abs/2402.14373\\n[334] Yehui Tang, Fangcheng Liu, Yunsheng Ni, Yuchuan Tian, Zheyuan Bai, Yi-Qi Hu, Sichao Liu, Shangling Jui, Kai\\nHan, and Yunhe Wang. 2024. Rethinking optimization and architecture for tiny language models. arXiv:2402.02791.\\nRetrieved fromhttps://arxiv.org/abs/2402.02791\\n[335] RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,CarlosGuestrin,PercyLiang,andTatsunoriB.\\nHashimoto. 2023. Stanford Alpaca: An instruction-following LLaMA model. Retrieved fromhttps://github.com/tatsu-\\nlab/stanford_alpaca\\n[336] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton,\\nViktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. arXiv:2211.09085. Retrieved\\nfrom https://arxiv.org/abs/2211.09085\\n[337] CodeGemma Team. 2024. CodeGemma: Open code models based on Gemma. arXiv:2406.11409. Retrieved from\\nhttps://arxiv.org/abs/2406.11409\\n[338] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre,\\nMorgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. 2024. Gemma: Open models based on gemini research and\\ntechnology. arXiv:2403.08295. Retrieved fromhttps://arxiv.org/abs/2403.08295\\n[339] MorganeRiviere,ShreyaPathak,Pier GiuseppeSessa, Cassidy Hardin,Surya Bhupatiraju, LéonardHussenot, Thomas\\nMesnard, Bobak Shahriari, Alexandre Ramé, et al. 2024. Gemma 2: Improving open language models at a practical\\nsize. arXiv:2408.00118. Retrieved fromhttps://arxiv.org/abs/2408.00118\\n[340] TensorOpera Team. 2024.TensorOpera Unveils Fox Foundation Model: A Pioneering Small Language Model (SLM) for\\nCloud and Edge . Retrieved fromhttps://blog.tensoropera.ai/tensoropera-unveils-fox-foundation-model-a-pioneering-\\nopen-source-slm-leading-the-way-against-tech-giants/. Accessed: June 13, 2024.\\n[341] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. 2016. Branchynet: Fast inference via early exiting\\nfrom deep neural networks. In2016 23rd International Conference on Pattern Recognition (ICPR) . IEEE, 2464–2469.\\n[342] Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M. Anwer, Michael Felsberg, Tim Baldwin,\\nEric P. Xing, and Fahad Shahbaz Khan. 2024. Mobillama: Towards accurate and lightweight fully transparent GPT.\\narXiv:2402.16840. Retrieved fromhttps://arxiv.org/abs/2402.16840\\n[343] Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. 2024. Toward self-improvement\\nof LLMs via imagination, searching, and criticizing. arXiv:2404.12253. Retrieved fromhttps://arxiv.org/abs/arXiv:\\n2404.12253.\\n[344] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste\\nRozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models.\\narXiv:2302.13971. Retrieved fromhttps://arxiv.org/abs/2302.13971\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 81, 'page_label': '82'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:81\\n[345] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.\\narXiv:2307.09288. Retrieved fromhttps://arxiv.org/abs/2307.09288\\n[346] Jonathan Tow, Marco Bellagente, Dakota Mahan, and Carlos Riquelme. 2024. StableLM 3B 4E1T. Retrieved from\\nhttps://huggingface.co/stabilityai/stablelm-3b-4e1t\\n[347] Trieu H. Trinh, and Quoc V. Le. 2018. A simple method for commonsense reasoning. arXiv:1806.02847. Retrieved\\nfrom https://arxiv.org/abs/1806.02847\\n[348] Adina Trufinescu. 2024. Discover the New Multi-Lingual High-Quality Phi-3.5 SLMs. Retrieved from\\nhttps://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/discover-the-new-multi-lingual-high-quality-phi-\\n3-5-slms/ba-p/4225280\\n[349] Dennis Ulmer, Martin Gubri, Hwaran Lee, Sangdoo Yun, and Seong Joon Oh. 2024. Calibrating large language models\\nusing their generations only. arXiv:2403.05973. Retrieved fromhttps://arxiv.org/abs/2403.05973\\n[350] Sander Van Der Linden. 2022. Misinformation: Susceptibility, spread, and interventions to immunize the public.Nat.\\nMed. 28, 3 (2022), 460–467.DOI: https://doi.org/10.1038/s41591-022-01713-6\\n[351] Chien Van Nguyen, Xuan Shen, Ryan Aponte, Yu Xia, Samyadeep Basu, Zhengmian Hu, Jian Chen, Mihir Parmar,\\nSasidhar Kunapuli, Joe Barrow, et al. 2024. A survey of small language models. arXiv:2410.20011. Retrieved from\\nhttps://arxiv.org/abs/2410.20011\\n[352] A. Vaswani. 2017. Attention is all you need.Adv. Neural Inf. Process. Syst . 30 (2017), 5998–6008.DOI: https://doi.org/\\n10.5555/3295222.3295349\\n[353] Olga Veksler. 2023. Test time adaptation with regularized loss for weakly supervised salient object detection. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 7360–7369.\\n[354] Paul Voigt, and Axel Von Dem Bussche. 2017. The EU general data protection regulation (GDPR).A Practical Guide\\n(1st ed.). Springer International Publishing, Cham.\\n[355] Yuxian Wan, Wenlin Zhang, and Zhen Li. 2023. Multi-Task feature Self-Distillation for Semi-Supervised machine\\ntranslation. InInternational Conference on Neural Information Processing . Springer, 238–254.\\n[356] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A Multi-Task\\nbenchmark and analysis platform for natural language understanding. InProceedings of the 2018 EMNLP Workshop\\nBlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , 353–355.\\n[357] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik\\nDutta, Rylan Schaeffer, et al. 2023. DecodingTrust: A comprehensive assessment of trustworthiness in GPT models.\\nIn Proceedings of the Annual Conference on Neural Information Processing Systems .\\n[358] Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, and Bo Li.\\n2021. Adversarial glue: A multi-task benchmark for robustness evaluation of language models. arXiv:2111.02840.\\nRetrieved fromhttps://arxiv.org/abs/2111.02840\\n[359] Fali Wang, Minhua Lin, Yao Ma, Hui Liu, Qi He, Xianfeng Tang, Jiliang Tang, Jian Pei, and Suhang Wang. 2025. A\\nsurvey on small language models in the era of large language models: Architecture, capabilities, and trustworthiness.\\nIn. Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2 (KDD ’25) . ACM,\\n6173–6183. DOI: https://doi.org/10.1145/3711896.3736563\\n[360] Fali Wang, Hui Liu, Zhenwei Dai, Jingying Zeng, Zhiwei Zhang, Zongyu Wu, Chen Luo, Zhen Li, Xianfeng Tang, Qi\\nHe, et al. 2025. AgentTTS: Large language model agent for test-time compute-optimal scaling strategy in complex\\ntasks. arXiv:2508.00890. Retrieved fromhttps://arxiv.org/abs/2508.00890\\n[361] Guan Wang, Sijie Cheng, Qiying Yu, and Changling Liu. 2023. OpenLLMs: Less is More for Open-Source Models.\\nDOI: https://doi.org/10.5281/zenodo.8105775\\n[362] Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2024. OpenChat: Advancing open-\\nsource language models with Mixed-Quality data. InThe Twelfth International Conference on Learning Representations.\\nRetrieved fromhttps://openreview.net/forum?id=AOJyfhWYHf\\n[363] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi\\nWu, and Furu Wei. 2023. Bitnet: Scaling 1-bit transformers for large language models. arXiv:2310.11453. Retrieved\\nfrom https://arxiv.org/abs/2310.11453\\n[364] Jindong Wang, H. U. Xixu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Wei Ye, Haojun Huang,\\nXiubo Geng, et al. n.d. On the robustness of ChatGPT: An adversarial and out-of-distribution perspective. InICLR\\n2023 Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models .\\n[365] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024. Improving text\\nembeddingswithlargelanguagemodels.In Proceedings of the 62nd Annual Meeting of the Association for Computational\\nLinguistics, Vol. 1 Long Papers. Association for Computational Linguistics, 11897–11916.https://doi.org/10.18653/v1/\\n2024.acl-long.642\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 82, 'page_label': '83'}, page_content='145:82 F. Wang et al.\\n[366] Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, and Xiang Ren. 2023. SCOTT: Self-Consistent\\nchain-of-thoughtdistillation.In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics,\\nVol. 1 Long Papers, Association for Computational Linguistics, 5546–5558.\\n[367] Wenxiao Wang, Wei Chen, Yicong Luo, Yongliu Long, Zhengkai Lin, Liye Zhang, Binbin Lin, Deng Cai, and Xiaofei\\nHe. 2024. Model compression and efficient inference for large language models: A survey. arXiv:2402.09748. Retrieved\\nfrom https://arxiv.org/abs/2402.09748\\n[368] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. MiniLM: Deep self-attention\\ndistillation for task-agnostic compression of pre-trained transformers. arXiv:2002.10957. Retrieved fromhttps:\\n//arxiv.org/abs/2002.10957\\n[369] Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang\\nZhang, Yizhou Sun, and Wei Wang. 2024. SciBench: Evaluating College-Level scientific Problem-Solving abilities\\nof large language models. InForty-First International Conference on Machine Learning . Retrieved fromhttps://\\nopenreview.net/forum?id=bq1JEgioLr\\n[370] Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy Baldwin. 2024. Do-Not-Answer: Evaluating Safe-\\nguards in LLMs. InFindings of the Association for Computational Linguistics: EACL ’24 . Association for Computational\\nLinguistics, 896–911. Retrieved fromhttps://aclanthology.org/2024.findings-eacl.61\\n[371] Yile Wang, Peng Li, Maosong Sun, and Yang Liu. 2023. Self-Knowledge guided retrieval augmentation for large lan-\\nguage models. InFindings of the Association for Computational Linguistics: EMNLP ’23 . Association for Computational\\nLinguistics, 10303–10315.\\n[372] Yubo Wang, Xueguang Ma, and Wenhu Chen. 2023. Augmenting black-box LLMS with medical textbooks for clinical\\nquestion answering. arXiv:2309.02233. Retrieved fromhttps://arxiv.org/abs/2309.02233\\n[373] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaeiandet al. 2022. Super-\\nNaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. InEMNLP ’22 . Association for\\nComputational Linguistics, 5085–5109.DOI: https://doi.org/10.18653/v1/2022.emnlp-main.340\\n[374] Yuyan Wang, Mohit Sharma, Can Xu, Sriraj Badam, Qian Sun, Lee Richardson, Lisa Chung, Ed. H. Chi, and Minmin\\nChen. 2022. Surrogate for Long-Term user experience in recommender systems. InProceedings of the 28th ACM\\nSIGKDD Conference on Knowledge Discovery and Data Mining.ACM,4100–4109. DOI: https://doi.org/10.1145/3534678.\\n3539073\\n[375] Yuling Wang, Changxin Tian, Binbin Hu, Yanhua Yu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou, Liang Pang, and Xiao\\nWang. 2024. Can small language models be good reasoners for sequential recommendation? InProceedings of the\\nACM on Web Conference 2024 . ACM, 3876–3887.\\n[376] Yuqing Wang, and Yun Zhao. 2024. RUPBench: Benchmarking reasoning under perturbations for robustness evalua-\\ntion in large language models. arXiv:2406.11020. Retrieved fromhttps://arxiv.org/abs/2406.11020\\n[377] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai,\\nand Quoc V. Le. 2022. Finetuned language models are Zero-Shot learners. InInternational Conference on Learning\\nRepresentations. Retrieved fromhttps://openreview.net/forum?id=gEZrGCozdqR\\n[378] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2023. Magicoder: Source code is all you\\nneed. arXiv:2312.02120. Retrieved fromhttps://arxiv.org/abs/2312.02120\\n[379] Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne Hendricks, Kirsty\\nAnderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang. 2021. Challenges in detoxifying language models. In\\nFindings of the Association for Computational Linguistics: EMNLP ’21 . Association for Computational Linguistics,\\n2447–2469. DOI: https://doi.org/10.18653/v1/2021.findings-emnlp.210\\n[380] Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice science questions. In\\nProceedings of the 3rd Workshop on Noisy User-Generated Text , 94–106.\\n[381] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang,\\nand Yunxin Liu. 2024. AutoDroid: LLM-powered task automation in android. arXiv:2308.15272. Retrieved from\\nhttps://arxiv.org/abs/2308.15272\\n[382] Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. 2024. QuRating: Selecting High-Quality data\\nfor training language models. InForty-First International Conference on Machine Learning . Retrieved fromhttps:\\n//openreview.net/forum?id=GLGYYqPwjy\\n[383] Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A Broad-Coverage challenge corpus for sentence\\nunderstandingthroughinference.In Proceedings of the 2018 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies , Vol. 1 Long Papers, Association for Computational\\nLinguistics, 1112–1122.DOI: https://doi.org/10.18653/v1/N18-1101\\n[384] Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2021. Empowering news recommendation with pre-trained\\nlanguage models. InProceedings of the 44th International ACM SIGIR Conference on Research and Development in\\nInformation Retrieval . ACM, 1652–1656.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 83, 'page_label': '84'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:83\\n[385] Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Fikri Aji. 2024. Minghao LaMini-LM: A\\ndiverse herd of distilled models from Large-Scale instructions. InProceedings of the 18th Conference of the European\\nChapter of the Association for Computational Linguistics (Volume 1: Long Papers) , Yvette Graham and Matthew Purver\\n(Eds.). Association for Computational Linguistics, 944–964. Retrieved fromhttps://aclanthology.org/2024.eacl-long.57\\n[386] Taiqiang Wu, Cheng Hou, Shanshan Lao, Jiayi Li, Ngai Wong, Zhe Zhao, and Yujiu Yang. 2024. Weight-Inherited\\nDistillation for Task-Agnostic BERT Compression. InFindings of the Association for Computational Linguistics:\\nNAACL 2024, Kevin Duh, Helena Gomez, and Steven Bethard (Eds.). Association for Computational Linguistics,\\n13–28. https://doi.org/10.18653/v1/2024.findings-naacl.2\\n[387] Xuansheng Wu, Huachi Zhou, Yucheng Shi, Wenlin Yao, Xiao Huang, and Ninghao Liu. 2024. Could small language\\nmodels serve as recommenders? Towards data-centric cold-start recommendation. InProceedings of the ACM on Web\\nConference 2024. ACM, 3566–3575.\\n[388] Zhuofeng Wu, He Bai, Aonan Zhang, Jiatao Gu, V. G. Vydiswaran, Navdeep Jaitly, and Yizhe Zhang. 2024. Divide-or-\\nconquer? Which part should you distill your LLM? arXiv:2402.15000. Retrieved fromhttps://arxiv.org/abs/2402.15000\\n[389] Nuwa Xi, Yuhan Chen, Sendong Zhao, Haochun Wang, Bing Qin, and Ting Liu. 2024. AS-ES learning: Towards\\nefficient CoT learning in small models. arXiv:2403.01969. Retrieved fromhttps://arxiv.org/abs/2403.01969\\n[390] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. 2024. Sheared LLaMA: Accelerating language model\\npre-training via structured pruning. InThe Twelfth International Conference on Learning Representations . Retrieved\\nfrom https://openreview.net/forum?id=09iOdaeOzp\\n[391] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. 2023. Smoothquant: Accurate\\nand efficient post-training quantization for large language models. InInternational Conference on Machine Learning .\\nPMLR, 38087–38099.\\n[392] Tinghao Xie, Xiangyu Qi, Yi Zeng, Yangsibo Huang, Udari Madhushani Sehwag, Kaixuan Huang, Luxi He, Boyi Wei,\\nDacheng Li, Ying Sheng, et al. 2024. Sorry-bench: Systematically evaluating large language model safety refusal\\nbehaviors. arXiv:2406.14598. Retrieved fromhttps://arxiv.org/abs/2406.14598\\n[393] Tong Xie, Yuwei Wan, Wei Huang, Zhenyu Yin, Yixuan Liu, Shaozhou Wang, Qingyuan Linghu, Chunyu Kit, Clara\\nGrazian, Wenjie Zhang, et al. 2023. Darwin series: Domain specific large language models for natural science.\\narXiv:2308.13565. Retrieved fromhttps://arxiv.org/abs/2308.13565\\n[394] Weikai Xie, Li Zhang, Shihe Wang, Rongjie Yi, and Mengwei Xu. 2024. DroidCall: A dataset for LLM-powered\\nandroid intent invocation. arXiv:2412.00402. Retrieved fromhttps://arxiv.org/abs/2412.00402\\n[395] Xuan Xie, Jiayang Song, Zhehua Zhou, Yuheng Huang, Da Song, and Lei Ma. 2024. Online safety analysis for LLMs:\\nA benchmark, an assessment, and a path forward. arXiv:2404.08517. Retrieved fromhttps://arxiv.org/abs/2404.08517\\n[396] Ji Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin. 2021. BERxiT: Early exiting for BERT with better Fine-Tuning\\nand extension to regression. InProceedings of the 16th Conference of the European Chapter of the Association for\\nComputational Linguistics: Main Volume , Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty (Eds.). Association for\\nComputational Linguistics, Online, 91–104.https://doi.org/10.18653/v1/2021.eacl-main.8\\n[397] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023.\\nWizardlm: Empowering large language models to follow complex instructions. arXiv:2304.12244. Retrieved from\\nhttps://arxiv.org/abs/2304.12244\\n[398] Canwen Xu, Yichong Xu, Shuohang Wang, Yang Liu, Chenguang Zhu, and Julian McAuley. 2023. Small models are\\nvaluable plug-ins for large language models. arXiv:2305.08848. Retrieved fromhttps://arxiv.org/abs/2305.08848\\n[399] Daliang Xu, Wangsong Yin, Xin Jin, Ying Zhang, Shiyun Wei, Mengwei Xu, and Xuanzhe Liu. 2023. LLMCad: Fast and\\nscalable on-device large language model inference. arXiv:2309.04255. Retrieved fromhttps://arxiv.org/abs/2309.04255\\n[400] Daliang Xu, Hao Zhang, Liming Yang, Ruiqi Liu, Gang Huang, Mengwei Xu, and Xuanzhe Liu. 2024. Empowering\\n1000 tokens/second on-device LLM prefilling with MLLM-NPU. arXiv:2407.05858. Retrieved fromhttps://arxiv.org/\\nabs/2407.05858\\n[401] Jiaming Xu, Peng Wang, Guanhua Tian, Bo Xu, Jun Zhao, Fangyuan Wang, and Hongwei Hao. 2015. Short text\\nclustering via convolutional neural networks. InProceedings of the 1st Workshop on Vector Space Modeling for\\nNatural Language Processing , Phil Blunsom, Shay Cohen, Paramveer Dhillon, and Percy Liang (Eds.). Association for\\nComputational Linguistics, 62–69.DOI: https://doi.org/10.3115/v1/W15-1509\\n[402] Minrui Xu, Niyato Dusit, Jiawen Kang, Zehui Xiong, Shiwen Mao, Zhu Han, Dong In Kim, and Khaled B. Letaief.\\n2024. When large language model agents meet 6G networks: Perception, grounding, and alignment. arXiv:2401.07764.\\nRetrieved fromhttps://arxiv.org/abs/2401.07764\\n[403] Mengwei Xu, Wangsong Yin, Dongqi Cai, Rongjie Yi, Daliang Xu, Qipeng Wang, Bingyang Wu, Yihao Zhao, Chen\\nYang,ShiheWang,etal.2024.Asurveyofresource-efficientllmandmultimodalfoundationmodels.arXiv:2401.08092.\\nRetrieved fromhttps://arxiv.org/abs/2401.08092\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 84, 'page_label': '85'}, page_content='145:84 F. Wang et al.\\n[404] Weijia Xu, Sweta Agrawal, Eleftheria Briakou, Marianna Martindale, and Marine Carpuat. 2023. Understanding and\\ndetecting hallucinations in neural machine translation via model introspection.Trans. Assoc. Comput. Linguist . 11,\\n(2023), 546–564.DOI: https://doi.org/10.1162/tacl_a_00563\\n[405] Yuzhuang Xu, Xu Han, Zonghan Yang, Shuo Wang, Qingfu Zhu, Zhiyuan Liu, Weidong Liu, and Wanxiang Che.\\n2024. OneBit: Towards extremely low-bit large language models. arXiv:2402.11295. Retrieved fromhttps://arxiv.org/\\nabs/2402.11295\\n[406] Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective retrieval augmented generation.\\narXiv:2401.15884. Retrieved fromhttps://arxiv.org/abs/2401.15884\\n[407] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng\\nLiu,FeiHuang,etal.2024.Qwen2TechnicalReport.arXiv:2407.10671.Retrievedfrom https://arxiv.org/abs/2407.10671\\n[408] Chuanpeng Yang, Wang Lu, Yao Zhu, Yidong Wang, Qian Chen, Chenlong Gao, Bingjie Yan, and Yiqiang Chen. 2024.\\nSurvey on knowledge distillation for large language models: Methods, evaluation, and application. arXiv:2407.01885.\\nRetrieved fromhttps://arxiv.org/abs/2407.01885\\n[409] Cheng Yang, Yang Sui, Jinqi Xiao, Lingyi Huang, Yu Gong, Yuanlin Duan, Wenqi Jia, Miao Yin, Yu Cheng, and\\nBo Yuan. 2024. MoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert\\nLow-Rank Decomposition. InFindings of the Association for Computational Linguistics: EMNLP 2024 , Yaser Al-\\nOnaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, 10456–10466.DOI:\\nhttps://doi.org/10.18653/v1/2024.findings-emnlp.612\\n[410] Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, and Yuandong Tian. 2024. RLCD: Reinforcement learning\\nfrom contrastive distillation for LM alignment. InThe Twelfth International Conference on Learning Representations .\\nRetrieved fromhttps://openreview.net/forum?id=v3XXtxWKi6\\n[411] Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, Jimin Huang, and Sophia Ananiadou. 2024. MentaLLaMA:\\nInterpretable mental health analysis on social media with large language models. InProceedings of the ACM on Web\\nConference 2024, 4489–4500.\\n[412] Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue Zhang.\\n2023. GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-Distribution Generalization\\nPerspective. InFindings of the Association for Computational Linguistics: ACL ’23 . Association for Computational\\nLinguistics, 12731–12750.DOI: https://doi.org/10.18653/v1/2023.findings-acl.806\\n[413] Linyi Yang, Shuibai Zhang, Zhuohao Yu, Guangsheng Bao, Yidong Wang, Jindong Wang, Ruochen Xu, Wei Ye, Xing\\nXie, Weizhu Chen, and Yue Zhang. 2024. Supervised knowledge makes large language models better In-context\\nlearners. InThe Twelfth International Conference on Learning Representations . Retrieved fromhttps://openreview.net/\\nforum?id=bAMPOUF227\\n[414] Runming Yang, Taiqiang Wu, Jiahao Wang, Pengfei Hu, Ngai Wong, and Yujiu Yang. 2024. LLM-Neo: Parameter\\nefficient knowledge distillation for large language models. arXiv:2411.06839. Retrieved fromhttps://arxiv.org/abs/\\n2411.06839\\n[415] YifeiYang,ZouyingCao,andHaiZhao.2024.Laco:Largelanguagemodelpruningvialayercollapse.arXiv:2402.11187.\\nRetrieved fromhttps://arxiv.org/abs/2402.11187\\n[416] Yu Yang, Siddhartha Mishra, Jeffrey N. Chiang, and Baharan Mirzasoleiman. 2024. SmallToLarge (S2L): Scalable\\ndata selection for fine-tuning large language models by summarizing training trajectories of small models. InThe\\nThirty-Eighth Annual Conference on Neural Information Processing Systems . Retrieved fromhttps://openreview.net/\\nforum?id=K9IGlMQpif\\n[417] Yizhe Yang, Huashan Sun, Jiawei Li, Runheng Liu, Yinghao Li, Yuhang Liu, Heyan Huang, and Yang Gao. 2023.\\nMindllm: Pre-training lightweight large language model from scratch, evaluations and domain applications.\\narXiv:2310.15777. Retrieved fromhttps://arxiv.org/abs/2310.15777\\n[418] Zhou Yang, Zhaochun Ren, Wang Yufeng, Shizhong Peng, Haizhou Sun, Xiaofei Zhu, and Xiangwen Liao. 2024. En-\\nhancing empathetic response generation by augmenting LLMs with small-scale empathetic models. arXiv:2402.11801.\\nRetrieved fromhttps://arxiv.org/abs/2402.11801\\n[419] YunzhiYao,ShaohanHuang,WenhuiWang,LiDong,andFuruWei.2021.Adapt-and-distill:Developingsmall,fastand\\neffective pretrained language models for domains. arXiv:2106.13474. Retrieved fromhttps://arxiv.org/abs/2106.13474.\\n[420] Mert Yazan, Suzan Verberne, and Frederik Situmeang. 2024. The impact of quantization on retrieval-augmented\\ngeneration: An analysis of small LLMs. arXiv:2406.10251. Retrieved fromhttps://arxiv.org/abs/2406.10251\\n[421] Rongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shangguang Wang, and Mengwei Xu. 2023. EdgeMoE: Fast on-device\\ninference of MoE-based large language models. arXiv:2308.14352. Retrieved fromhttps://arxiv.org/abs/2308.14352\\n[422] Rongjie Yi, Xiang Li, Weikai Xie, Zhenyan Lu, Chenghua Wang, Ao Zhou, Shangguang Wang, Xiwen Zhang, and\\nMengwei Xu. 2024. PhoneLM: An efficient and capable small language model family through principled pre-training.\\narXiv:2411.05046. Retrieved fromhttps://arxiv.org/abs/2411.05046\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 85, 'page_label': '86'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:85\\n[423] Wen-Tau Yih, Matthew Richardson, Chris Meek, Ming-Wei Chang, and Jina Suh. 2016. The value of semantic parse\\nlabeling for knowledge base question answering. InProceedings of the 54th Annual Meeting of the Association for Com-\\nputational Linguistics (Volume 2: Short Papers) , Katrin Erk and Noah A. Smith (Eds.). Association for Computational\\nLinguistics, 201–206.DOI: https://doi.org/10.18653/v1/P16-2033\\n[424] Wangsong Yin, Mengwei Xu, Yuanchun Li, and Xuanzhe Liu. 2024. LLM as a system service on mobile devices.\\narXiv:240311805. Retrieved fromhttps://arxiv.org/abs/2403.11805\\n[425] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller,\\nand Weiyang Liu. 2024. MetaMath: Bootstrap your own mathematical questions for large language models. InThe\\nTwelfth International Conference on Learning Representations . Retrieved fromhttps://openreview.net/forum?id=\\nN8N0hgNDRt\\n[426] Yue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, Mohammad Shoeybi, and Bryan Catanzaro.\\n2024. Rankrag: Unifying context ranking with retrieval-augmented generation in LLMS. arXiv:2407.02485. Retrieved\\nfrom https://arxiv.org/abs/2407.02485\\n[427] Zhongzhi Yu, Zheng Wang, Yuhan Li, Haoran You, Ruijie Gao, Xiaoya Zhou, Sreenidhi Reedy Bommu, Yang Katie\\nZhao, and Yingyan Celine Lin. 2024. EDGE-LLM: Enabling efficient large language model adaptation on edge\\ndevices via layerwise unified compression and adaptive layer tuning and voting. arXiv:2406.15758. Retrieved from\\nhttps://arxiv.org/abs/2406.15758\\n[428] Jinliang Yuan, Chen Yang, Dongqi Cai, Shihe Wang, Xin Yuan, Zeling Zhang, Xiang Li, Dingge Zhang, Hanzi Mei,\\nXianqing Jia, et al. 2024. Mobile foundation model as firmware. InProceedings of the 30th Annual International\\nConference on Mobile Computing and Networking , 279–295.\\n[429] Sha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding, Xiao Liu, Yukuo Cen, Xu Zou, Zhilin Yang, and Jie Tang. 2021.\\nWudaocorpora: A super large-scale Chinese Corpora for pre-training language models.AI Open 2, (2021), 65–68.\\nDOI: https://doi.org/10.1016/j.aiopen.2021.06.001\\n[430] Xiaohan Yuan, Jinfeng Li, Dongxia Wang, Yuefeng Chen, Xiaofeng Mao, Longtao Huang, Hui Xue, Wenhai Wang,\\nKui Ren, and Jingyi Wang. 2024. S-Eval: Automatic and adaptive test generation for benchmarking safety evaluation\\nof large language models. arXiv:2405.14191. Retrieved fromhttps://arxiv.org/abs/2405.14191\\n[431] Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen Tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. 2024.\\nGPT-4 is too smart to Be safe: Stealthy chat with LLMs via cipher. InThe Twelfth International Conference on Learning\\nRepresentations. Retrieved fromhttps://openreview.net/forum?id=MbfAK4s61A.\\n[432] Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen, Shujun Liu, Yuxuan Zhou, Yao Xiao, Song Yun,\\nWei Lin, et al. 2023. Disc-lawllm: Fine-tuning large language models for intelligent legal services. arXiv:2309.11325.\\nRetrieved fromhttps://arxiv.org/abs/2309.11325\\n[433] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2024. Mammoth:\\nBuilding math generalist models through hybrid instruction tuning. InThe Twelfth International Conference on\\nLearning Representations. Retrieved fromhttps://openreview.net/forum?id=yLClGs770I\\n[434] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a machine really\\nfinish your sentence?. InProceedings of the 57th Annual Meeting of the Association for Computational Linguistics .\\nAssociation for Computational Linguistics, 4791–4800.\\n[435] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019.\\nDefending against neural fake news.Adv. Neural Inf. Process. Syst . 32, (2019).DOI: https://doi.org/10.5555/3454287.\\n3455099\\n[436] Biao Zhang, and Rico Sennrich. 2019. Root mean square layer normalization.Adv. Neural Inf. Process. Syst . 32, (2019),\\n12381–12392. DOI: https://doi.org/10.5555/3454287.3455397\\n[437] Cheng Zhang, Jianyi Cheng, George A. Constantinides, and Yiren Zhao. 2024. LQER: low-rank quantization error\\nreconstruction for LLMs. arXiv:2402.02446. Retrieved fromhttps://arxiv.org/abs/2402.02446\\n[438] Collin Zhang, John X. Morris, and Vitaly Shmatikov. 2024. Extracting prompts by inverting LLM outputs.\\narXiv:2405.15012. Retrieved fromhttps://arxiv.org/abs/2405.15012\\n[439] Chen Zhang, Dawei Song, Zheyu Ye, and Yan Gao. 2023. Towards the law of capacity gap in distilling language\\nmodels. arXiv:2311.07052. Retrieved fromhttps://arxiv.org/abs/2311.07052\\n[440] Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, and\\nJie Tang. 2024. Sciglm: Training scientific language models with self-reflective instruction annotation and tuning.\\narXiv:2401.07950. Retrieved fromhttps://arxiv.org/abs/2401.07950\\n[441] Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue,\\nDongzhan Zhou, et al. 2024. ChemLLM: A chemical large language model. arXiv:2402.06852. Retrieved fromhttps:\\n//arxiv.org/abs/2402.06852\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 86, 'page_label': '87'}, page_content='145:86 F. Wang et al.\\n[442] Kaiyan Zhang, Jianyu Wang, Ermo Hua, Biqing Qi, Ning Ding, and Bowen Zhou. 2024. Cogenesis: A framework\\ncollaborating large and small language models for secure context-aware instruction following. arXiv:2403.03129.\\nRetrieved fromhttps://arxiv.org/abs/2403.03129\\n[443] Mingyang Zhang, Hao Chen, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, and Bohan Zhuang. 2023. Loraprune:\\nPruning meets low-rank parameter-efficient fine-tuning. arXiv:2305.18403. Retrieved fromhttps://arxiv.org/abs/\\n2305.18403\\n[444] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. 2024. TinyLlama: An open-source small language\\nmodel. arXiv:240102385. Retrieved fromhttps://arxiv.org/abs/2401.02385\\n[445] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona\\nDiab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv:2205.01068.\\nRetrieved fromhttps://arxiv.org/abs/2205.01068\\n[446] Xinran Zhang, Xin Yuan, Yunwei Li, and Yanru Zhang. 2019. Cold-Start representation learning: A recommendation\\napproach with bert4Movie and movie2Vec. InProceedings of the 27th ACM International Conference on Multimedia .\\nACM, 2612–2616.\\n[447] Yingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu Hou, and Carlo Vittorio Cannistraci. 2024. Plug-and-play:\\nAn efficient post-training pruning method for large language models. InThe Twelfth International Conference on\\nLearning Representations.\\n[448] Yiming Zhang, Nicholas Carlini, and Daphne Ippolito. 2024. Effective prompt extraction from language models.\\narXiv:230706865. Retrieved fromhttps://arxiv.org/abs/2307.06865\\n[449] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian,\\nChristopherRé,ClarkBarrett,etal.2024.H 2O:Heavy-hitteroracleforefficientgenerativeinferenceoflargelanguage\\nmodels. Adv. Neural Inf. Process. Syst . 36, (2024), 34661–34710.DOI: https://doi.org/10.5555/3666122.3667628\\n[450] Bowen Zhao, Hannaneh Hajishirzi, and Qingqing Cao. 2024. APT: Adaptive pruning and tuning pretrained language\\nmodels for efficient training and inference. InForty-First International Conference on Machine Learning .\\n[451] Junchen Zhao, Yurun Song, Simeng Liu, Ian G. Harris, and Sangeetha Abdu Jyothi. 2023. LinguaLinked: A distributed\\nlarge language model inference system for mobile devices. arXiv:2312.00388. Retrieved fromhttps://arxiv.org/abs/\\n2312.00388\\n[452] Juntao Zhao, Borui Wan, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. LLM-PQ: Serving LLM on heterogeneous\\nclusters with phase-aware partition and adaptive quantization. arXiv:2403.01136. Retrieved fromhttps://arxiv.org/\\nabs/2403.01136\\n[453] Kun Zhao, Bohao Yang, Chen Tang, Chenghua Lin, and Liang Zhan. 2024. SLIDE: A framework integrating small\\nand large language models for open-domain dialogues evaluation. arXiv:2405.15924. Retrieved fromhttps://arxiv.\\norg/abs/2405.15924\\n[454] Theodore Zhao, Mu Wei, J. Samuel Preston, and Hoifung Poon. 2023. Automatic calibration and error correction for\\nlarge language models via Pareto optimal self-supervision. arXiv:2306.16564. Retrieved fromhttps://arxiv.org/abs/\\n2306.16564\\n[455] Youpeng Zhao, Ming Lin, Huadong Tang, Qiang Wu, and Jun Wang. 2024. Merino: entropy-driven design for\\ngenerative language models on IoT devices. arXiv:240307921. Retrieved fromhttps://arxiv.org/abs/2403.07921\\n[456] Zhengyun Zhao, Qiao Jin, Fangyuan Chen, Tuorui Peng, and Sheng Yu. 2022. PMC-patients: A large-scale dataset of\\npatientsummariesandrelationsforbenchmarkingretrieval-basedclinicaldecisionsupportsystems.arXiv:2202.13876.\\nRetrieved fromhttps://arxiv.org/abs/2202.13876\\n[457] Zhanhui Zhou, Zhixuan Liu, Jie Liu, Zhichen Dong, Chao Yang, and Yu Qiao. 2024. Weak-to-strong search: Align\\nlarge language models via searching over small language models. InThe Thirty-Eighth Annual Conference on Neural\\nInformation Processing Systems . Retrieved fromhttps://openreview.net/forum?id=dOJ6CqWDf1\\n[458] Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng\\nChua. 2021. TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance. In\\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International\\nJoint Conference on Natural Language Processing , Vol. 1 Long Papers, Association for Computational Linguistics,\\n3277–3287.\\n[459] Jiachen Zhu, Jianghao Lin, Xinyi Dai, Bo Chen, Rong Shan, Jieming Zhu, Ruiming Tang, Yong Yu, and Weinan Zhang.\\n2024. Lifelong personalized low-rank adaptation of large language models for recommendation. arXiv:2408.03533.\\nRetrieved fromhttps://arxiv.org/abs/2408.03533\\n[460] Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue Zhang,\\nNeil Zhenqiang Gong, et al. 2023. Promptbench: Towards evaluating the robustness of large language models on\\nadversarial prompts. arXiv:2306.04528. Retrieved fromhttps://arxiv.org/abs/2306.04528\\n[461] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. 2023. A survey on model compression for large language\\nmodels. arXiv:2308.07633. Retrieved fromhttps://arxiv.org/abs/2308.07633\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-12-23T10:51:41-08:00', 'moddate': '2025-12-23T10:51:41-08:00', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'source': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'page': 87, 'page_label': '88'}, page_content='Survey of Small Language Models in the Era of Large Language Models 145:87\\n[462] Yun Zhu, Yinxiao Liu, Felix Stahlberg, Shankar Kumar, Yu Hui Chen, Liangchen Luo, Lei Shu, Renjie Liu, Jindong\\nChen, and Lei Meng. 2023. Towards an on-device agent for text rewriting. arXiv:2308.11807. Retrieved fromhttps:\\n//arxiv.org/abs/2308.11807\\n[463] Yuanyuan Zhuang, and Jaekyeong Kim. 2021. A BERT-based multi-criteria recommender system for hotel promotion\\nmanagement. Sustainability 13, 14 (2021), 8039.DOI: https://doi.org/10.3390/su13148039\\n[464] Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. 2023. Red teaming chatGPT via jailbreaking:\\nBias, robustness, reliability and toxicity. arXiv:2301.12867. Retrieved fromhttps://arxiv.org/abs/2301.12867\\n[465] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, Zico Kolter, and Matt Fredrikson. 2023. J Universal and\\ntransferable adversarial attacks on aligned language models. arXiv:2307.15043. Retrieved fromhttps://arxiv.org/abs/\\n2307.15043\\n[466] Lixin Zou, Weixue Lu, Yiding Liu, Hengyi Cai, Xiaokai Chu, Dehong Ma, Daiting Shi, Yu Sun, Zhicong Cheng, Simiu\\nGu, et al. 2022. Pre-trained language model-based retrieval and ranking for web search.ACM Trans. Web 17, 1 (2022),\\n1–36. DOI: https://doi.org/10.1145/3568681\\n[467] Jingwei Zuo, Maksim Velikanov, Dhia Eddine Rhaiem, Ilyas Chahed, Younes Belkada, Guillaume Kunsch, and Hakim\\nHacid. 2024. Falcon Mamba: The first competitive attention-free 7B language model. arXiv:2410.05355. Retrieved\\nfrom https://arxiv.org/abs/2410.05355\\nReceived 2 January 2025; revised 18 August 2025; accepted 23 August 2025\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.')]\n"
     ]
    }
   ],
   "source": [
    "# data parsing using pypdfloader\n",
    "try:\n",
    "    pyPdfLoader = PyPDFLoader(\"data/pdf/SLMS.pdf\")\n",
    "    pypdf_docs = pyPdfLoader.load()\n",
    "    print(pypdf_docs)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70140ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 0}, page_content='.\\n.\\nLatest updates: h\\ue03cps://dl.acm.org/doi/10.1145/3768165\\n.\\n.\\nSURVEY\\nA Comprehensive Survey of Small Language Models\\nin the Era of Large Language Models: Techniques,\\nEnhancements, Applications, Collaboration with LLMs,\\nand Trustworthiness\\nFALI WANG, Pennsylvania State University, University Park, PA, United\\nStates\\n.\\nZHIWEI ZHANG, Pennsylvania State University, University Park, PA,\\nUnited States\\n.\\nXIANREN ZHANG, Pennsylvania State University, University Park, PA,\\nUnited States\\n.\\nZONGYU WU, Pennsylvania State University, University Park, PA, United\\nStates\\n.\\nTZU HAO MO, University of Pennsylvania, Philadelphia, PA, United\\nStates\\n.\\nQIUHAO LU, University of Texas Health Science Center at Houston,\\nHouston, TX, United States\\n.\\nView all\\n.\\n.\\nOpen Access Support provided by:\\n.\\nPennsylvania State University\\n.\\nRensselaer Polytechnic Institute\\n.\\nUniversity of Texas Health Science Center at Houston\\n.\\nUniversity of Pennsylvania\\n.\\nAmazon.com, Inc.\\n.\\nPDF Download\\n3768165.pdf\\n23 December 2025\\nTotal Citations: 12\\nTotal Downloads:\\n2364\\n.\\n.\\nPublished: 24 November 2025\\nOnline AM: 18 September\\n2025\\nAccepted: 23 August 2025\\nRevised: 18 August 2025\\nReceived: 02 January 2025\\n.\\n.\\nCitation in BibTeX format\\n.\\n.\\nACM Transactions on Intelligent Systems and Technology, Volume 16, Issue 6 (December 2025)\\nh\\ue03cps://doi.org/10.1145/3768165\\nEISSN: 2157-6912\\n.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 1}, page_content='A Comprehensive Survey of Small Language Models in the\\nEra of Large Language Models: Techniques, Enhancements,\\nApplications, Collaboration with LLMs, and Trustworthiness\\nFALI WANG, ZHIWEI ZHANG, XIANREN ZHANG, and ZONGYU WU, The Pennsylvania\\nState University, University Park, PA, USA\\nTZUHAO MO, University of Pennsylvania, Philadelphia, PA, USA\\nQIUHAO LU, WANJING WANG, and RUI LI, The University of Texas Health Science Center at\\nHouston, Houston, TX, USA\\nJUNJIE XU, The Pennsylvania State University, University Park, PA, USA\\nXIANFENG TANG and QI HE, Amazon.com Inc., Palo Alto, CA, USA\\nYAO MA, Rensselaer Polytechnic Institute, Troy, NY, USA\\nMING HUANG, The University of Texas Health Science Center at Houston, Houston, TX, USA\\nSUHANG WANG, The Pennsylvania State University, University Park, PA, USA\\nLarge language models (LLMs) have demonstrated emergent abilities in text generation, question answering,\\nand reasoning, facilitating various tasks and domains. Despite their proficiency in various tasks, LLMs like\\nPaLM 540B and Llama-3.1 405B face limitations due to large parameter sizes and computational demands,\\noften requiring cloud API use, which raises privacy concerns, limits real-time applications on edge devices, and\\nincreases fine-tuning costs. Additionally, LLMs often underperform in specialized domains such as healthcare\\nand law due to insufficient domain-specific knowledge, necessitating specialized models. Therefore, Small\\nLanguage Models (SLMs) are increasingly favored for their low inference latency, cost-effectiveness, efficient\\ndevelopment, and easy customization and adaptability. These models are particularly well-suited for resource-\\nlimited environments and domain knowledge acquisition, addressing LLMs’ challenges and proving ideal for\\napplications that require localized data handling for privacy, minimal inference latency for efficiency, and\\ndomain knowledge acquisition through lightweight fine-tuning. The rising demand for SLMs has spurred\\nThis material is based upon work supported by, or in part by, the Army Research Office under grant number W911NF21-1-\\n0198, the Department of Homeland Security CINA under grant number 17STCIN0000105-00, Cisco Faculty Research Award.\\nThe findings and conclusions in this article do not necessarily reflect the views of the funding agencies.\\nAuthors’ Contact Information: Fali Wang, The Pennsylvania State University, University Park, PA, USA; e-mail:\\nfqw5095@psu.edu; Zhiwei Zhang, The Pennsylvania State University, University Park, PA, USA; e-mail: zbz5349@psu.edu;\\nXianren Zhang, The Pennsylvania State University, University Park, PA, USA; e-mail: xzz5508@psu.edu; Zongyu Wu, The\\nPennsylvania State University, University Park, PA, USA; e-mail: zzw5373@psu.edu; TzuHao Mo, University of Pennsylvania,\\nPhiladelphia, PA, USA; e-mail: investdmo@gmail.com; Qiuhao Lu, e-mail: qiuhao.lu@uth.tmc.edu; Wanjing Wang, The\\nUniversity of Texas Health Science Center at Houston, Houston, TX, USA; e-mail: wanjing.wang@uth.tmc.edu; Rui Li,\\nThe University of Texas Health Science Center at Houston, Houston, TX, USA; e-mail: rui.li.1@uth.tmc.edu; Junjie Xu, The\\nPennsylvania State University, University Park, PA, USA; e-mail: jmx5097@psu.edu; Xianfeng Tang; Amazon.com Inc., Palo\\nAlto, CA, USA; e-mail: tangxianfeng@outlook.com; Qi He, Amazon.com Inc., Palo Alto, CA, USA; e-mail: qih@amazon.com;\\nYao Ma, Rensselaer Polytechnic Institute, Troy, NY, USA; e-mail: may13@rpi.edu; Ming Huang, The University of Texas\\nHealth Science Center at Houston, Houston, USA; e-mail: ming.huang@uth.tmc.edu; Suhang Wang (corresponding author),\\nThe Pennsylvania State University, University Park, PA, USA; e-mail: szw494@psu.edu.\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the\\nfull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\\n© 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM 2157-6912/2025/11-ART145\\nhttps://doi.org/10.1145/3768165\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 2}, page_content='145:2\\nF. Wang et al.\\nextensive research and development. However, a comprehensive survey investigating issues related to the\\ndefinition, acquisition, application, enhancement, and reliability of SLM remains lacking, prompting us to\\nconduct a detailed survey on these topics. The definition of SLMs varies widely; thus, to standardize, we\\npropose defining SLMs by their capability to perform specialized tasks and suitability for resource-constrained\\nsettings, setting boundaries based on the minimal size for emergent abilities and the maximum size sustainable\\nunder resource constraints. For other aspects, we provide a taxonomy of relevant models/methods and develop\\ngeneral frameworks for each category to enhance and utilize SLMs effectively. We have compiled the collected\\nSLM models and related methods on GitHub: https://github.com/FairyFali/SLMs-Survey.\\nCCS Concepts: • Computing methodologies →Natural language generation;\\nAdditional Key Words and Phrases: Small Language Models, On-Device LLMs, Domain-specific Models,\\nTrustworthiness\\nACM Reference format:\\nFali Wang, Zhiwei Zhang, Xianren Zhang, Zongyu Wu, TzuHao Mo, Qiuhao Lu, Wanjing Wang, Rui Li, Junjie\\nXu, Xianfeng Tang, Qi He, Yao Ma, Ming Huang, and Suhang Wang. 2025. A Comprehensive Survey of Small\\nLanguage Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration\\nwith LLMs, and Trustworthiness. ACM Trans. Intell. Syst. Technol. 16, 6, Article 145 (November 2025), 87 pages.\\nhttps://doi.org/10.1145/3768165\\n1\\nIntroduction\\nThe evolution of neural language models (LMs) from BERT’s [86] pre-training and fine-tuning\\nparadigm to T5’s [291] pre-training plus prompting approach and finally to GPT-3’s [38] pre-\\ntraining plus in-context learning has greatly enhanced natural language processing (NLP).\\nThese advancements have broadened NLP’s application across various fields, including language\\nunderstanding [356], programming [262, 337], recommendation systems [375], information retrieval\\n[45, 155, 232, 323], mobile-device control [90], scientific discovery [318, 441], medical question\\nanswering [35, 372], and legal question answering [12]. In particular, the recent emergence of\\nproprietary commercial models, including ChatGPT, Bard, and Claude, and open-sourced models\\nsuch as Llama [96, 344, 345] has led to rapid growth in the development of large language\\nmodels (LLMs). Even though neural networks consistently improve on various tasks with longer\\ntraining times, larger datasets, and increased model sizes—a phenomenon known as a neural scaling\\nlaw [169], these models unpredictably exhibit a sudden acquisition of versatile abilities, termed\\n“emergent ability,” once they reach a critical scale threshold, thereby supporting the “larger is\\nbetter” trend. This ability is not present in small-scale models. For instance, the latest Llama-3.1\\nmodel with 405 billion parameters performs better in dialogue, logical reasoning, and programming\\ncompared to the smaller 7B counterpart [96].\\nDespite their prowess in complex tasks, LLMs’ huge parameters and computational needs impose\\nsignificant limitations, hindering their adoption in many real-world applications. For example,\\nthe LLaMa 3.1 model with 405 billion parameters [96], trained on 16\\u2009K H100 GPUs for 54\\u2009days,\\nrequires about 202.5\\xa0GB of GPU memory using int4 precision and has large inference latency.\\nThese issues present several challenges in specific contexts: (1) LLMs are generally hosted in the\\ncloud and used via cloud-based APIs due to the large GPU memory and computational cost. Users\\nneed to upload their data to query LLMs, raising data leakage and privacy concerns, especially in\\nhigh-stake scenarios such as healthcare, finance, and e-commerce; (2) Driven by personal agents,\\non-device deployment is a critical requirement. Several factors, including cloud costs, latency, and\\nprivacy concerns, hinder the on-device processing of cloud-based LLMs, and direct deployment is\\nimpractical due to their high parameter and cache requirements, which often exceed the capabilities\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 3}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:3\\nFig. 1. Overview of small language models.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 4}, page_content='145:4\\nF. Wang et al.\\nFig. 2. Download statistics last month in Hugging face for LLMs with various model sizes, obtained on\\nOctober 7, 2024.\\nof devices such as mobile phones; (3) Their large parameter count can cause inference delays\\nfrom seconds to minutes, unsuitable for real-time applications. For instance, Llama 2 7B takes\\napproximately 84\\u2009seconds to process 100 tokens on benchmarks including HellaSwag, TruthfulQA,\\nMMLU, and Arc_C when run on a smartphone equipped with a Snapdragon 685 processor [342];\\n(4) To boost performance in specialized domains such as healthcare and law, where generic LLMs\\nunderperform, LLMs are often fine-tuned. However, this process is computationally expensive due\\nto their large size. (5) Though general-purpose LLMs are powerful, many real-world applications\\nrequire only specific abilities and domain knowledge; deploying general-purpose LLMs would be\\na waste of resources, and such LLMs often cannot match the performance of models tailored for\\nspecific tasks [54, 127, 159, 282, 375].\\nRecently, small language models (SLMs) have shown great potential in alleviating these issues\\nwhile achieving performance comparable to LLMs for domain-specific problems [1, 27, 118, 146, 227,\\n281, 339, 342, 359, 407, 444]. Owing to fewer parameters, SLMs excel in efficiency, cost, flexibility,\\nand customization. They provide significant computational savings in pre-training and inference\\nwith reduced memory and storage needs, which is vital for applications requiring efficient resource\\nuse. These small models are especially effective in resource-limited settings, performing well on\\nlow-power devices such as edge devices. Besides, SLMs improve on-device processing by enhancing\\nprivacy, security, response times, and personalization. This supports advanced personal assistants\\nand cloud-independent applications, boosting energy efficiency and reducing carbon emissions. For\\nexample, the Llama 3.2 models (1B and 3B) demonstrate that local processing enables immediate\\nexecution of prompts and responses [7]. This approach protects privacy by keeping sensitive data\\nsuch as patient health information (PHI), business data, personal messages, and calendar details\\nlocal, enhancing confidentiality. It also allows for precise control over which queries are processed\\non-device versus those requiring cloud-based models. Therefore, SLMs are gaining increasing\\nattention as alternatives to LLMs, as indicated in Figure 2, which shows that SLMs are downloaded\\nmore frequently than larger models in the Hugging Face community, and Figure 3, which illustrates\\nthe growing popularity of SLM releases over time.\\nTypically, LMs that exhibit emergent abilities are classified as LLMs. However, the categorization\\nof SLMs remains unclear. Studies vary in their contexts: some define SLMs as models with fewer\\nthan one billion parameters [227], while others consider the term “small language model” relative\\nto the larger counterparts [186, 333, 375], with no consensus on a unified definition in the current\\nlandscape of LLMs. Research suggests SLMs for mobile devices, typically possessing around 6\\xa0GB\\nof memory, consist of sub-billion parameter models [227], whereas others classify models with up\\nto 10 billion parameters as small, noting their lack of emergent abilities [107]. Given their use in\\nresource-constrained environments and for specific tasks, we propose a generalized definition:\\nGiven specific tasks and resource constraints, we define SLMs as falling within a range where the lower\\nbound is the minimum size at which the model exhibits emergent abilities for a specialized task, and\\nthe upper bound is the largest size manageable within limited resource conditions. This definition\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 5}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:5\\nFig. 3. A timeline of existing SLMs.\\nintegrates various perspectives and addresses factors related to mobile computing and capability\\nthresholds.\\nDue to the growing demand for SLMs, extensive literature has emerged on various aspects of\\nSLMs. For example, several resource-efficient techniques [403] and training methods optimized for\\nSLMs, such as Quantization-Aware Training (QAT) [226, 363, 405] and selective architectural\\ncomponent choices [227, 287, 342], aim to enhance performance in specific applications [9, 37,\\n54, 282, 299, 387]. These methods have led to the development of numerous open-source, general-\\npurpose, and domain-specific SLMs [3, 27, 35, 339, 407, 440]. Beyond their inherent capabilities,\\nSLMs can enhance LLMs by serving as modules or effective proxies [252, 304, 388, 404, 418, 454].\\nFurthermore, the complementary advantages of SLMs and LLMs can be leveraged collectively\\nto better complete tasks [82, 208, 239, 314, 360, 442]. Despite the commendable performance of\\nSLMs, it is crucial not to overlook their credibility issues, such as the risks of adversarial attacks,\\nproducing hallucinations, and privacy breaches [91, 97, 141, 179, 261, 280, 254, 357, 376, 430].\\nHowever, currently, there is no comprehensive survey thoroughly exploring these works on SLMs\\nin the era of LLMs. Therefore, this article presents the first comprehensive survey analyzing various\\naspects of SLMs in the LLM era and their future directions. The overview structure of our article is\\nshown in Figure 1. To summarize, our major contributions are:\\n—In Section 3, we examine various techniques for improving the performance of SLMs, including\\ntraining from scratch, fine-tuning, knowledge distillation (KD), quantization, and leveraging\\nLLM-enhancing technologies to optimize SLMs.\\n—In Section 4, we discuss the tasks that SLMs can enhance and the deployment strategies\\nthat enable models to fit within the resource constraints of edge devices while maintaining\\nacceptable inference speed.\\n—In Section 5, we collect SLMs with fewer than 7 billion parameters across both general-\\npurpose and domain-specific applications, reviewing common architectural choices, training\\ntechniques, and datasets, and providing a comparative summary of performance across\\ndifferent model sizes. Recent SLMs are listed.\\n—In Section 6, we explore how SLMs can address key challenges faced by LLMs, such as high\\ninference latency, labor-intensive fine-tuning, susceptibility to knowledge noise, and risks of\\ncopyright infringement.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 6}, page_content='145:6\\nF. Wang et al.\\n—In Section 7, we survey two kinds of synergies between LLMs and SLMs: one involves cloud-\\nbased LLMs and local SLMs, while the other leverages the unique advantages of both to more\\neffectively solve tasks.\\n—In Section 8, we investigate the trustworthiness issues of SLMs, including hallucination and\\nprivacy concerns, by providing a taxonomic summary of current evaluation methods.\\nConcurrently with our survey, Lu et al. [233] evaluate open-source SLMs, focusing on their\\narchitectures, datasets, algorithms, and on-device performance metrics such as inference latency\\nand memory usage. Van Nguyen et al. [351] delve into optimization strategies for SLMs, including\\nmodel compression, pruning, and quantization. Chen and Varoquaux [51] investigate how SLMs\\nenhance LLMs and vice versa. In contrast, our survey offers a more comprehensive review with the\\nfollowing differences: (1) we present a detailed taxonomy of recent advancements in SLMs in the\\nera of LLMs; (2) we define SLMs based on emergent capabilities and device specifications, which\\nrefines previous unclear definitions related to LLMs; (3) we discuss SLM applications, especially in\\non-device tasks and deployment, topics previously unexplored; (4) we examine domain-specific\\nSLMs previously overlooked; and (5) we additionally consider the synergy between SLMs and\\xa0LLMs.\\n2\\nFoundational Concepts in Building LMs\\nThis section will introduce foundational concepts and background knowledge for LMs, including\\nthe concepts of architecture and the training process, as well as methods for obtaining SLMs from\\nLLMs. The advanced training strategy to improve SLM performance will be introduced in Section 3.\\n2.1\\nArchitecture of SLMs\\nSLMs commonly employ the Transformer architecture [352] (see Figure 4), which utilizes self-\\nattention mechanisms to manage long-range text dependencies, essential for maintaining\\nFig. 4. Transformer architecture [352].\\nperformance with constrained resources. However, due\\nto the attention mechanism, Transformers have a large\\ninference cost. Hence, to alleviate the issue, several\\nsubquadratic-time architectures such as Mamba [119],\\nHymba [93], and xLSTM [26] are proposed. Next, we\\nwill give details of Transformer due to its popularity and\\nbriefly introduce newly emerged models.\\n2.1.1\\nTransformer. The Transformer’s self-attention\\nmechanism [352] allows LMs to efficiently capture con-\\ntextual information across longer sequences, even with\\nlimited resources. The Transformer generally adopts\\nan encoder–decoder structure featuring self-attention\\nmechanisms, FFNs positional embeddings, and layer nor-\\nmalization. The Transformer architecture design tai-\\nlored for SLMs is detailed in Section 5; this section\\nwill provide only foundational concepts.\\nThe Self-Attention Mechanism enables the model to\\nevaluate the importance of tokens relative to each other.\\nThe self-attention mechanism is written as\\nAttention(Q, K, V) = softmax\\n\\x12QK⊤\\n√푑푘\\n\\x13\\nV,\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 7}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:7\\nwhere Q, K, and V are query, key, and value matrices, scaled by √푑푘for stability where 푑푘is the\\ndimension of key matrices. The dot product QK⊤reflects the similarity between the query and key\\nvectors.\\nMulti-Head Attention (MHA) [352] is the first method that uses multiple heads to capture\\ndiverse information. MHA allows the model to attend to different parts of the input sequence using\\nmultiple attention heads as\\nMultiHead(Q, K, V) = Concat(head1, head2, . . . , headℎ)W푂, with head푖\\n= Attention(QW푄\\n푖, KW퐾\\n푖, VW푉\\n푖),\\n(1)\\nEach head in the MHA mechanism operates independently, allowing the model to capture diverse\\naspects of the data. The outputs are combined using learned projection matrices W푄\\n푖, W퐾\\n푖, and W푉\\n푖,\\nconcatenated, and passed through the output projection matrix W푂.\\nBuilding on this foundation, several modifications have been introduced to further optimize self-\\nattention mechanisms for specific challenges such as memory efficiency and computational speed.\\nTo address the KV-cache bottleneck in MHA, Multi-Query Attention (MQA) [310] proposes\\nthat all attention heads share the same set of keys and values, which reduces the memory and\\ncomputational overhead associated with storing and managing multiple key-value pairs. Grouped\\nQuery Attention (GQA) [8] serves as a middle ground between MHA and MQA. It introduces\\nsubgroups of query heads (fewer than the total number of attention heads), where each subgroup\\nshares a single key and value head. Unlike MQA and GQA, which reduce the number of key\\nand value heads, Multi-Head Latent Attention (MLA) [216] compresses the keys and values\\ninto a joint latent vector. This compression allows for efficient handling of key-value pairs while\\nmaintaining high performance, significantly reducing the KV-cache and improving inference\\nefficiency. Flash Attention [76, 77] accelerates the self-attention mechanism by minimizing the\\nmemory overhead typical of standard attention calculations. This optimization allows SLMs to\\nprocess longer sequences more efficiently, enhancing their functionality under strict hardware\\nconstraints.\\nFeedforward Network (FFN) comprises two linear transformations separated by a non-linearity,\\ntypically modeled as FFN(x) = 휎(xW1 + 푏1)W2 + 푏2, where W1 and W2 are the weight matrices,\\nand 푏1 and 푏2 are bias terms. 휎is the activation function, which introduces non-linearity, allowing\\nmodels to learn complex patterns. Generally, ReLU is used as the activation function. In addition to\\nReLU, activation functions such as GeLU and SiLU are also used in SLMs to improve performance.\\nWe give the details here: (i) ReLU (Rectified Linear Unit) [5] is defined as 휎(푥) = max(0,푥), which\\nis commonly used for its simplicity and effectiveness. (ii) Gaussian Error Linear Unit (GELU)\\n[138] is defined as GELU(푥) = 푥· Φ(푥) = 푥· 1\\n2\\nh\\n1 + erf\\n\\x10\\n푥√\\n2\\n\\x11i\\n, where Φ(푥) is the standard Gaussian\\nCDF and erf is the error function. It is smoother than ReLU and widely used in models such as BERT\\n[86] and GPT [289] for better gradient flow control. Since calculating the Gaussian error function\\nfor each neuron is computationally expensive and time-consuming, there are approximations\\nusing tanh and sigmoid functions, corresponding to GELUtanh and SiLU: (iii) GELU with tanh is\\ndefined as GELUtanh(푥) = 0.5 · 푥·\\n\\x12\\n1 + tanh\\n\\x12q\\n2\\n휋· (푥+ 0.044715 · 푥3)\\n\\x13\\x13\\n. This approximation uses\\nthe Tanh function to simplify computations. (iv) Sigmoid Linear Unit (SiLU) [99] is calculated as\\nSiLU(푥) = 푥· sigmoid(푥) = 푥·\\n1\\n1+푒−푥. It effectively combines the sigmoid function with its input,\\nenhancing modeling capabilities. (v) Swish-Gated Linear Units (SwiGLU) [311] integrates the\\nSwish activation function with Gated Linear Units, defined as SwiGLU(푥) = Swish(푥·푊+푏)⊙(푥·푉+\\n푐), where푊,푉are the weight matrix and 푏,푐are the bias terms. The Swish function is expressed as\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 8}, page_content='145:8\\nF. Wang et al.\\nSwish(푥) = 푥·sigmoid(푥). This combination enhances expressiveness and computational efficiency,\\nmaking it a preferred choice in advanced models such as the Qwen series [407].\\nPositional Embeddings in Transformer models [352] are essential for capturing token order, pro-\\nviding context about relative positions within a sequence. Traditional positional embeddings in the\\nTransformer\\narchitecture\\nutilize\\na\\nsinusoidal\\nfunction,\\ndefined\\nas:\\n푃퐸(푝표푠, 2푖)\\n=\\nsin\\n\\x10\\n푝표푠\\n100002푖/푑model\\n\\x11\\n, 푃퐸(푝표푠, 2푖+ 1) = cos\\n\\x10\\n푝표푠\\n100002푖/푑model\\n\\x11\\n, where 푝표푠represents the position within the\\nsequence, 푖is the dimension index, and 푑model is the dimensionality of the model. To improve the\\nmodel’s capacity for understanding the relative positions of tokens within a sequence, Rotary\\nPositional Embedding (RoPE) [324] introduces a rotational matrix to the embeddings. RoPE\\nsignificantly enhances the positional encoding by maintaining the relative distances through ro-\\ntational transformations, thus optimizing the model’s interpretative ability regarding sequence\\ndynamics.\\nLayer Normalization [188] stabilizes the training process by normalizing layer outputs, acceler-\\nating convergence. Two types of layer normalization are commonly used [188]: (i) Non-Parametric\\nLayer Norm normalizes inputs using the mean and variance calculated across the layer’s dimen-\\nsions without learnable parameters as LN(푥) = 푥−휇\\n휎, where 휇is the mean and 휎is the standard\\ndeviation of the inputs. Its simplicity makes it ideal for SLMs. (ii) Parametric Layer Norm in-\\ncludes learnable parameters 훾and 훽for adaptive scaling and bias, enhancing model flexibility:\\nPLN(푥) = 훾\\x00 푥−휇\\n휎\\n\\x01 + 훽Additionally, RMSNorm (Root Mean Square Layer Normalization) [436] sim-\\nplifies the calculation by using the root mean square of inputs, reducing computational demands:\\nRMSNorm(푥) = 훾\\n푥\\nq\\n1\\n푁\\nÍ푁\\n푖=1 푥2\\n푖+휖+ 훽, where 푁is the number of inputs, 푥푖is the 푖th input, and 휖is a\\nsmall constant to prevent division by zero.\\nFig. 5. Mamba 1 architecture [119].\\n2.1.2\\nMamba. The attention mechanism in Trans-\\nformer suffers from a drawback: it requires recalcu-\\nlating attention scores with every previous token for\\neach new token generated during inference, lead-\\ning to quadratic time complexity. This increases the\\ninference cost as sequence lengths grow. In con-\\ntrast, Mamba [78, 119], based on state space mod-\\nels (SSMs) [167], which are a superclass of recurrent\\nneural networks, relies only on the last hidden state\\nfor generating the next token, enabling faster infer-\\nence speeds, as shown in Figure 5. To address the\\nLinear Time Invariant nature of traditional SSMs,\\nwhich hinders their ability to focus on or ignore\\nspecific inputs, Mamba improves SSMs with a dynamic selection mechanism. This mechanism\\nselectively filters out irrelevant information while retaining essential data, tailored to the content\\nof the input. Leveraging this selective SSM foundation, Mamba adeptly captures complex global\\nrelationships within sequence data. Due to its focus on the immediate previous hidden state, as\\nopposed to Transformer, which requires access to all previous hidden states, Mamba achieves\\na higher utilization rate of model parameters. This makes it more suitable for SLMs. However,\\nwe identify two drawbacks of Mamba: (i) its focus on selectively capturing global information\\nmay compromise performance on tasks that require nuanced understanding, such as detailed\\nsentiment analysis or complex entity recognition and (ii) to balance inference speed, Mamba’s\\nrecurrent structure primarily encodes static global information, which limits its effectiveness in\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 9}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:9\\nhandling multi-round tasks within a single query, such as interactive dialogue systems or iterative\\nproblem-solving scenarios.\\nIn language modeling, Mamba 1 [119] is pre-trained on the Pile dataset [109] using the training\\nrecipe from [38] and ranges from 125M to 1.3\\xa0B parameters. It outperforms comparable models\\nsuch as Pythia [32] and RWKV [277] in various tasks; for instance, Mamba-1.4B achieves a 32.8%\\naccuracy on the Arc-Challenge [68] dataset, surpassing Pythia-1.4B’s 28.5% and RWKV-1.5B’s\\n29.4%. Mamba 2 [78] develops a theoretical framework linking SSMs with attention mechanisms\\nthrough structured semi-separable matrices, enhancing the selective SSM to achieve 2–8× faster\\nspeeds while competing with Transformer models. Training and configuration for Mamba 2 align\\nwith Mamba 1. Additionally, Mamba-series models are applied widely across different fields [163,\\n287, 288, 467]. Other follow-up Mamba-based LMs, such as Falcon Mamba 7B [467] and Jamba\\n[210], also demonstrate the efficiency and scalability of the Mamba architecture for NLP tasks.\\nFalcon Mamba 7B scales Mamba’s long-sequence processing capabilities to large-scale language\\ndata, reducing memory overhead and excelling in long-form generation. Jamba further extends this\\nby blending Transformer and Mamba layers (with Mixture-of-Experts (MoEs)), achieving both\\nhigh throughput and a compact memory footprint even at a massive scale.\\nFig. 6. Hymba [93] architecture.\\n2.1.3\\nHymba. Attention heads\\nin the Transformer facilitate\\nhigh-resolution recall, while SSM\\nheads in Mamba efficiently sum-\\nmarize context. To balance per-\\nformance and efficiency for\\nSLMs, Hymba [93] integrates\\nboth attention and SSM heads\\nwithin the same layer, allowing\\nfor parallel and complementary\\nprocessing of inputs, as depicted\\nin Figure 6. This hybrid-head approach enables each layer to simultaneously leverage the high-\\nresolution recall of attention heads and the contextual summarization of SSMs, increasing the\\nmodel’s flexibility and expressiveness in managing diverse information flows and memory access\\npatterns.\\nHymba has been developed in models of varying sizes—125M, 350M, and 1.5B, trained on a\\ncombination of the DCLM-Baseline-1.0 [195], SmolLM-Corpus [28], and a proprietary high-quality\\ndataset, with token counts of 1 trillion, 250 billion, and 50 billion, respectively. The models incor-\\nporate the Warmup-Stable-Decay (WSD) learning rate scheduler [146] and the data annealing\\ntechnique [96] to ensure stable pretraining, conducted on 128 NVIDIA A100 GPUs. The 1.5B base\\nmodel was post-trained using full fine-tuning (FFT), followed by direct preference optimiza-\\ntion (DPO) [290] to develop the Hymba-1.5B-Instruct model. In commonsense reasoning tasks,\\nthe Hymba 1.5B model surpasses Llama-3.2-3B [7] by achieving 1.32% higher average accuracy,\\nrequiring an 11.67× smaller cache size, and delivering a 3.49× increase in processing speed.\\n2.1.4\\nxLSTM. Long Short-Term Memory (LSTM) [140] shares a conceptual similarity with\\nMamba that achieves success in language modeling through the introduction of time-dependent\\nweights. This similarity raises an intriguing question: how effective would LSTMs be at language\\nmodeling if scaled to billions of parameters, incorporating advanced techniques from modern LLMs\\nwhile addressing known LSTM limitations? Inspired by this question, Beck et al. [26] propose the\\nxLSTM architecture, which performs favorably compared to state-of-the-art Transformers and\\nSSMs in empirical evaluations. To address the limitations of LSTM, xLSTM designs exponential\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 10}, page_content='145:10\\nF. Wang et al.\\ngates to enhance effectiveness with long sequences, expands memory cells from scalars to matrices\\nto increase storage capacity, and removes memory mixing to enable parallel processing.\\nTo test the language modeling capabilities of xLSTM scaled to billions of parameters, it is trained\\non a large dataset comprising 300 billion tokens from SlimPajama [321] across various model sizes\\n(125M, 350M, 760M, 1.3B). The performance of pre-trained xLSTM is compared against RWKV-4\\n[277], Llama [156], and Mamba [119] across 571 text domains of the PALOMA benchmark [242] and\\nvarious downstream tasks. Across all model sizes and the majority of tasks, xLSTM consistently\\noutperforms the others, suggesting that larger xLSTM models could become formidable competitors\\nto existing LLMs that utilize Transformer technology.\\n2.2\\nTraining SLMs from Scratch\\nTraining SLMs from scratch entails several critical steps: (i) Pre-training, focused on acquiring\\ngeneral features and knowledge from the corpus; (ii) Fine-tuning, targeted at boosting the model’s\\nabilities and performance for specific tasks; (iii) Decoding strategies, which involve the methods\\nused for iteratively selecting the next token during generation.\\n2.2.1\\nPre-Training. Typically, the pre-training paradigm for LMs is divided into encoder-based\\nand decoder-based approaches. Encoder-based models, such as BERT [86], utilize Masked Lan-\\nguage Modeling (MLM) tasks where the goal is to predict masked tokens within a sentence.\\nThis is achieved by maximizing: 푃(masked token | context) = softmax(W · hmask + 푏), where\\nthe masked token is the original token that has been masked, context represents the other un-\\nmasked tokens in the sentence, W and 푏are trainable parameters of a linear output layer, hmask\\nis the output from the transformer encoder for the masked position, and softmax is the activa-\\ntion function that converts logits to probabilities over the vocabulary. This process enhances the\\nmodel’s language encoding capabilities. Decoder-based models, such as GPT [289], employ Next\\nToken Prediction (NTP) tasks, aiming to model the distribution of the next token by maximizing\\n푃(next token | context) = softmax(W · hlast + 푏), where next token is the token that the model\\naims to predict, context represents the sequence of tokens preceding the token to be predicted,\\nand hlast is the output from the transformer encoder for the last token in the context. Effective\\ndata preprocessing, crucial for optimizing the performance of SLMs trained from scratch, involves\\nmeticulous data cleaning and strategic tokenization.\\nData Cleaning involves techniques such as filtering, deduplication, and noise reduction, which\\nimprove data quality and help the model generalize better. Filtering noisy or irrelevant data,\\naddressing outliers, and handling imbalances in the dataset ensure that the training data is both\\nrepresentative and efficient. Deduplication, in particular, helps prevent overfitting by removing\\nrepeated instances, making the model more robust with efficient parameter usage.\\n2.2.2\\nFine-Tuning. After the initial training, SLMs are fine-tuned on specific tasks using task-\\nspecific data and loss functions. Parameter-efficient fine-tuning methods [120, 143, 145, 203], such\\nas Low-Rank Adaptation (LoRA), prefix-tuning, and adapter modules, are particularly effective\\nfor SLMs. LoRA [145] modifies Transformer weights by introducing trainable low-rank matrices A\\nand B for efficient fine-tuning, avoiding significant alterations to pretrained weights. The update\\nis represented as: ΔW = AB⊤The fine-tuned weight matrix used in Transformer operations then\\nbecomes: Wft = W + 훼ΔW, where 훼is a scaling factor adjusting the adaptation’s impact, allowing\\nfine-tuning on a smaller set of parameters while retaining the model’s foundational capabilities.\\nPrefix-Tuning [203] prepends learnable prefixes to the input sequence, guiding the model’s attention\\nwithout altering core model parameters. It is especially useful for generative tasks. Adapter Modules\\n[143] are small, trainable layers inserted into the pre-trained model. These layers are fine-tuned on\\ntask-specific data, allowing the base model to remain fixed while the adapters learn the necessary\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 11}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:11\\nadjustments. The typical structure of an adapter module includes a down-projection, a non-linearity,\\nand an up-projection: Adapter(h) = h + Wup · 휎(Wdown · h + bdown) + bup, where h is the input\\nhidden state, Wdown and Wup are the projection matrices, bdown and bup are the bias terms, and 휎\\nis a non-linear activation function.\\n2.2.3\\nDecoding Strategies. After pre-training or fine-tuning, employing an effective decoding\\nstrategy is crucial for generating output from language models. Decoding, the process of text\\ngeneration from SLMs, involves iteratively selecting the next word. A fundamental method is\\nthe greedy search, which predicts the most likely token at each step. This is formally modeled\\nas: 푥푖= arg max푥푃(푥| 푥<푖), where 푥푖is the token with the highest probability at the 푖th step,\\nconditioned on the preceding context 푥<푖. Other decoding strategies, such as beam search or top-k\\nsampling, are crucial for generating high-quality outputs. Beam search balances exploration and\\nexploitation by considering multiple possible sequences simultaneously, while top-k sampling\\nintroduces diversity and creativity in text generation. These strategies collectively ensure that\\nSLMs are efficient and capable of delivering high performance across various natural language\\nprocessing tasks.\\n2.3\\nObtain SLMs from LLMs\\nObtaining an SLM from an LLM is crucial for deploying in resource-constrained environments.\\nInstead of training from scratch, leveraging an LLM allows for knowledge transfer, enabling SLMs to\\nretain much of the LLM’s linguistic and domain knowledge with reduced training time and data. To\\nobtain SLMs from LLMs, three primary techniques are used: pruning, KD, and quantization. Pruning\\nremoves less critical parameters, reducing model size while aiming to maintain performance. KD\\ntransfers knowledge from a large teacher model to a smaller student model, preserving much of the\\noriginal model’s understanding. Quantization decreases parameter precision, significantly lowering\\nmemory and computation needs with minimal impact on accuracy. These methods balance size\\nreduction, efficiency, and performance retention.\\nFig. 7. Unstructured and structured prun-\\ning.\\n2.3.1\\nPruning. Pruning is a technique used to reduce a\\nmodel’s size and computational requirements (e.g., LLMs)\\nwithout significantly sacrificing its performance [128].\\nThis process involves identifying and removing less im-\\nportant or redundant parameters and components from\\nthe model. The primary goal of LLM pruning is to make\\nthe model more efficient, faster, and suitable for deploy-\\nment in resource-constrained environments. Typically,\\npruning can be categorized into two main types: unstruc-\\ntured pruning and structured pruning. An illustration of\\nunstructured pruning and structured pruning is shown\\nin Figure 7.\\nUnstructured Pruning [79, 104, 205, 307, 327, 443,\\xa0447]\\nprunes an LLM by removing weights individually without considering its internal structure. The\\nleast significant parameters are pruned according to specific criteria, e.g., magnitude or impact\\non the output). This method can achieve significant compression while maintaining performance.\\nHowever, it can also lead to irregular memory access patterns and reduced hardware efficiency\\nbecause the pruned model lacks a regular structure. SparseGPT [104] is a representative unstructured\\npruning method that can reduce large-scale GPT models like OPT-175B [445] and BLOOM-176B\\n[184] to up to 60% sparsity using a novel sparse regression solver. Wanda [327] combines weight\\nmagnitudes with input activations to efficiently identify and discard less impactful parameters.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 12}, page_content='145:12\\nF. Wang et al.\\nIt operates in a single forward pass, rapidly achieving high sparsity without retraining. It is also\\nworth noting that recent studies specifically address the compatibility issues between pruning and\\nLoRA [145], such as LoRAPrune [443].\\nStructured Pruning [14, 20, 53, 112, 126, 191, 200, 237, 248, 312, 313, 390, 415, 450], which prunes\\nan LLM by targeting entire structural components—such as neurons, channels, or layers—rather.\\nThis approach allows for a direct reduction in dimensionality, thus efficiently reducing model com-\\nplexity and memory usage. Although structured pruning may lead to higher accuracy degradation\\nthan unstructured pruning, it simplifies implementation without requiring specialized hardware.\\nShortGPT [248] proposes the Block Influence (BI) metric, which measures the significance of\\neach layer based on its transformation of hidden states. Essentially, a transformer block’s influence\\nis measured by how much it alters the hidden states. By calculating BI scores, ShortGPT determines\\nwhich layers contribute minimally to the overall performance and removes these low-importance\\nlayers. This simple yet effective layer removal strategy significantly reduces the model’s parameters\\nand computational requirements. LLM Pruner [237] offers a method to efficiently prune LLMs\\nwithout access to the original training dataset. It employs a three-step compression pipeline: Dis-\\ncovery (identifying interdependent structures), Estimation (evaluating the performance impact of\\neach group), and Recovery (post-training to address performance loss). NutePrune [200] enhances\\nstructured pruning with a Numerous-teacher method, employing variable sparsity masks and LoRA\\nmodules to guide the pruning process. This approach effectively reduces model size and complexity.\\nCOST-EFF [312] introduces a slenderized backbone—a form of structured pruning—and a multi-exit\\nmodel that employs task-specific calibration through KD. This slenderization reduces the model’s\\nspatial footprint, while the multi-exit strategy effectively balances utility and runtime costs. To\\nenhance the flexibility of structural pruning, DISP-LLM [112] breaks the structural dependencies\\nin regular methods by allowing different layers to have different subsets of features along the\\nembedding dimension.\\n2.3.2\\nKD. KD compresses a larger teacher model into a smaller student model by training the\\nstudent to mimic the teacher’s outputs [139]. This enables the student to retain much of the teacher’s\\ncapabilities with fewer parameters, making it ideal for scaling down LLMs for resource-limited\\nenvironments while maintaining performance. KD can be categorized into white-box and black-box\\napproaches [367, 408, 461] as shown in Figure 8. In White-Box KD, the student has access to the\\nteacher’s internal states or output distributions [6, 121, 160, 172, 176, 272, 386, 439]. Generalized\\nKnowledge Distillation (GKD) [176] introduces skew KL divergence to stabilize gradients and\\nenhance performance, using an adaptive off-policy approach to minimize noisy feedback and\\nimprove efficiency. Black-Box KD relies only on teacher outputs without having access to model\\ninternals [50, 185, 198, 278, 366]. Methods like Distilling Step-by-Step [144] use teacher-generated\\nrationales to train smaller models, improving performance with fewer examples. LaMini-LM [385]\\ncreates a diverse instruction dataset with GPT-3.5 Turbo responses, enabling robust performance\\nin smaller models.\\n2.3.3\\nQuantization. Quantization reduces the storage and computational demands of LLMs by\\nconverting floating-point representations into lower-precision formats, significantly cutting both\\nstorage requirements and computational complexity. Existing methods fall into two categories:\\nPost-Training Quantization (PTQ) and QAT. Figure 9 illustrates the two quantization methods.\\nPTQ, applied after training, simplifies model compression without altering the architecture or\\nrequiring retraining, though it may result in precision loss. Consider a group or block of weights w;\\nthe linear operation can be expressed as푦= wx, while the quantized version is given by푦= 푄(w)x.\\nGenerally, the quantization function 푄is defined as [212]: 푄(w) = Δ · Round \\x00 w\\nΔ\\n\\x01 , Δ = max(|w|)\\n2푁−1\\n,\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 13}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:13\\nFig. 8. Illustration of white-box and black-box KD [251].\\nFig. 9. Illustration of QAT and PTQ.\\nwhere 푁is the number of quantization bits, and Δ is the quantization scale factor determined by\\nthe absolute maximum value of w. Quantizing weights reduces model size and storage bandwidth,\\nwhile quantizing activations directly reduces memory traffic and inference latency—especially\\ncritical for hardware accelerators such as GPUs and TPUs. However, activation quantization is\\noften more sensitive due to its dynamic range variation across inputs and layers. QAT enhances\\nLLM efficiency by directly incorporating quantization into the training process, often resulting in\\nhigher accuracy than PTQ. During QAT, the forward pass utilizes quantized weights 푄(W) and\\nactivations 푄(X), while retaining full-precision values during the backward pass and for updating\\ngradients to ensure stable learning dynamics. The comparisons of the PTQ methods are summarized\\nin Table 1, detailing precision, addressed problems, and technical contributions of each method.\\nIn Table 2, we summarize recent quantization methods for language models, which demonstrate\\nthat substantial compression can be achieved with minimal performance degradation. Several\\napproaches, such as OneBit [405], BitNet [363], and BiLLM [150], achieve 8–16× parameter com-\\npression while maintaining perplexity or accuracy within 1–2% of full-precision baselines. Notably,\\nBitNet b1.58 [236] and PEQA [171] even match or surpass the performance of their full-precision\\ncounterparts. These methods are evaluated on a wide range of benchmarks, including WikiText2\\n[249], C4 [291], MMLU [136], and LM-Eval [111], ensuring a comprehensive assessment. Hybrid\\ncompression techniques are also gaining popularity; for example, JSQ [124] combines 4-bit quanti-\\nzation with 50% sparsity, achieving up to 8× compression with less than a 2% accuracy drop. In\\naddition, hardware-aware strategies are becoming increasingly important: I-LLM [147] emphasizes\\ninteger-only inference for efficient deployment, and QLoRA [84] reduces memory usage by 4× while\\nenabling low-resource fine-tuning with negligible performance loss. Overall, these results suggest\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 14}, page_content='145:14\\nF. Wang et al.\\nTable 1. Representative Quantization Methods\\nMethods\\nBit\\nType\\nTechnical Contribution\\nProblems\\nSqueezeLLM [173]\\n3-Bit\\nPTQ\\nSensitivity-based non-uniform\\nquantization, dense and sparse\\ndecomposition\\nUltra-low bit quantization\\nJSQ [124]\\nFlexible\\nPTQ\\nJoint Sparsification and Quantization\\nBetter compression-accuracy trade-offs.\\nFrameQuant [4]\\nFractional bit\\nPTQ\\nFractional bit widths\\nBetter compression-accuracy trade-offs.\\nOneBit [405]\\n1-bit\\nPTQ\\nQuantization-aware KD\\n1-Bit quantization\\nBiLLM [150]\\n1-bit\\nPTQ\\nCrucial Weights Selection, Block-based\\nerror compensation\\n1-Bit quantization\\nLQER [437]\\nFlexible\\nPTQ\\nQuantization Error Minimization\\nBetter compression-accuracy trade-offs\\nI-LLM [147]\\nFlexible\\nPTQ\\nFully Smooth Block Reconstruction,\\nDynamic Integer-only MatMul and\\nInteger-only Non-linear Operators\\nInteger-only Quantization\\nPV-Tuning [244]\\n1-Bit/2-bit\\nPTQ\\nPV algorithm\\nBetter compression-accuracy trade-offs.\\nBitNet [363]\\n1-Bit\\nQAT\\n1-Bit Transformer Architecture\\n1-Bit quantization\\nBitNet b1.58 [236]\\n−1, 0, 1\\nQAT\\nTernary Parameters\\n1-Bit quantization\\nPEQA [171]\\nFlexible\\nQAT\\nQuantization Scales Optimization\\nParameter-Efficient Fine-tuning\\nQLoRA [84]\\nNF4\\nQAT\\n4-Bit Normal Float and Double\\nQuantization\\nParameter-Efficient Fine-tuning\\nthat 3–4\\u2009bit quantization strikes an effective balance between compression, inference efficiency,\\nand accuracy, making it a practical solution for deploying LLMs in resource-constrained settings.\\n2.3.4\\nLow-Rank Techniques. Low-rank techniques compress LLMs by approximating a high-\\ndimensional weight matrix with two lower-dimensional matrices, reducing computational and\\nmemory requirements. A matrix W ∈R푚×푛is approximated as W ≈A × B, where A ∈R푚×푟and\\nB ∈R푟×푛, with 푟much smaller than 푚or 푛, reducing the number of parameters. Building on this\\nconcept, Ji et al. [161] propose a low-rank method tailored for LLMs, leveraging the observation that\\nwhile LLMs have high-rank weights, their feature interactions tend to exhibit low-rank properties.\\nThe method estimates feature distributions using pooled covariance matrices and allocates distinct\\ncompression ratios to layers based on their sensitivity to low-rank compression. A Bayesian\\noptimization strategy, using a Gaussian process as the surrogate model, optimizes the allocation\\nof low-rank dimensions, ensuring the model maintains performance while achieving significant\\ncompression. Transitioning from model compression to fine-tuning, Cho et al. [64] tackle system and\\ndata heterogeneity with the HETLORA method, which uses heterogeneous low-rank approximations\\nto accommodate the diverse capabilities of clients and data complexities. By combining local rank\\nself-pruning with sparsity-weighted aggregation, it balances high- and low-rank LoRA modules,\\nimproving convergence speed and performance compared to uniform approaches. LLM-Neo [414]\\ncombines KD with low-rank adaptation (LoRA) to improve the efficiency of transferring knowledge\\nfrom a teacher LLM to a compact student model.\\n3\\nAdvanced Enhancement Strategies for SLMs\\nWith the foundational concepts introduced in Section 2, this section explores various advanced\\ntechniques that enhance the performance of SLMs, including innovative training methods for\\ntraining SLMs from scratch, supervised fine-tuning (SFT) to align SLMs to adhere to instructions,\\nadvanced KD and quantization techniques, and techniques frequently used in LLMs, such as MoEs to\\nenhance SLMs for specific applications. A summary of enhancement techniques is also summarized\\nin Table 3.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 15}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:15\\nTable 2. Summary of Quantization Methods with Evaluation Details\\nMethod\\nEvaluated\\nBase Models\\nBenchmarks\\nFull-Precision\\nvs\\nQuantized Perfor-\\nmance\\nCompres-\\nsion\\nSpeed up\\nSqueezeLLM [173]\\nLLaMA-2\\n7/13/70B\\nWikiText2\\nPPL\\n[249]; C4 [291]\\nFP16 PPL 5.52 (C4); 3-\\nbit PPL 5.73 (C4)\\n∼5.3×\\nsmaller\\nUp to ∼2.3×\\nfaster\\nJSQ [124]\\nLLaMA-\\n7B/13B/33B;\\nLLaMA2-7B\\nPIQA [33]; BoolQ\\n[67]; MMLU [136]\\nSimilar acc @ 50%\\nsparsity\\u2009+\\u20094b quant.;\\n<2% avg. drop\\nUp to ∼8×\\n(sparsity+\\nquant.)\\n–\\nFrameQuant [4]\\nLLaMA-\\n7B/13B\\nWikiText2\\nPPL;\\nC4\\n<0.5\\u2009pt avg. drop vs\\n4\\u2009b GPTQ\\n4–8×\\nFaster than\\nGPTQ (same\\nprecision)\\nOneBit [405]\\nLLaMA-\\n7B/13B;\\nOPT-2.7B\\nWikiText2\\nPPL;\\nC4;\\nLM-Eval\\nsubset\\n≥81% of FP16 avg.;\\nsimilar in few-shot\\n16×\\n–\\nBiLLM [150]\\nLLaMA fam-\\nily\\nWikiText2\\nPPL;\\nC4;\\nLM-Eval\\nsubset\\n∼2% PPL increase\\n8–16×\\n–\\nLQER [437]\\nLLaMA\\nand\\nOPT families\\nWikiText2 PPL\\n0.3% accuracy drop\\n4× (W4) to\\n5.3× (W3)\\n–\\nI-LLM [147]\\nLLaMA fami-\\nlies\\nWikiText2\\nPPL;\\nC4\\n<1pt drop at 8\\u2009b int-\\nonly; modest at 4\\u2009b\\n2–4×\\nInteger mat-\\nmul accel.\\nBitNet [363]\\nBitNet\\n(1.3B–6.7B)\\nWikiText2\\nPPL;\\nZero/Few-shot\\nΔavg ≲2pts zero-shot\\nvs FP16\\n16×\\n1.5–3×\\nBitNet b1.58 [236]\\nBitNet\\nb1.58\\n(3B+)\\nWikiText2 PPL\\nMatches FP\\n10.1×\\n–\\nPEQA [171]\\nLLaMA fami-\\nlies\\nLM-Eval subset\\nRecovers or improves\\nvs FP <4b\\n4–8×\\nFine-tuning\\nmemory\\n↓10–20×\\nQLoRA [84]\\nLLaMA fami-\\nlies\\nMT-Bench;\\nMMLU;\\ncustom\\ndata\\n<1% drop vs FP LoRA\\n4×\\n–\\n3.1\\nInnovative Training Methods for SLMs from Scratch\\nIn scenarios with limited resources, we aim to train SLMs to provide efficient, cost-effective solutions\\ntailored for specific domains while still maintaining competitive performance with larger models.\\nTraining SLMs from scratch involves unique strategies that diverge significantly from those used for\\nLLMs. This section synthesizes cutting-edge techniques tailored to optimize the inherent capabilities\\nof SLMs, underscoring their potential to match or surpass larger counterparts in efficiency and\\neffectiveness. As shown in Figure 10, the methods for training SLMs from scratch can be categorized\\ninto three primary categories: Architecture Design, Data Construction, and Optimization Strategy.\\nNext, we introduce each category in detail.\\nArchitecture Design for SLMs. When designing SLM architectures, parameter-sharing techniques\\nare employed to minimize space usage and reduce the model’s size. As shown in the first part of\\nFigure 10, parameter sharing is achieved by two approaches: (i) a single Feed-Forward Network\\n(FFN) module is shared by every transformer layer. As shown in Figure 10(1) middle, FFN layer\\nsharing/reusing can maintain a smaller size while still benefiting from the depth and complexity\\ngained through repeated processing of input data. This technique is firstly applied in MobiLlama\\n[342] which surpasses the performance of existing SLMs of comparable size. (ii) Entire transformer\\nblocks are shared. As shown in Figure 10(1) right, Transformer Block-wise Sharing is another\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 16}, page_content='145:16\\nF. Wang et al.\\nTable 3. Advanced Enhancement Methods for SLM\\nTopic\\nMethod\\nMain Contribution\\nTraining\\nfrom\\nScratch\\nMindLLM [417]\\nBilingual models with advanced features.\\nMobiLlama [342]\\nOn-device SLM with dual objectives for efficiency and capability.\\nMobileLLM [227]\\nOptimizes LLM deployment on mobile with advanced architecture.\\nSFT\\nMobileBERT [328]\\nCompact BERT for efficient fine-tuning.\\nAlpaca 7B [335]\\nUses ChatGPT-generated tasks to tune Llama 7B.\\nRLHF [271]\\nTrains using human-preferred data and reinforcement learning.\\nDPO [290]\\nDynamically adjusts log probabilities to prevent model degradation.\\nData\\nQuality\\nin KD\\nTinyStory [98]\\nEnhances narrative coherence in child-friendly datasets.\\nAS-ES [389]\\nImproves CoT by categorizing reasoning steps.\\nSelf-Amplify [30]\\nAutomates CoT data annotation for small models.\\nDistillation\\nfor SLM\\nGKD [6]\\nAligns training and inference distributions using on-policy sequences.\\nDistiLLM [176]\\nUses skew KL divergence and adaptive off-policy for output utilization.\\nAdapt-and-Distill [419]\\nDomain adapts both teacher and student models before distillation.\\nQuantization\\nSmoothQuant [391]\\nBalances quantization difficulty using per-channel scaling.\\nBiLLM [150]\\nApplies Hessian-based metrics for binary residual approximation.\\nLLM-QAT [226]\\nUses data-free KD and logit distillation for fine-tuning.\\nPB-LLM [306]\\nBinarizes non-salient weights while preserving others in higher preci-\\nsion.\\nOneBit [405]\\nAchieves near 1-bit quantization with minimal performance loss.\\nBitNet [363]\\nIntroduces 1-bit Transformer architecture with BitLinear layers.\\nBitNet b1.58 [236]\\nImplements a ternary weight system in enhanced BitNet.\\nLLM techniques\\nfor SLM\\nMa et al. [239]\\nCombines filtering and re-ranking to improve Information Extraction\\ntasks.\\nMoQE [175]\\nApplies quantization to expert weights to outperform dense models.\\nSLM-RAG [220]\\nShows that SLMs with RAG can match LLM performance.\\nFig. 10. Innovative training methods for SLMs from scratch.\\nparameter-sharing approach that maintains depth and complexity. There are different transformer\\nblock-wise sharing strategies, such as repeating the transformer blocks all over again or repeating\\nthe immediate transformer block. This technique is applied in MobileLLMs [227], which have 125M\\nand 350M parameters. MobileLLMs demonstrate performance improvements of 2.7% and 4.3%,\\nrespectively, compared to previous models with equivalent parameters. Moreover, they exhibit\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 17}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:17\\naccuracy comparable to LLaMa-2-7B on API call tasks, highlighting the capabilities of smaller\\nmodels in mobile environments.\\nData Construction. For SLMs, the emphasis on data quality surpasses that of quantity and diversity\\n[417]. Experiments demonstrate that using a quality filtering approach to remove low-quality data\\ncan lead to improved performance in SLMs [417]. Unlike large models, which can handle diverse\\nand large datasets, SLMs benefit more from cleaner, high-quality data probably due to their limited\\ncapacity against noise. Generally, data processing has several steps: (i) Remove HTML, CSS, JS, and\\nnon-text elements for clean text; (ii) Filter low text-to-content ratio web pages; (iii) Deduplicate using\\nSimHash [80, 300]; (iv) Exclude sensitive/offensive content with heuristics and token replacements;\\n(v) Remove self-repeating phrases of advertisements to enhance dataset informativeness [48, 417].\\nThese steps collectively ensure that training data has high-quality, informative texts. SLMs also\\nsignificantly benefit from these techniques. For example, MindLLMs [417], which are bilingual\\nlightweight language models (available in 1.3B and 3B versions), adopt these data processing\\ntechniques and achieve improved capability acquisition.\\nTraining Strategy for SLMs. For LLMs, due to the large model size and data volume, LLMs are\\nusually trained with one round. For SLMs, multiple-round training can be applied [334]. Considering\\nsome examples are hard to fit, hard examples can be trained with a high probability [334]. For each\\nround of training, the data sampling probability is updated according to the overall loss of that\\nsample. Experiment results show that two rounds of training and a 50% sampling rate are a good\\ntradeoff between performance and training efficiency. Tang et al. [334] show that a deep and thin\\nneural architecture and multiple-round training can enhance the performance of the trained Pangu\\n1.5B pro model. This model outperforms the conventionally trained Pangu 1.5B and a series of\\nother comparable LLMs with similar model sizes on multiple benchmark datasets, achieving an\\naverage performance increase of 8.87%.\\n3.2\\nSFT for Enhancing SLM Performance\\nSFT employs a training methodology similar to pre-training but is specifically tailored to align\\nmodels to adhere to the instructions encapsulated within various instructional datasets. This\\napproach is designed to refine the model’s responsiveness and appropriateness to given contexts\\nas the training data dictates. For example, various models, such as Alpaca [335], UltraChat [89],\\nWizardLM [397], SlimOrca [207], ShareGPT [362], Capybara [75], Deita [221], VLAA-Thinking [49],\\nGALLM [234], and MetaMathQA [425], incorporate a suite of conversational datasets to enhance\\ntheir capabilities in context-aware dialogue and instruction adherence. Usually, as shown in Figure\\n11, existing SFT methods can be categorized into three categories:\\n(i) Classical fine-tuning with downstream data [86, 289] trains SLMs on task-specific annotated\\ndata, transferring general language representations to specific tasks such as sentiment\\nanalysis. In the LLM era, this approach remains effective, such as enhancing LLMs by\\ncalibrating responses or assigning risk scores with smaller models such as BERT [454], or\\noptimizing for mobile devices with MobileBERT [328].\\n(ii) Instruction tuning with LLM-generated data [89, 207, 335] or human-generated questions with\\nLLM annotations [362] aims to align generative models with specific instructions, enhancing\\ntheir instruction-following and reasoning capabilities. For example, Alpaca 7B [335] uses 52k\\nChatGPT-generated instruction-following examples from 175 self-instructed seed tasks to\\ntune Llama 7B [344]. Meanwhile, StableLM [27, 346] is trained on the Restruct-v1 dataset,\\nwhich includes summarization, question-answering (QA), and sentiment analysis tasks,\\nusing instruction data from [230].\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 18}, page_content='145:18\\nF. Wang et al.\\nFig. 11. Fine-tuning for enhancing SLMs.\\n(iii) Preference optimization with human feedback [271, 290, 362] aims to better align language\\nmodels with human preferences. Reinforcement Learning from Human Feedback (RLHF)\\n[271] gathers human-preferred data, trains a reward model, and fine-tunes the LM using\\nreinforcement learning. DPO [290] provides a simpler alternative to RLHF. Unlike RLHF,\\nDPO avoids explicit reward modeling and reinforcement learning techniques. Instead, it\\nadjusts the log probabilities of preferred versus non-preferred responses using a dynamic\\nweighting mechanism, preventing model degradation issues typical of methods relying on\\nprobability ratios. For instance, Llama 3.2 1B and 3B apply SFT and DPO in post-training to\\nenhance alignment with instructions and human preferences.\\n3.3\\nData Quality in KD\\nTransitioning from the discussion on training SLMs from scratch, this section delves into the critical\\nrole of data quality in KD. The motivation here is to highlight how high-quality data generated\\nfrom LLMs can significantly enhance the learning efficiency and performance of SLMs. The central\\nidea is that meticulously crafted datasets, when used in KD, enable SLMs to more effectively mimic\\nthe advanced capabilities of their larger counterparts. As shown in Figure 12, the data can come\\neither from (1) other strong LLMs (e.g., GPT-4 [2]), which are much larger and more powerful than\\nthe target SLM, or (2) the target SLM itself.\\nAugment Data from LLMs. LLM-generated data could be categorized as pre-training data and\\nfine-tuning data. Firstly, due to the limitations of model size, studies have shown that training SLMs\\nrequires simple and comprehensible data [98, 186, 190, 389]. As shown in Figure 12(1) left, TinyStory\\n[98] shows that small models (tens of millions of parameters) can generate coherent stories for\\n3-4-year-olds. GPT-3.5 or GPT-4 [2] prompts create simple stories from three keywords chosen\\nfrom a 1,500-word vocabulary, which are then used to train SLMs for similar outputs. This approach\\nshows that simple and comprehensible data can help smaller models exhibit behaviors similar to\\nthose of larger language models, such as obeying scaling laws and achieving enhanced performance.\\nOn the other hand, many efforts to enhance the Chain-of-Thought (CoT) capabilities of small\\nmodels involve using LLMs to generate high-quality fine-tuning CoT data. As shown in Figure\\n12 (1) right, these data train small models end-to-end to mimic CoT reasoning [240, 389]. AS-ES\\nLearning [389] highlights that small models struggle with complex reasoning, even when provided\\ndetailed steps, as these require nuanced extraction and abstraction. Therefore, the study introduces\\na paradigm that splits reasoning into extractive segments (context reminders) and abstractive\\nsegments (inferred insights).\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 19}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:19\\nFig. 12. Data quality in KD.\\nAugment Data from Itself. Besides distilling data from other LLMs, language models can also\\ntrain on their own outputs [30, 148, 343]. Since voting strategies can improve the performance of\\nLLMs, reasoning paths that lead to the majority answer can be further utilized to fine-tune LLMs\\n[148]. Similarly, SLMs can generate their training data with the aid of existing rationale generation\\nmethods. Self-Amplify [30] notes that human annotation of CoT data is very time-consuming;\\nthus, automated rationale generation methods have been proposed. These methods involve three\\nmain steps: (1) Selection of samples (푥,푦) that the model predicts correctly as few-shot examples;\\n(2) Rationale generation, where rationales are produced using post hoc explanation methods; (3)\\nPrompt design for SLMs, where the final prompt is crafted based on the previously generated\\nrationales.\\n3.4\\nDistillation Techniques for Enhancing SLM Performance\\nFollowing the discussion on data quality in KD, this section reviews specialized KD training\\nstrategies designed to enhance the performance of SLMs. The motivation is to address the unique\\nchallenges and constraints involved in distilling knowledge from LLMs to SLMs, ensuring that\\nthe smaller models can maximize their performance gains. As shown in Figure 13, two main gaps\\nbetween LLMs and SLMs lead to challenges in distillation: distribution mismatch and domain gap.\\nDistribution mismatch [6, 176] occurs when the distribution of output sequences during training\\ndoes not align with the distribution of sequences that SLMs produce during inference, leading\\nto suboptimal performance of the student model. The domain gap [419] arises when there is a\\ndiscrepancy between the domains or tasks on which the LLMs and SLMs are trained and applied.\\nThis gap can cause significant degradation in the performance of the student model if not properly\\naddressed during the distillation process. To address these issues, specialized strategies involve first\\naligning the teacher and student models with the target domain before proceeding with knowledge\\ndistillation. To explore these challenges further, we now delve into the details of these two branches\\nof methods.\\nDistribution Mismatch. In original KD illustrated in Figure 13 Distribution Mismatch (a), the\\nteacher and student are provided with the same input sequences 푥and output labels 푦, producing\\nprobability distributions for the next token (푞and 푝). The loss is calculated as the difference between\\nthese two distributions, 퐷(푞, 푝). However, a key challenge arises due to distribution mismatch: the\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 20}, page_content='145:20\\nF. Wang et al.\\nFig. 13. Distillation techniques for enhancing SLM performance. On-policy means learning only uses data\\nfrom the current student (policy), while off-policy permits the use of previously gathered data.\\noutput sequences during training (푦) differ in distribution from those the SLMs produce during\\ninference (푦′). To address this challenge, various techniques have been proposed. As shown in\\nFigure 13 Distribution Mismatch (b), one approach trains the student model using on-policy\\nsequences—sequences generated by the student itself—guided by the teacher model’s feedback.\\nSpecifically, both the student and teacher take the same input (푥) and the student-generated output\\n(푦′), producing probability distributions for the next token (푞and 푝, respectively). The loss is\\ncalculated as the difference between these two distributions, 퐷(푞, 푝). This approach helps the\\nstudent model reduce the distribution gap between training and inference by learning from the\\nteacher’s feedback on its own generated sequences. GKD [6] is the first work using this technique\\nand improves distillation outcomes. However, a drawback of this technique is that it requires the\\nstudent to constantly produce new training sequences, which can be computationally expensive.\\nTo improve efficiency, as shown in Figure 13 Distribution Mismatch (c), an adaptive off-policy\\napproach can be used to efficiently manage student-generated outputs by storing them in a replay\\nbuffer, thereby reducing computational costs. DistiLLM [176] employs this off-policy approach\\nand improves the efficiency of KD. While it has approximately 1.5× the training time of vanilla\\ndistillation methods, it achieves up to a 2.3× speedup compared to on-policy approaches like GKD.\\nIn addition to improved efficiency, DistiLLM boosts ROUGE-L scores of distilled SLMs across five\\ninstruction-following benchmarks, outperforming both GKD and vanilla distillation.\\nDomain Gap. When training an SLM in a specific domain that differs from the domain of the\\nLLMs, the gap between the two domains becomes problematic. As illustrated in Figure 13 Domain\\nGap (a), domain adaptation fine-tunes a language model, initially trained on a general corpus, using\\na specialized dataset such as PubMed to enhance performance in that specific domain. As illustrated\\nin Figure 13 Domain Gap (b), KD transfers knowledge from the larger model to the smaller one.\\nHowever, because the teacher model may not produce high-quality outputs on specialized datasets,\\ndomain adaptation is needed before KD. As illustrated in Figure 13 Domain Gap (c), Adapt-and-\\nDistill [419] tackles the domain gap by distilling general large models into smaller ones. This article\\nintroduces AdaLM and demonstrates that the “Adapt-and-Distill” strategy—first involving domain\\nadaptation of both the large teacher model and the small student model, followed by distillation—is\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 21}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:21\\nthe most effective compared to three other strategies: training directly from scratch, distillation\\nfollowed by adaptation, and adapting the teacher model before distillation into a general small\\nstudent model. These innovative techniques are crucial for enhancing the capabilities of SLMs,\\nmaking them more efficient and effective for various applications. However, adapting both the\\nteacher (LLMs) and the student (SLMs) models to the target domain can be time-consuming. Future\\nresearch could focus on efficiently solving the domain gap problem. In experimental evaluations,\\nAdapt-and-Distill produces a 6-layer student model that is 3.3× smaller and 5.1× faster than the\\nBERT base model, while consistently outperforming BERT on both biomedical and computer-science\\ndownstream tasks.\\n3.5\\nPerformance Improvement through Quantization\\nAs mentioned in Section 2, quantization is one of the most effective methods for adapting LLMs\\nto SLMs. However, compression to smaller sizes often compromises performance. To address the\\nperformance drop associated with quantization, various methods have been proposed. This section\\nexamines how these quantization methods specifically enhance the performance of SLMs. While\\nthe general introduction to compression methods is discussed in the compression section, the\\nfocus here is on detailing those approaches that boost the efficiency and effectiveness of SLMs.\\nAs shown in Figure 9, we categorize these quantization methods into two main approaches: PTQ,\\nwhere quantization is conducted on a well-trained fixed model, and QAT, where quantization is\\nintegrated into the training process. This section introduces advanced techniques in PTQ and QAT,\\nrespectively.\\nPTQ primarily includes weight quantization and activation quantization. Weight quantization\\naims to quantize model parameters while preserving performance. GPTQ [105] compresses LLMs\\nto 4-bit or 2-bit by quantizing weights layer-by-layer to minimize layer-wise quantization errors.\\nGPTQ quantizes OPT-175B and BLOOM-176B in approximately four GPU hours with a negligible\\nincrease in perplexity, and the resulting 3-bit OPT-175B model achieves up to a 3.25× speedup\\non a single NVIDIA A100 GPU and 4.5× on two NVIDIA A6000 GPUs, while fitting the model\\ninto a single 80GB A100. PB-LLM [306], applicable to both PTQ and QAT, retains the most salient\\nweights while binarizing the rest based on magnitudes. BiLLM [150], another PTQ method, uses a\\nHessian-based metric to identify salient and non-salient weights. Salient weights undergo binary\\nresidual approximation to minimize loss, while non-salient weights are divided into sparse and\\nconcentrated groups for separate binarization, reducing quantization errors. Activation quantiza-\\ntion faces challenges with outliers that can stretch the quantization range, causing most values\\nto cluster at a few bits and introducing significant errors. To address this, LLM.int8() [83] isolates\\noutlier features for 16-bit processing and handles the rest in 8-bit. SmoothQuant [391] circumvents\\nper-channel quantization issues by employing a “smoothing” technique that shifts the quantization\\nchallenge from activations to weights through a per-channel scaling transformation. This balance\\nbetween activating and weight quantization allows effective 8-bit quantization (W8A8), preserving\\naccuracy while significantly reducing memory and computational costs. SmoothQuant thus en-\\nhances the efficiency of SLMs in resource-constrained environments. SmoothQuant achieves up to\\n1.56× inference speedup and 2× memory reduction on LLMs with negligible accuracy loss.\\nQAT differs from PTQ in that it includes a training phase after the model has been quantized.\\nWhen models are quantized to extremes, such as 2-bit or 1-bit, performance typically drops sig-\\nnificantly, but further training can help the model retain its capabilities. For instance, to mitigate\\nperformance degradation from binarization, PB-LLM [306] selectively binarizes only non-salient\\nweights, preserving the most salient ones at higher precision. This method effectively reduces the\\nmodel size without significantly impacting performance. Salient weights are chosen based on their\\nmagnitude, ensuring that the most influential weights maintain higher precision to preserve the\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 22}, page_content='145:22\\nF. Wang et al.\\nmodel’s reasoning capabilities. The article explores both PTQ and QAT to fine-tune and recover the\\nperformance of partially binarized models, achieving a balance between compression and accuracy.\\nOneBit [405] and BitNet [363] address the severe performance degradation associated with 1-bit\\nquantization by decomposing floating-point matrices and employing mixed-precision strategies.\\nSpecifically, OneBit introduces Sign-Value-Independent Decomposition (SVID), which decomposes\\na floating-point matrix into a 1-bit matrix and two floating-point vectors. This method allows\\nLLMs to be quantized to a 1-bit level while minimizing performance loss. By retaining critical\\ninformation with the floating-point vectors, OneBit effectively balances extreme compression with\\nmaintaining model accuracy. BitNet b1.58 [236] improves on the original BitNet by introducing a\\nternary matrix weight system of −1, 0, 1, resulting in a 1.58-bit model. BitNet b1.58 matches the\\nperformance of full-precision models starting from a 3 billion parameter size while further reducing\\nmemory and latency costs. LLM-QAT [226] employs data-free KD where the pre-trained model\\nitself generates data for fine-tuning the quantized model (student) using logit distillation from the\\nfull-precision model (teacher). This method incorporates quantization of weights, activations, and\\nkey-value cache, achieving accurate 4-bit quantization for weights and key-value caches and 6-bit\\nfor activations, demonstrating substantial improvements over existing PTQ methods.\\n3.6\\nTechniques in LLMs Contributing to SLMs\\nThis subsection explores the potential of advanced techniques such as RAG and MoE, which enhance\\nLLM performance, to also maintain or boost SLM performance within constrained computational\\nbudgets. However, effectively integrating these techniques into SLMs, which inherently possess\\nlimited capabilities, remains an unresolved challenge.\\nRetrieval Augmented Generation (RAG) enhances the capabilities of language models in\\nknowledge-intensive tasks by incorporating a retrieval mechanism. This approach allows models\\nto access relevant contextual information from a data repository in response to user queries. By\\nintegrating this retrieved data, RAG-equipped models better understand specific topics, enabling\\nmore informed and accurate outputs. For SLMs, a significant concern is whether they possess the\\ncapacity for long-context reasoning. A recent study [220] compares SLMs at the 7B level with\\nRAG to larger models such as GPT-3.5 and GPT-4, suggesting that SLMs equipped with RAG can\\nsometimes perform comparably or even better than LLMs. These findings indicate that RAG for\\nSLMs is effective and represents a promising direction for future research.\\nMoE [40] has emerged as an effective method for substantially scaling up model capacity with\\nminimal computation overhead in LLMs. The MoE framework is founded on a straightforward\\nyet potent concept: distinct components of a model, referred to as “experts,” specialize in different\\ntasks or data facets. In this paradigm, only the relevant experts are activated for a specific input,\\nwhich manages computational costs while leveraging a vast pool of specialized knowledge. This\\nscalable and adaptable approach enables increased model capacity without proportionally escalating\\ncomputational demands. We argue that MoE is particularly suitable for SLM architectures [164]\\nas it minimizes both computational load and memory overhead. However, research on MoE for\\nSLMs remains sparse. Future studies could investigate how large LLM MoE architectures can be\\neffectively compressed into small ones or how to develop an SLM with MoE tailored for specific\\ndevices from scratch.\\n4\\nApplications of SLMs\\nIn this section, we delve into the applications of SLMs across various NLP tasks and their deploy-\\nment strategies. Due to benefits such as enhanced privacy, faster inference, and lower memory\\nrequirements, many NLP applications are now leveraging SLMs over LLMs. Additionally, deploy-\\ning SLMs often involves considerations of memory and runtime efficiency, which are crucial for\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 23}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:23\\nTable 4. Task-Specific SLM Applications\\nAspect\\nRepresentative Work\\nKey Point\\nSLM in QA\\nAlpaca [335]\\nTune Llama 7B [344] using 52k ChatGPT-generated examples.\\nStable Beluga 7B [243]\\nEmploy explanation tuning to Llama-2 7B [345] on an Orca-style dataset.\\nFine-tuned BioGPT [127]\\nFine-tuning BioGPT (1.6B) [235] on PubMedQA.\\nFinancial SLMs [282]\\nTransfer financial knowledge from GPT-4 [2] to multiple SLMs.\\nColBERT [114]\\nFetch retrieval documents for SLMs to answer domain-specific questions.\\nRationale Ranking [130]\\nFor unseen questions, combine retrieval with LLM-generated rationales.\\nT-SAS [159]\\nEnhance SLMs adaptability with self-generated pseudo labels.\\nSLM in\\nCoding\\nPhi-3.5-mini [1]\\nNew addition to the Phi-3 series and focus on high-quality data.\\nTinyLlama [444]\\n1.1B Transformer model is trained on 3\\u2009T corpus.\\nCodeLlama [299]\\nA derivative of Llama 2 fine-tuned on domain-specific datasets.\\nCodeGemma [337]\\nFine-tuning Gemma to enhance coding capabilities.\\nSLM in\\nRecommendation\\nPromptRec [387]\\nTraining on prompt templates.\\nSLIM [375]\\nStep-by-step KD\\nBiLLP [316]\\nLLaMa-2-7B as planner and reflector.\\nONCE [219]\\nLLaMa-2-7B as Content Encoder.\\nRecLoRA [459]\\nPersonalized low-rank adaptation.\\nSLM in\\nWeb Search\\nContent encoder [45, 155, 232]\\nEncode concatenated queries and documents.\\nRanker [65, 267]\\nRe-rank retrieved documents using a special SLM.\\nRewriter [238]\\nBridge the gap between queries and needed knowledge by rewriting inputs.\\nSLM in\\nMobile-\\ndevice\\nOctopus [54]\\nCalling software APIs via learning in documents.\\nMobileAgent [90]\\nStandard Operating Procedure (SOP).\\n훼-UMI [314]\\nSLMs serve as Multi-agents in tool uses.\\nMobile Interaction [43]\\nText-to-action control and tests on 6\\xa0GB and 4\\xa0GB Android devices.\\nAutoDroid [381]\\nInteraction based on GUI and APP knowledge injection.\\nM4 [428]\\nA foundation model handling all mobile AI tasks.\\nAgent for Text Rewriting [462]\\nData KD from LLMs.\\noptimizing resource use on budget-constrained edge devices, particularly mobile phones. Then, we\\nwill discuss task-specific applications of SLMs and their deployment methods on mobile and edge\\ndevices.\\n4.1\\nTask-Specific SLM Applications\\nThis subsection explores the diverse NLP tasks to which SLMs can contribute. QA and coding\\nrepresent generative tasks, while recommender systems and web search (though not strictly within\\nthe NLP domain) typically leverage the encoding capabilities of SLMs. Additionally, the application\\nof SLMs on mobile devices is particularly well-suited due to constraints in memory and computing\\nresources. The representative works are systematically organized in Table 4.\\n4.1.1\\nSLM Applications in QA. QA is a fundamental task in the NLP field, demanding language\\nmodels to exhibit abilities in understanding language, reasoning, common sense, and recalling\\nspecialized knowledge. Typically, larger language models yield better QA performance. However, the\\nsubstantial size of these models introduces challenges such as immense computational requirements,\\nprivacy concerns when using proprietary LLMs, and difficulties in customization. These issues\\nlead researchers and developers to favor SLMs in scenarios that demand efficiency, privacy, and\\ncustomization. Therefore, we explore methods to enhance the capabilities of SLMs in QA across three\\nkey areas: (i) Instruction Tuning of Generic SLMs for QA, (ii) Instruction Tuning of Domain-Specific\\nSLMs for QA, and (iii) Enhancing SLMs for Out-of-Domain Questions.\\nInstruction Tuning Generic SLMs for QA. Despite the Phi series’ high QA capability, its training\\ncost with over 3.4\\u2009T tokens on 512 H100 GPUs for 10\\u2009days [1] is prohibitive for many researchers\\nand developers. Instruction tuning [377] offers a cost-effective alternative, enhancing small models\\nby fine-tuning on large model outputs. Alpaca 7B [335] tunes Llama 7B [344] with 52k ChatGPT-\\ngenerated examples from 175 seed tasks. This behavior cloning mimics teacher models effectively\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 24}, page_content='145:24\\nF. Wang et al.\\nTable 5. Comparison of Instruction-Tuned Domain SLMs for QA and LLMs on FinQA [58] and\\nPubMedQA [166]\\nModel\\nSize\\nInstruction Tuned?\\nTask Name\\nShot Type\\nAccuracy (%)\\nGPT-4 [2]\\n-\\n×\\nFinQA\\nZero-shot\\n77.5\\nPhi-3-Mini [1]\\n2.7B\\n✓\\nFinQA\\nZero-shot\\n77.6\\nMeditron-70B [57]\\n70B\\n×\\nPubMedQA\\nZero-shot\\n81.6\\nRankRAG-llama3-70B [426]\\n70B\\n×\\nPubMedQA\\nZero-shot\\n79.8\\nFlan-PaLM [320]\\n540B\\n×\\nPubMedQA\\nFew-shot\\n79.0\\nGAL 120B [336]\\n120B\\n×\\nPubMedQA\\nZero-shot\\n77.6\\nFlan-PaLM [320]\\n62B\\n×\\nPubMedQA\\nFew-shot\\n77.2\\nBioGPT [235]\\n345M\\n✓\\nPubMedQA\\nZero-shot\\n78.2\\nBioGPT-Large [235]\\n1.5B\\n✓\\nPubMedQA\\nZero-shot\\n81.0\\nbut struggles in reasoning-intensive QA tasks where accuracy is key, not style [62]. To counter it,\\nexplanation tuning [243] enhances Llama-2 7B [345] using explanatory LLM answers to improve\\nreasoning. However, its effectiveness varies with system instructions, and those effective for larger\\nmodels like GPT-4 may not suit smaller ones. SLMs also struggle to identify optimal system instruc-\\ntions for different tasks. Therefore, Orca 2 [253] addresses this by promoting cautious reasoning,\\ndeciding which solution strategy to choose for a given task among direct answer generation, or\\n“Slow Thinking” strategies (step-by-step, guess and check or explain-then-answer, etc.) and erasing\\nspecific system instructions during training. This involves (1) a solution strategy guided by the\\nperformance of Orca 1 [258], (2) writing task-specific system instructions corresponding to the\\nchosen strategy to obtain teacher responses for each task, and (3) at training time, employing\\nPrompt Erasing to replace the student’s system instructions with generic ones vacated of details of\\nhow to approach the task, encouraging students to learn not just task solutions but also deeper\\nreasoning abilities.\\nInstruction Tuning Domain SLMs for QA. Beyond instruction tuning for generic SLMs, tuning\\ndomain-specific SLMs is also crucial, as they provide specialized assistance where generic SLMs\\nmay underperform. Instruction-tuning generic SLMs can derive domain SLMs. We summarize some\\nrepresentatives in several domains. (1) In finance, Phogat et al. [282] transfer financial QA abilities\\nfrom teacher LLMs such as GPT-4 [2] to specialized SLMs such as Phi-3-Mini [1], using datasets such\\nas FinQA [58], ConvFinQA [59], and TATQA [458]. They train SLMs with Python programs created\\nby the teacher model, which detail steps for financial reasoning, including concept comprehension,\\nformula identification, entity extraction, and calculations. During inference, SLMs generate Python\\ncode that an external interpreter executes. (2) In the medical field, Guo et al. [127] enhance student\\nSLMs, including domain-specific BioGPT (1.6B) [235] and general Llama 7B [344], by fine-tuning\\non enriched PubMedQA [166] data. This enhancement is achieved by generating new samples or\\nrewriting existing ones using teacher LLMs, which include the highly knowledgeable GPT-4 and\\nthe relatively weaker ChatGPT. The best SLM, with under 1.6 billion parameters, achieves 75.4%\\naccuracy, surpassing GPT-4’s 74.4% in few-shot settings on the PubmedQA test sets. It demonstrates\\nthat LLMs effectively refine and diversify question-answer pairs, leading to enhanced performance\\nin a significantly smaller model after fine-tuning. We report the detailed results of comparisons of\\ninstruction-tuned domain-specific language models for QA and larger language models on FinQA\\n[58] and PubMedQA [166], as shown in Table 5.\\nEnhancing SLMs for Out-of-Domain Questions. One of the major advantages of LLMs is their strong\\ncomprehension and logical reasoning abilities, which SLMs often struggle to match due to their\\nlimited parameters, especially when handling unseen or out-of-domain questions. Various methods\\nhave been developed to address this limitation, including RAG and self-adaptive techniques.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 25}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:25\\n(1) RAG: Incorporating External Knowledge for Domain-Specific QA. RAG addresses OOD questions\\nby integrating external knowledge during inference, allowing models to access information\\nbeyond their pre-trained parameters. By retrieving relevant documents in real time, RAG\\nenables SLMs to provide accurate answers on specialized topics. In the telecommunications\\ndomain, Gichamba et al. [114] use ColBERT as a dense retrieval system to fetch documents\\nfrom technical datasets. By encoding queries and documents separately, ColBERT computes\\nrelevance scores, helping small models like Phi-2 and Falcon-7B retrieve precise technical\\ninformation to answer complex telecom-related queries. Rationale Ranking [130] addresses\\nanswering unseen questions using smaller language models by integrating external explana-\\ntory contexts from retrieval systems with reasoning rationales from LLMs. This method\\ninvolves ranking both the retrieved explanatory contexts and LLM-generated rationales using\\na scoring module, which then combines them to form a cohesive context. Consequently, this\\nintegrated approach enhances the SLMs’ performance on unseen questions.\\n(2) Self-Adaptive Techniques: Enhancing Model Adaptability with Self-Generated Pseudo Labels.\\nFine-tuning, while effective in adapting domain knowledge, can be impractical in realistic\\nscenarios where labeled datasets are scarce. To overcome this, self-adaptive techniques\\nemploy self-generated pseudo labels to activate specific aspects of the target tasks, thereby\\nenhancing model adaptability [319, 353]. Test-time Self-Adaptive Small LMs (T-SAS) [159]\\nfirst stochastically generates multiple answers for an unlabeled question. The most plausible\\nanswer is then selected via majority voting to enhance pseudo-label accuracy, serving as a\\npseudo-label for training during test time.\\nComparison between LLMs and SLMs for QA. When comparing LLMs such as GPT-4 [2] or BLOOM-\\n175B [184] with fine-tuned SLMs in QA tasks, the benefits of SLMs are clear. LLMs, while versatile\\nacross multiple domains due to extensive pre-training, are computationally demanding, making\\nthem less ideal for resource-limited settings. SLMs, however, when fine-tuned for specific domains,\\noften match or exceed the performance of larger models within those specialties. The tradeoff is\\nbetween large-scale models’ generalization and small-scale models’ specialization: LLMs handle\\ndiverse domains but may need additional techniques such as knowledge injection for domain-\\nspecific queries. In contrast, domain-specific SLMs, though less flexible, provide higher accuracy and\\nmore relevant responses, making them ideal for edge deployments where computational resources\\nare scarce but domain precision is crucial.\\n4.1.2\\nSLM Applications in Coding. The adoption of SLMs for coding offers an alternative to\\nLLMs due to their lower computational needs and potential for domain-specific tuning. Despite\\nLLMs’ proficiency in code generation and programming support, SLMs are advantageous for\\ntheir faster inference, reduced operational costs, and suitability for real-time environments where\\nrapid responses are crucial. Representative works are discussed next. The Phi series [1, 158, 204]\\nshowcases SLMs’ evolution in coding tasks. For instance, Phi-1 [122], a Transformer with 1.3B\\nparameters, specializes in basic Python coding and achieves notable scores in benchmarks such as\\nHumanEval [122], which includes 164 programming problems. Subsequent models, Phi-1.5 and\\nPhi-2, have enhanced these capabilities, while Phi-3 demonstrated SLMs’ potential to rival larger\\nmodels [1]. The latest model, Phi-3.5-mini, with 3.8B parameters, excels in long-context tasks using\\nadvanced fine-tuning and optimization techniques, performing comparably to larger models such\\nas Llama-3.1-8B-instruct [96] and surpassing smaller ones like Gemma-2 [339].\\nAnother avenue of development is the fine-tuning of general-purpose SLMs for coding tasks [24,\\n123, 231, 299, 337]. For instance, CodeLlama models [299], derivatives of Llama 2 [345], undergo a\\nrigorous fine-tuning process on domain-specific datasets, enhancing their proficiency in specific\\nprogramming languages such as Python. They are trained to handle tasks such as syntax error\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 26}, page_content='145:26\\nF. Wang et al.\\ndetection, code suggestion, and infilling, where they learn to predict and complete missing parts\\nof the code. This specialized fine-tuning improves their ability to interpret and execute detailed\\nprogramming instructions, making them highly effective in real-time code editing environments\\n[299]. CodeGemma models [337], stemming from Google DeepMind’s Gemma framework, also\\nexhibit a focused approach to enhancing coding capabilities through fine-tuning. These models\\nare specifically engineered for high-performance code generation and infilling, underpinned by\\nextensive training on a vast corpus of over 500 billion to 1 trillion tokens, predominantly consisting\\nof code. This comprehensive dataset enables CodeGemma models to excel in mathematical reasoning\\nand complex problem-solving within code contexts, setting new benchmarks in latency-sensitive\\napplications such as real-time IDE support and automated code reviews [337].\\nTable 6. Performance Comparison Between\\nSLMs and LLMs in Coding Benchmarks\\nModel\\nSize\\nHumanEval\\nMBPP\\nDeepSeek-Coder [123]\\n1.3B\\n65.2\\n49.4\\nCodeGemma [337]\\n2B\\n37.8\\n49.2\\nGemma 2 [339]\\n2B\\n17.7\\n40.2\\nPhi-3.5-mini [348]\\n3.8B\\n62.8\\n69.6\\nDeepSeek-Coder [123]\\n6.7B\\n78.6\\n65.4\\nCodeGemma [337]\\n7B\\n60.4\\n55.2\\nLlama 3.1 [96]\\n8B\\n66.5\\n69.4\\nGemma 2 [339]\\n9B\\n61.0\\n69.3\\nGPT-3.5 Turbo\\n-\\n68.0\\n71.2\\nDeepSeek-Coder [123]\\n33B\\n79.3\\n70.0\\nLlama 3.1 [96]\\n70B\\n80.5\\n75.4\\nLlama 3.1 [96]\\n405B\\n89.0\\n78.8\\nGPT-4o OpenAI [270]\\n-\\n90.2\\n81.4\\nClaude 3.5 Sonnet [16]\\n-\\n92.0\\n76.6\\nAll models listed are chat or instruct versions, and per-\\nformance are sourced from respective research papers\\nor technical reports [96, 123, 299, 337, 348].\\nComparison between SLMs and LLMs on Cod-\\ning. Table 6 provides a comparative analy-\\nsis of SLMs and LLMs on coding benchmarks\\nHumanEval [52] and MBPP [21]. Insights in-\\nclude: (i) Small SLMs (1.3B–3.8B Parameters)\\nlike Phi-3.5-mini [348] achieve high scores,\\ndemonstrating the efficacy of small models.\\nMid-sized SLMs (6.7B–9B Parameters), such as\\nDeepSeek-Coder 6.7B [123] and Llama 3.1 8B\\n[96], show improved performance, indicating\\nthat larger model sizes and enhanced train-\\ning contribute to better accuracy. Large models\\n(33B and above) like Llama 3.1 405B [96], GPT-\\n4o [270], and Claude 3.5 Sonnet [16] excel, sup-\\nporting the idea that bigger models generalize\\nbetter across diverse coding tasks; (ii) There’s\\na notable tradeoff between computational ef-\\nficiency and performance, with larger models\\nrequiring more resources, impacting their prac-\\ntical deployment in constrained environments; and (iii) Specialized training and fine-tuning, as\\nused in models like DeepSeek-Coder [123], are crucial for excelling in coding tasks, though such\\nmodels may not handle complex requests as effectively, highlighting the versatility of general SLMs\\nfor broader applications.\\n4.1.3\\nSLM Applications in Recommender Systems. Recommender systems are essential in various\\nonline services, helping to manage information overload and meet users’ personal needs. SLMs\\nenhance recommendation systems by (1) addressing the cold start problem; (2) reducing popularity\\nbias; (3) improving long-term planning; (4) serving as personalized recommenders; and (5) acting\\nas content encoders. These applications show the versatility and effectiveness of SLMs in boosting\\nperformance and personalization in recommendations. Next, we introduce the details.\\nSLM for System Cold Start Problem. Traditional recommendation systems, which utilize historical\\nuser–item interactions such as clicks, purchases, and ratings to learn representations and match\\nitems to users, fail in scenarios lacking any user–item interactions, known as the cold-start recom-\\nmendation problem, often occurring in start-up businesses [296]. Although LLMs address this with\\nin-context learning, their slow and costly inference restricts real-time use. Thus, PromptRec [387]\\nexplores using SLMs as in-context recommenders for recommendation system cold-start problems.\\nHowever, SLMs often struggle without emergent context-learning abilities. To overcome this, SLMs\\nare enhanced by pre-training on relevant corpora, using an improved C4 corpus subset [291], and\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 27}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:27\\nby developing training prompts for different domains, enhancing cold-start performance. Results\\nshow that enhanced SLMs like BERT-mini [86], with 11.3M parameters, achieve BERT-large’s\\nperformance in cold-start scenarios, with only 17% of BERT-large’s inference time. Similarly, many\\nstudies have addressed the cold-start problem by leveraging BERT [135, 268, 446, 463]. For example,\\nADLRS [135] employs BERT to convert web-crawled item profiles into vectors that highlight key\\naspects, aiding recommender systems in acquiring essential initial information.\\nSLM for Mitigating Popularity Bias. Popularity bias in recommender systems, marked by dis-\\ncrepancies between item popularity in training datasets and the real world, often stems from\\nusing closed-loop datasets with limited information. Recent LLMs leverage their broad open-world\\nknowledge to better reason about user–item interactions [211, 219], reducing this bias by providing\\nrecommenders with more extensive item details. Using the CoT prompting, LLMs decompose\\ncomplex tasks into intermediate reasoning steps, enhancing understanding of user behavior and\\ninterests. However, LLMs’ high resource demands limit their practical use. To overcome this, the\\nStep-by-step Knowledge Distillation Framework for Recommendation (SLIM) [375] distills LLM\\nreasoning capabilities into SLMs, keeping just 4% of the original parameters, transitioning from\\nChatGPT to Llama 7B [344]. SLIM uses detailed LLM templates to extract reasoning steps and\\nstreamlined templates for fine-tuning, enabling SLMs to improve recommender systems by better\\nreasoning on richer item information.\\nSLM for Long-term Planning. Traditional recommender systems focus on optimizing immediate\\nuser responses, often maximizing short-term gains but overlooking long-term engagement. This\\ncan trap users in echo chambers and filter bubbles [108, 374]. To tackle this, integrating planning\\ncapabilities into recommendations to balance immediate and long-term outcomes is vital. LMs, with\\ntheir extensive knowledge and reasoning abilities, are expected to enhance planning capabilities.\\nBiLLP [316] adopts a hierarchical learning approach with macro- and micro-learning phases. In\\nmacro-learning, a Planner and a Reflector, both as SLM instances like Llama-2-7B [345], operate;\\nthe Planner forms long-term plans using high-level experiences, while the Reflector updates plans\\nbased on past actions. Micro-learning uses an SLM-based Actor-Critic mechanism for personalized\\nplanning, with the Actor implementing plans and the Critic assessing actions for long-term benefits.\\nFig. 14. The illustration of lifelong behavior sequence\\nand personalized low-rank adaption (LoRA) for recom-\\nmendation [459].\\nSLMs as a Personalized Recommender. Gener-\\native language model-based recommender sys-\\ntems require integrating user knowledge, typi-\\ncally achieved through fine-tuning. Fine-tuning\\ntechniques like LoRA [145] can incorporate ex-\\ntensive knowledge across all users by training\\nan external module with a small number of pa-\\nrameters A and B, but this approach often over-\\nlooks individual user preferences. To address\\nthis, RecLoRA [459] utilizes Vicuna-7B [63] to\\nintegrate personalized knowledge into SLM-\\ns/LLMs tailored for recommendation tasks, as\\nillustrated in Figure 14. Specifically, RecLoRA\\nmaintains a set of parallel, independent LoRA\\nweights (A푖, B푖), allowing for the customiza-\\ntion of language model parameters to match individual user preferences more effectively.\\nSLM as a Content Encoder. Language models, particularly when deep, provide an effective starting\\npoint for fine-tuning on downstream tasks. In news recommendation systems, the representational\\ncapability of a model significantly impacts performance. Consequently, many news recommender\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 28}, page_content='145:28\\nF. Wang et al.\\nFig. 15. Roles of SLM in web search.\\nsystems now employ language models fine-tuned on specific datasets as text encoders. For ex-\\nample, Wu et al. [384] conduct pioneering work using a pre-trained language model to enhance\\nlarge-scale news recommender systems by substituting traditional news encoders with a BERT\\nmodel [86]. However, BERT may struggle to capture content as it is pre-trained on limited data.\\nTherefore, ONCE [219] proposes using Llama-2-7B [345] as an encoder to overcome the limitations\\nof BERT in content-based recommendations. Additionally, the study explores the synergistic use of\\nLLMs in recommendation systems, finding that SLMs optimized with LoRA [145] outperform the\\nrecommendation results of systems assisted by generic LLMs such as ChatGPT.\\n4.1.4\\nSLM Applications in Web Search. Web search systems, involving retrieval and ranking, face\\nchallenges due to the diverse web documents and search queries. Traditional keyword-matching\\nmethods often fall short because of phrasing variations and the long-tail distribution of queries and\\ncontent, complicating accurate semantic inference. Effective integration of retrieval and ranking\\nmodels is also crucial. Language models, serving as content encoders, help overcome semantic\\nchallenges through their language understanding from pre-training [65, 92, 365]. Joint training of\\nretrieval and ranking models addresses integration, with SLMs ranking retrieved documents and\\nacting as re-rankers. Additionally, SLMs serve as rewriters in scenarios requiring enhanced query\\nunderstanding. Thus, in web search, SLMs fulfill three roles: (1) content encoder, (2) ranker, and (3)\\nrewriter, as depicted in Figure 15. Next, we give details.\\nSLM as a Content Encoder. Text embeddings are vector representations that encode semantic\\ninformation, widely used in retrieval; SLM-based dense retrieval utilizes pre-trained deep language\\nunderstanding to effectively tackle semantic challenges. H-ERNIE [65] employs a hierarchical\\nmodel that encodes queries and documents at multiple granularity—character, word, and phrase—to\\nimprove specificity and relevance in web search results by aggregating finer details into coarser\\nlayers, addressing issues like ambiguous queries. Implicit Interaction (퐼3) [92] uses BERT [86]\\nas a content encoder, generating implicit pseudo-queries from passages to enable high online\\nefficiency with offline caching of passage vectors. However, ERNIE- and BERT-style models overlook\\nadvancements in SLMs such as context length extension [299]. Thus, Peng et al. [279] employ\\nLLaMa-7B [344] and Vicuna-7B [63] as semantic encoders for embedding retrieval, demonstrating\\nimproved performance through soft prompt tuning. CoCondenser [110] addresses sensitivity to noisy\\ndata and large batch requirements during dense retriever training. Using the Condenser architecture\\nwith Transformer blocks, the model condenses information into dense vectors effectively.\\nSLM as a Ranker. The reranking task improves the order of multiple candidates to enhance\\nretrieval quality because rerankers are more accurate than embedding retrievers. InPars (Inquisitive\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 29}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:29\\nParrots for Search) [37] employs the T5 base 220M [291] as a re-ranker to enhance the BM25 retriever\\n[298]. Initially, BM25 selects 1\\u2009K candidates, re-ranked by a fine-tuned T5 model (monoT5) adapted\\nas a binary classifier to assess document-query relevance. Training data, generated by GPT-3 [38],\\nformulates queries and selects random negative examples. Experiments show the monoT5-enhanced\\nretriever significantly outperforms GPT-3; for example, it achieves a 0.3599 MAP score on the\\nTREC-DL 2020 dataset [72], surpassing GPT-3’s 0.3163.\\nSLM as a Rewriter. Queries to the retriever, typically just a few keywords, may reveal a knowledge\\ngap between the query and the knowledge needed for effective retrieval, thus limiting performance.\\nTo address this, the “rewrite-retrieve-read” framework [238] uses T5-large [291] to bridge the\\nknowledge gap in queries by rewriting them for more effective retrieval. This rewriter, trained via\\nreinforcement learning with downstream LLM performance as a reward, outperforms general LLM\\nrewrites. For example, it achieves a 45.97 F1 score on HotpotQA, surpassing the generic LLM’s\\n43.85 F1 score.\\n4.1.5\\nSLM Applications in Mobile-Device. The use of cloud-based LLMs on devices raises privacy\\nconcerns, and their large size limits real-time responses in urgent scenarios such as medical\\nemergencies. To overcome these issues, researchers are creating smaller, domain-specific models\\n(SLMs) that offer accurate results and suit mobile use. This subsection discusses SLM applications\\non mobile devices, focusing on three aspects: (1) software API calls, (2) mobile control, and (3) basic\\nNLP applications.\\nSLM for Tool Learning. Integrating LLMs with APIs enhances capabilities but incurs high training\\ncosts, prompting a shift to smaller, task-specific models that cut costs but risk errors. In response,\\nOctopus [54] uses a diverse dataset from over 30\\u2009K APIs and curriculum learning [222] to improve\\nAPI function accuracy. This method boosts API performance in models like Codellama-7b [299] and\\nGoogle’s Gemma series [338]. PhoneLM-1.5B-Call [422] is fine-tuned on DroidCall [394] datasets\\nand achieves comparable performance compared to GPT-4o-mini [269]. 훼-UMI [314] employs SLMs\\nas planners, callers, and summarizers within multi-agent systems, outperforming a single LLM in\\ntool uses.\\nFig. 16. An example workflow for an automated execution tool [90].\\nThe screenshot on the left is taken from [90].\\nSLM for Mobile Control. LM\\nagents facilitate user-device in-\\nteractions through taps, ges-\\ntures, and text, automating tasks\\nand enhancing user hands-\\nfree convenience. Unlike tra-\\nditional developer-based ap-\\nproaches that require extensive\\ndeveloper effort to design inter-\\nfaces and translate commands\\ninto API calls, LMs offer scalable\\nautomation via GUI-based text\\ncontent. MobileAgent [90] inte-\\ngrates instructions and Standard\\nOperating Procedures (SOP) to\\nimprove SLMs for mobile con-\\ntrol. As shown in Figure 16, it processes goals (e.g., booking a dental appointment) by analyzing\\nscreens, queries, prior actions, and UI elements, forming prompts to generate outputs and execute\\nactions (e.g., selecting text, XPath). Fine-tuning Qwen-7B [24] on AIA medical data, it outperforms\\nGPT-4 [2] on AitW [297], a key mobile benchmark, without extra inference costs. Carreira et al.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 30}, page_content='145:30\\nF. Wang et al.\\n[43] run a small offline model on mobile devices, fine-tuned with ChatGPT-3.5 data, enabling tasks\\nlike calls and web searches. RedPajama-INCITE-Chat-3B-v1 Computer [70], selected for its size\\nand chat features, uses native code and quantization, performing well on 6\\xa0GB and 4\\xa0GB Android\\ndevices.\\nFig. 17. An illustration of Vicuna-7B-powered mobile task automation\\n[43] shows a user asking to be reminded about doing laundry on Aug\\n17. The figure is taken from [43].\\nAutoDroid\\n[381] improves\\nAndroid app interactions via\\nGUI automation. Figure 17 shows\\nLLM-powered task automation\\n(e.g., setting a laundry reminder\\nfor Aug 17) in four steps: (1) click\\n“New Event,” (2) enter “laun-\\ndry” in “Title,” (3) click “Save,”\\nand (4) finish. Using Vicuna-\\n7B and app-specific knowledge,\\nAutoDroid generates privacy-\\nfiltered prompts for tasks. On\\nits DroidTask benchmark, it out-\\nperforms GPT-3.5 (34.7%) and\\nGPT-4 (54.5%) with 57.7% accuracy. M4 (composable mobile foundation model) [428] introduces a 9.2B\\nparameter model (7.5\\xa0GB memory) for mobile AI tasks, managed by the OS and hardware. Currently\\nlimited to high-end devices, its applicability will expand with increasing mobile memory/storage.\\nThese works highlight customizing smaller, domain-specific SLMs to address memory limits while\\npreserving functionality in mobile environments.\\nSLM for Basic NLP Applications on Devices. Performing basic NLP tasks such as text rewriting\\ndirectly on the device can enable personalization while ensuring privacy. The sparse annotation\\non devices is a challenge. Qin et al. [285] utilize self-supervised data selection and synthesis for\\non-device fine-tuning, leveraging sparse annotations and limited storage effectively. This approach,\\ndemonstrated in Figure 18, employs the Llama-3B model [344] and the LoRA fine-tuning method\\n[145], enhancing personalization by efficiently managing data through metrics including embedding\\nentropy and domain-specific scores. In mobile text rewriting, Zhu et al. [462] train the compact\\nPalm 2-XXS model [15] using data generated by the larger Palm 2-L to ensure user privacy and\\naccommodate device constraints. On its new benchmark, MessageRewriteEval, Palm 2-XXS achieves\\na BLEU score of 34.59, outperforming LLaMa-7B (16.65) [344]. Tests on the Samsung S23 show\\nlower latency (29 tokens/s) than a 4-bit LLaMa-7B on a MacBook M1 Pro (18-22 tokens/s), proving\\nits mobile efficiency for text rewriting.\\nInsights: We draw several key insights from the development of task-specific SLMs:\\n—There is considerable potential in enhancing the efficiency and effectiveness of small models by integrating\\nself-adaptive techniques with further fine-tuning and optimization of RAG-based methods.\\n—The growing relevance of SLMs in coding highlights their cost-effectiveness and efficiency as alternatives to\\nLLMs, providing quick processing and easy fine-tuning for specialized tasks; while LLMs handle complex tasks\\nwell, SLMs, optimized and fine-tuned on specific data, are increasingly essential in resource-limited settings.\\n—SLMs significantly enhance recommendation systems due to their robust generalization, reasoning abilities, and\\nin-context learning, addressing key challenges such as cold-start problems and distribution biases. They support\\nlong-term planning, replace traditional encoders, and use parallel low-rank parameters to inject personalized\\nuser knowledge effectively.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 31}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:31\\n—SLMs play a crucial role in web searches such as document encoding, text reordering, and query rewriting,\\noften outperforming LLMs through techniques such as SFT, soft prompt tuning, unsupervised contrastive loss,\\nand reinforcement learning, thereby enhancing adaptability and efficiency.\\n—SLMs are utilized on mobile devices primarily for privacy and memory constraints, with applications in API\\ncalls and mobile control; they are typically developed by generating data with LLMs and fine-tuning with\\nSLMs, or by using local SLMs to handle privacy with LLMs boosting performance, and their training involves\\ninnovative techniques like learning from data streams and managing non-IID time series data.\\n4.2\\nSLM Deployment on Mobile and Edge Devices\\nFig. 18. Overview of fine-tuning SLMs with synthesized and user\\ndata [285]. Data synthesis generates semantically similar text via\\nprompts, creating dialogue sets. Data selection processes user\\ndata, tags domains, and calculates metrics (EOE, DSS, IDD). Se-\\nlected data fine-tunes SLMs with user annotations. The iterative\\nframework refines SLMs through continuous data generation and\\nselection.\\nOn-device applications benefit uniquely\\nfrom the memory-saving efficiency\\nand rapid runtime performance of\\nSLMs, which offer advantages over\\nLLMs. However, devices with ex-\\ntremely limited resources may still\\nstruggle with the parameter sizes\\nof SLMs. To ensure both memory\\nand runtime remain within accept-\\nable ranges while still maintaining\\nperformance, it is crucial to inte-\\ngrate technologies that facilitate the\\ndeployment of SLMs on resource-\\nconstrained devices. The primary\\nchallenge for memory-efficient tech-\\nnologies arises from the inherent size\\nof the SLMs and their associated\\ncaches. To address this, we survey ex-\\nisting works focused on compressing\\nSLMs and their caches. Additionally,\\nthe large size of models significantly\\nimpacts runtime efficiency due to the\\nextensive computing workload and potential weight transfers between the memory buffer and\\nRAM/GPU memory. Other challenges include switching the Mixture of Experts between CPU\\nand GPU memory and managing resource scheduling when deploying SLMs across multiple local\\ndevices. Therefore, in this subsection, we review representative works that address these chal-\\nlenges under two aspects: memory efficiency optimization and runtime efficiency optimization, as\\nsystematically compiled in Table 7.\\n4.2.1\\nMemory Efficiency Optimization. Memory efficiency involves minimizing the memory\\nusage of both the model and the KV cache during deployment. This is typically achieved through\\nmodel compression techniques such as quantization [212, 292, 427, 452], the cache of MoE experts\\n[421], and KV cache compression [168].\\nCompression on model parameters. Quantization, a common method for deploying SLMs, lowers\\nnumerical precision to significantly save memory while preserving accuracy. We detail quantization\\nstrategies in Sections 2.3.3 and 3.5, focusing here on representative works for edge devices. EDGE-\\nLLM [427] adapts LLMs for edge devices using a Layer-wise Unified Compression (LUC) method\\nthat combines layer-specific pruning and quantization to reduce computational demands and an\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 32}, page_content='145:32\\nF. Wang et al.\\nTable 7. On-Device Deployment Optimization Techniques\\nAspect\\nRepresentative Work\\nKey Point\\nMemory\\nEfficiency\\nOptimization\\nEDGE-LLM [427]\\nEdge LLMs use LUC and adaptive tuning for efficiency.\\nLLM-PQ [452]\\nOptimize quantization and layer partitioning for complex setups.\\nAWQ [212]\\nPreserve key weights based on activation distribution.\\nMoE-I2 [409]\\nPrune less important experts in MoE.\\nMobileAIBench [260]\\nEvaluation.\\nEdgeMoE [421]\\nLoad experts on activation, tripling memory savings.\\nGEAR [168]\\nEnhance KV cache quantization by integrating error-reduction techniques.\\nDMC [263]\\nAdaptively compress KV cache, optimizing storage efficiency.\\nTransformer-Lite [196]\\nOptimize KV cache to reduce redundancy and memory use.\\nLLMaaS [424]\\nLLMaaS manages apps via chunk-wise KV cache optimization on mobiles.\\nRuntime\\nEfficiency\\nOptimization\\nmllm-NPU [400]\\nOn-device NPU (neural processing units) to reduce prefill latency.\\nCOST-EFF [312]\\nDistill a multi-exit model from the original PLM.\\nLLMCad [399]\\nUse SLM for fast token generation and cloud verification.\\nEdgeMoE [421]\\nPredict expert needs, boosting inference speed and reducing latency.\\nLinguaLinked [451]\\nOptimize dataflow and load, enhancing multi-threading efficiency.\\nAdaptive Layer Tuning and Voting scheme to optimize memory use while ensuring performance.\\nMeanwhile, LLM-PQ [452] addresses quantization and model layer partitioning for heterogeneous\\nclusters, incorporating a Cost Model and an Indicator Generator to optimize bit-width assignment\\nand layer partitioning through integer linear programming, enhancing quantization for complex\\ncomputational setups. Activation-aware Weight Quantization (AWQ) [212] is a hardware-\\nfriendly, low-bit, weight-only quantization method for on-device LLMs, preserving essential weights\\nbased on activation distribution to minimize quantization loss. MoE-I 2 [409] prunes less important\\nexperts and applies low-rank decomposition to the remaining experts to further optimize efficiency.\\nCache of MoE Experts. Beyond standard quantization, which reduces storage for all model pa-\\nrameters, another strategy involves caching a mixture of experts (MoE). Driven by the fact that\\nmemory storage is more cost-effective and scalable than computing capacity, the MoE architecture\\n[157] boosts performance while minimizing computational costs by activating only portions of the\\nLLM as needed. However, this approach incurs significant memory overhead, making it impractical\\nfor edge device memory constraints. For example, Switch Transformers [102], with 32 experts per\\nlayer, require 54\\xa0GBs of memory for inference, exceeding the capacity of most edge devices. Yi et al.\\n[421] note that in the Switch Transformers model, although most of the model weights (86.5%) are\\nattributed to experts, these weights account for only a small fraction (26.4%) of the computations. To\\naddress this, EdgeMoE [421] introduces a method where experts are loaded into an expert memory\\nbuffer only when activated, achieving approximately 3× memory savings compared to the baseline\\nwhere all weights are held in memory.\\nKV Cache Compression. When serving LMs for inference, using a KV cache is common practice\\nto avoid intensive recalculations and speed up generation [283]. However, cache memory con-\\nsumption escalates with model size and sequence length, posing a challenge for edge devices. To\\nmanage this, offloading techniques transfer KV caches to CPU memory [13, 315], although this\\ncan introduce significant overhead due to the switching costs between GPUs and CPUs. Token\\ndropping compresses cache size by keeping only key tokens, often identified by low attention scores\\n[113, 225, 449]. However, this method struggles with complex tasks, especially at high compression\\nlevels, due to increased estimation errors in compressed KV values. GEAR [168] addresses these\\nissues by enhancing KV cache quantization with error-reduction techniques, including: (i) quantiz-\\ning caches of similar magnitudes to ultra-low precision, (ii) using a low-rank matrix for efficient\\nquantization residual approximation, and (iii) employing a sparse matrix for correcting outliers.\\nThis approach separates coherent from incoherent approximation errors, enabling near-lossless\\nKV cache compression and achieving up to 2.29× peak memory reduction. Besides, Dynamic\\nMemory Compression (DMC) [263] adaptively compresses the KV cache by either adding new\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 33}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:33\\nkey and value representations directly or blending them with the top cache item using a weighted\\naverage. Transformer-Lite [196] tackles the redundancy of storing the KV cache twice in model\\ninputs and outputs, which increases memory use. It optimizes storage by allocating a large tensor\\nbased on the maximum sequence length needed for inference. Sub-tensors are then created from\\nthis main tensor at different address offsets to serve as input and output KV caches, allowing direct\\nwriting to the correct locations during inference and removing extra copying steps. LLMaaS [424]\\nintroduces LLM as a Service for mobile devices, managing all apps through LLMS. This system\\nuses chunk-wise KV cache compression and swapping, enabling efficient context switching within\\nmemory constraints. By segmenting the KV cache into independently compressed and swapped\\nchunks, LLMS balances memory use and I/O bandwidth better than token-level or context-level\\nmanagement.\\nHowever, all these compression schemes trade off memory for computation: decompressing the\\nKV cache (e.g., upscaling 4-bit values back to full precision or reconstructing dropped tokens) adds\\nextra operations and latency. In fact, GEAR notes that its low-rank/sparse reconstruction “incurs\\nextra latency,” and Transformer-Lite explicitly targets reducing “dequantization overhead” from\\nlow-bit caches. Similarly, the LLMaaS system even pipelines recompute when loading compressed\\nchunks.\\n4.2.2\\nRuntime Efficiency Optimization. The goal of decreasing computing workload aligns with\\nenhancing memory efficiency through methods such as quantization, as mentioned in the previous\\nsection. Decreasing model weight precision or reducing the number of weights naturally lowers\\nlatency. Other runtime efficiency techniques of minimizing inference latency involve, reducing\\nprefill latency, early exits, large and small model collaboration, decreasing switching time in MoE,\\nand reducing latency in distributed SLMs.\\nReducing Prefill Latency. mllm-NPU [400] is the first LLM inference system that leverages on-\\ndevice NPU (neural processing units) to reduce prefill latency and energy consumption. It incorpo-\\nrates a chunk-sharing graph, shadow outlier execution, and out-of-order subgraph execution to\\nenhance NPU offloading efficiency. Experiments have shown mllm-NPU’s superior performance\\nbenefits, including up to 43.6× speedup and 59.5× energy savings.\\nDynamic Early Exits. A decoupled runtime saving technique is dynamic early exits. Originating\\nfrom BranchyNet [341], which introduces exit branches after specific convolution layers in the\\nCNN model, this concept has been adapted for PLMs as Transformer layer-wise early exiting [396].\\nEarly exiting enables dynamic acceleration during inference and reduces temporal overhead by\\nallowing exit without passing through all model layers. To address the inconsistency issue arising\\nfrom exiting at different layers, COST-EFF [312] distills a multi-exit model from the original PLM.\\nLarge and Small Model Collaboration. Model collaboration, deploying SLMs on devices with cloud-\\nbased LLM support, enhances runtime efficiency. LLMs will increase latency when directly deployed\\nvia mobile engines like llama.cpp due to a large number of computing operations. LLMCad [399]\\naddresses this by using a real-time, memory-resident SLM for simple tokens such as determiners\\nand punctuation. The SLM generates tokens, while a cloud-based LLM verifies and corrects them,\\nspeeding up the process. LLMCad enhances token generation up to 9.3× by pairing the memory-\\nresident SLM, Llama 2 7B, with the cloud-based LLM, Llama 2 13B, cutting latency from 16.2 to\\n3.9\\u2009seconds on Xiaomi Pro for TruthfulQA tasks [213].\\nReducing MoE Switching Time. To reduce latency in MoE architectures caused by frequently\\nswitching experts in limited device memory, EdgeMoE [421] enhances runtime efficiency by pre-\\nemptively predicting which expert will be needed, based on the observed long-tail distribution\\nof unbalanced expert activations. It utilizes a statistical model, built offline, to estimate expert\\nactivation probabilities in transformer layers from previous activations. During inference, EdgeMoE\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 34}, page_content='145:34\\nF. Wang et al.\\npreemptively loads the most likely needed expert, accelerating inference by 1.11× to 2.78× and\\nsignificantly reducing latency. For instance, in a switch transformer model with 8 experts, latency\\ndrops from approximately 0.7\\u2009s to 0.3\\u2009s, outperforming the baseline method that preloads experts\\nbased on hit ratios.\\nReducing Latency in Distributed SLMs. Distributing an SLM across smaller devices reduces\\nthe need for extensive model compression while preserving accuracy. However, this approach\\nfaces challenges that incur latency, such as managing diverse device capabilities, handling data\\ndependencies between model segments, and adapting to dynamic resource availability. To address\\nthese issues, LinguaLinked [451] addresses these issues by optimizing model assignment to match\\ndevice capabilities and minimize data transmission, implementing runtime load balancing to\\nredistribute tasks and prevent bottlenecks, and enhancing communication for efficient data exchange\\nbetween segments. With multi-threading, the system improves, achieving acceleration rates between\\n1.73× and 2.65× for both quantized and full-precision models.\\nInsights: We draw several key insights from the deployment of SLMs:\\n—Model size remains a bottleneck for both memory and runtime efficiency. A common solution is model\\nquantization, which reduces model precision to save memory and lessen computing workload, thereby\\nboosting inference speed [212, 227, 260, 292, 427, 452]. Similarly, KV cache compression also helps achieve\\nthese efficiency gains [168, 196, 263, 424].\\n—Mixture of Experts (MoE) is commonly used in SLMs to enhance performance using the same computing\\nresources, but it results in increased memory usage. To address this, only activated experts are loaded\\ninto the memory buffer while the majority are stored cold on disk. However, the cost of switching can\\nslow down inference. Designing a preemptive expert pre-load strategy could therefore accelerate the\\ninference [421].\\n—Model collaboration between local SLMs and cloud-based LLMs enhances both memory and runtime\\nefficiency by using smaller models on local devices, which are then verified by cloud LLMs to ensure\\nperformance is maintained. Using SLMs locally reduces memory usage and shortens the inference time\\nfrom the local model. However, internet latency and delays in cloud LLM inference can still introduce\\nlatency. Verifying SLM outputs every 푁tokens using LLMs can effectively mitigate this latency [399].\\n—One deployment approach involves deploying SLMs/LLMs across multiple trusted local devices to\\nmaintain original performance while only loading a fraction of the model weights. However, this method\\ncan incur latency due to varying device capabilities and resource scheduling challenges. To address these\\nissues, optimizing model assignment to align with device capabilities and minimizing data transmission\\nare effective strategies [451].\\n5\\nGeneric and Domain-Specific SLMs\\nThis section investigates SLMs (with fewer than 7 billion parameters) in both general and specific\\ndomains. It details the methods of obtaining these SLMs the datasets, and the evaluation tasks,\\nexploring the techniques for acquiring SLMs through compression, fine-tuning, or training from\\nscratch. Additionally, we summarize the representative SLMs as detailed in Tables 8 and 11.\\n5.1\\nGeneric-Domain SLMs\\nOverview. SLMs, with fewer parameters than LLMs, enhance computational efficiency in pre-\\ntraining, fine-tuning, and inference, reducing memory and energy demands—crucial for resource-\\nlimited environments. Their compact, localized nature boosts privacy, personalization, and response\\ntimes, making them ideal for low-power edge devices. Therefore, SLMs are attracting increasing\\nattention, and various models are being developed. Table 8 summarizes current representative\\ngeneric-domain 42 SLMs/SLM families. Although all chosen SLMs have similar architectures, they\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 35}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:35\\nTable 8. High-Level Overview and Training Details of Generic-Domain SLMs\\nModel\\n#Params\\nDate\\nPara-\\ndigm\\nDo-\\nmain\\nTraining Datasets\\nTraining Techniques\\nPhoneLM\\n[422]\\n0.5B; 1.5B\\n2024.11\\nPre-train\\nGeneric\\nDCLM-baseline [195], Star-\\nCoderData [199]; OpenWeb-\\nMath [274], Dolma-algebraic\\nand Dolma-arXiv [322]\\nRoPE, MHA, Gated FFN, RMSNorm, ReLU,\\nFSDP, Flash Attention 2, ZeRO\\nLlama 3.2 [7]\\n1B; 3B\\n2024.9\\nPre-train\\nGeneric\\nno release (9T tokens)\\nGQA, SiLU, Multilingual Text and code,\\nShared embedding, Pruning, Distillation,\\nSFT, RLHF, RS, DPO\\nQwen 1 [24]\\n1.8B; 7B; >\\n2023.12\\nPre-train\\nGeneric\\nNo release\\nMHA; RoPE; SwiGLU; RMSNorm\\nQwen\\n1.5\\n[24]\\n0.5B;\\n1.8B;\\n4B; 7B; >\\n2024.2\\nPre-train\\nGeneric\\nNo release\\nMHA; RoPE; SwiGLU; RMSNorm; Multi-\\nlingual support\\nQwen 2 [407]\\n0.5B;1.5B;\\n7B; >\\n2024.6\\nPre-train\\nGeneric\\nNo release\\nGQA; RoPE; SwiGLU; RMSNorm; Multilin-\\ngual support\\nQwen\\n2.5\\n[407]\\n0.5B;\\n1.5B;\\n3B; 7B; >\\n2024.9\\nPre-train\\nGeneric\\nNo release\\nGQA; RoPE; SwiGLU; RMSNorm; Multilin-\\ngual support; Larger corpus\\nGemma [338]\\n2B; 7B\\n2024.2\\nPre-train\\nGeneric\\nUnknown\\nMHA, RoPE, GELUtanh\\nGemma\\n2\\n[339]\\n2B; >\\n2024.7\\nPre-train\\nGeneric\\nUnknown\\nGQA; RoPE; GELUtanh; Alternating Local\\nand Global Attention; Logit Soft-Capping;\\nRMSNorm for Pre- and Post-Normalization\\nSmolLM [10]\\n135M;\\n360M; 1.7B\\n2024.7\\nPre-train\\nGeneric\\nSmollm-corpus [28]\\nGQA, trapezoidal LR scheduler\\nH2O-\\nDanube3\\n[281]\\n500M; 4B\\n2024.7\\nPre-train\\nGeneric\\nUnknown\\nThree different training stages with differ-\\nent data mixes\\nFox-1 [340]\\n1.6B\\n2024.6\\nPre-train\\nGeneric\\nUnknown (3T tokens)\\nGQA; Deep architecture\\nRene [115]\\n1.3B\\n2024.5\\nPre-train\\nGeneric\\nDolma-1.7 [322]\\nMamba-2 layers, sliding-window attention\\n(SWA)\\nMiniCPM\\n[146]\\n1.2B; 2.4B\\n2024.4\\nPre-train\\nGeneric\\nDolma [322]; C4 [291]; Pile\\n[87]; stack [177]; StarCoder\\n[199]; UltraChat [89]; Os-\\nsInstruct [378]; EvolInstruct\\n[397]\\nWSD learning rate scheduler\\nCT-LLM [94]\\n2B\\n2024.4\\nPre-train\\nGeneric\\nMAP-CC\\nChinese, MHA, RoPE, SwiGLU, RMSNorm\\nPhi-1 [122]\\n1.3B\\n2023.6\\nPre-train\\nCoding\\nCodeTextBook [122]a\\nMHA, GELUtanh, RoPE, FlashAttention\\nPhi-1.5 [204]\\n1.3B\\n2023.9\\nPre-train\\nGeneric\\nCodeTextBook [122]; Syn-\\nthetic Datasets (20B)\\nMHA, GELUtanh, RoPE, FlashAttention,\\nDeep ZeRO Stage 2\\nPhi-2 [158]\\n2.7B\\n2023.12\\nPre-train\\nGeneric\\nCodeTextBook [122]; Syn-\\nthetic Datasets (20T)\\nMHA, GELUtanh, RoPE, FlashAttention,\\nDeep ZeRO Stage 2\\nPhi-3 [1]\\n3.8B; 7B; >\\n2024.4\\nPre-train\\nGeneric\\nScaled-up dataset from phi-2\\nMHA, SiLU, RoPE, FlashAttention, Deep\\nZeRO Stage 2\\nPhi-3.5 [1]\\n3.8B;\\n4.2B;\\n6.6B\\n2024.4\\nPre-train\\nGeneric\\nmore multilingual and long-\\ntext data\\nMultilingual; Vision; MHA, SiLU, RoPE,\\nFlashAttention, ZeRO 2\\nOpenELM\\n[246]\\n270M;\\n450M; 1.1B;\\n3B\\n2024.4\\nPre-train\\nGeneric\\nRefinedWeb\\n[276],\\ndedu-\\nplicated PILE [109], partial\\nRedPajama\\n[70],\\npartial\\nDolma v1.6 [322]\\nNo biases in FC layers; Pre-norm: RM-\\nSNorm; Pos encoding: RoPE; Attention:\\nGQA; FFN: SwiGLU; Tokenizer: LLaMA-\\nstyle\\nMobiLlama\\n[342]\\n0.5B; 0.8B\\n2024.2\\nPre-train\\nGeneric\\nLLM360 Amberb\\nGQA; SwiGLU; Parameter sharing; multi-\\nmodal\\n(Continued)\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 36}, page_content='145:36\\nF. Wang et al.\\nTable 8. Continued\\nModel\\n#Params\\nDate\\nPara-\\ndigm\\nDo-\\nmain\\nTraining Datasets\\nTraining Techniques\\nMobileLLM\\n[227]\\n125M; 350M\\n2024.2\\nPre-train\\nGeneric\\nUnknown (1\\u2009T tokens)\\nSwiGLU FFN, deep and thin architectures,\\nembedding sharing, and GQA\\nOLMo [118]\\n1B; 7B\\n2024.2\\nPre-train\\nGeneric\\nDolma [322]c\\nSwiGLU; RoPE, Non-parameteric Layer\\nNorm\\nTinyLlama\\n[444]\\n1B\\n2024.1\\nPre-train\\nGeneric\\nSlimPajama [321] and Star-\\nCoder [199]\\nGQA, SiLU, FSDP, Flash Attention [76],\\nxFormers [187]\\nStableLM\\n[346]\\n3B; 7B\\n2023.4\\nPre-train\\nGeneric\\nRefinedWeb [276],\\nRedPajama [70], the Stack\\n[177], OpenWebText [116],\\nOpenWebMath [274], and\\npartial CulturaX [264]\\nMHA; SiLU; Fine-tuning; DPO; Self-\\nknowledge; RoPE; LayerNorm; no Biases\\nStableLM\\n2\\n[27]\\n1.6B\\n2024.2\\nPre-train\\nGeneric\\nCerebras-\\nGPT [87]\\n111M;\\n256M;\\n590M; 1.3B;\\n2.7B;\\n6.7B;\\n>\\n2023.4\\nPre-train\\nGeneric\\nPile [109]\\nMHA; GELU; Maximal Update Parameteri-\\nzation\\nPythia [32]\\n14M; 70M;\\n160M;\\n410M;\\n1B;\\n1.4B;\\n2.8B;\\n6.9B; >\\n2023.4\\nPre-train\\nGeneric\\nPile [109]\\nMHA; GELU; Flash Attention [77]; RoPE\\n[324]; ZeRO [293]\\nBLOOM,\\nBLOOMZ\\n[184]\\n560M; 1.1B;\\n1.7B;\\n3B;\\n7.1B; >\\n2022.11\\nPre-train\\nGeneric\\nROOTS [183] and 13 pro-\\ngramming languages\\nMHA; GELUtanh; ALiBi Positional Embed-\\nding [284], Embedding LayerNorm [83]\\nGalactica\\n[336]\\n125M; 1.3B;\\n6.7B; >\\n2022.11\\nPre-train\\nScien-\\ntific\\nOpen-access scientific mate-\\nrials (106B tokens) but not\\nreleased\\nMHA; GeLU; Learned Positional Embed-\\ndings\\nOPT [445]\\n125M;\\n350M; 1.3B;\\n2.7B; 5.7B\\n2022.5\\nPre-train\\nGeneric\\nPile [109] and PushShift.io\\nReddit [25]\\nMHA; ReLU\\nXGLM [214]\\n1.7B;\\n2.9B;\\n>\\n2021.12\\nPre-train\\nGeneric\\nCC100-XL\\n–\\nGPT-Neo\\n[34]\\n125M;\\n350M; 1.3B;\\n2.7B\\n2021.5\\nPre-train\\nGeneric\\nPile [109]\\n–\\nMegatron-\\ngpt2 [317]\\n355M; 2.5B;\\n>\\n2019.9\\nPre-train\\nGeneric\\nWikipedia [86], CC-Stories\\n[347],\\nRealNews\\n[435],\\nOpenWebtext\\n–\\nMINITRON\\n[259]\\n4B; >\\n2024.7\\nDistil-\\nlation;\\nPruning\\nGeneric\\n8\\u2009T tokens in Nemotron-4\\n[273]\\nLR WSD Scheduler\\nOrca 2 [253]\\n7B\\n2023.11\\nDistilla-\\ntion\\nGeneric\\nOrca 2 dataset\\nLLaMA-2-7B based; prompt erasing\\nOrca [258]\\n13B\\n2023.6\\nDistilla-\\ntion\\nFLAN-v2 [229]\\nFrom ChatGPT and GPT4, Explanation\\ntuning; Progressive Learning\\nMINIMA\\n[439]\\n3B\\n2023.11\\nDistilla-\\ntion\\nGeneric\\nPile [109], Wudao [429],\\nGitHub [70]\\nFrom Llama-2-7B, Zero2, Flash Attention,\\nOptimal teacher size\\nDolly-v2 [71]\\n3B; 7B; >\\n2023.4\\nInstruc-\\ntion\\ntuning\\nGeneric\\nDatabricks-dolly-15k [71]\\nFrom pythia\\n(Continued)\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 37}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:37\\nTable 8. Continued\\nModel\\n#Params\\nDate\\nPara-\\ndigm\\nDo-\\nmain\\nTraining Datasets\\nTraining Techniques\\nLaMini-LM\\n[174]\\n61M-7B\\n2023.4\\nDistilla-\\ntion\\nGeneric\\nLaMini instruction dataset\\nA collection of SLMs distilled from\\nChatGPT-generated 2.58M instructions.\\nSpecialized\\nFlanT5 [107]\\n250M;\\n760M; 3B\\n2023.1\\nInstruc-\\ntion\\nTuning\\nGeneric\\n(math)\\nGSM8K\\nBase model is FlanT5\\n#Params means parameter amounts. “>” indicates parameters larger than 7B.\\naCodeTextBook includes stack v1.2, code contests, synthetic python textbooks and exercises.\\nbLLM360 Amber includes Arxiv, Book, C4, Refined-Web, StarCoder, StackExchange, and Wikipedia.\\ncDolma includes Dolma’s CC, RefinedWeb, Star Coder, C4, PushShift API, Semantic Scholar, RedPajama, Flan, CC News,\\nOpenWebMath, MetaWika, Wikipedia.\\nFig. 19. Llama 3.2 1B model card.\\nvary in specific training datasets and techniques, with some datasets not being openly available.\\nTaking the latest Llama 3.2 1B models [7] in Figure 19 as an example, its parameter size and use of\\nfiltered high-quality training data, pruning-based initialization, KD pre-training tasks, and training\\ntechniques such as SFT Rejection Sampling (RS), and DPO distinguish it from others.\\n5.1.1\\nArchitecture Design. From Table 8, we observe several trends in component choices for\\nSLMs:\\n(1) Recent SLMs frequently employ GQA in self-attention mechanisms because it can reduce\\ncomputational complexity. GQA achieves this by sharing query representations across mul-\\ntiple heads while keeping key and value representations separate. This approach aligns with\\nthe goals of SLM to enhance efficiency without compromising functionality.\\n(2) The choice of activation function should balance model capability and efficiency. ReLU,\\nknown for its efficiency, introduces greater sparsity to the model, which facilitates faster\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 38}, page_content='145:38\\nF. Wang et al.\\nTable 9. Zero-Shot Performance of Various Language Models from Small to Large on Common\\nBenchmarks\\nModel Size Range\\nModel\\nMMLU\\nHellaSwag\\nARC (C)\\nPIQA\\nWinogrande\\n<1B\\nGPT-Neo 125M\\n26.0\\n30.3\\n23.0\\n62.5\\n51.8\\nTiny-Starcoder 170M\\n26.8\\n28.2\\n21.0\\n52.6\\n51.2\\nCerberas-GPT 256M\\n26.8\\n29.0\\n22.0\\n61.4\\n52.5\\nOPT 350M\\n26.0\\n36.7\\n23.6\\n64.7\\n52.6\\nMegatron-GPT2 345M\\n24.3\\n39.2\\n24.2\\n66.9\\n53.0\\nLiteLlama\\n26.2\\n38.5\\n24.9\\n67.7\\n49.9\\nGPT-SW3 356M\\n25.9\\n37.1\\n23.6\\n64.9\\n53.0\\nPythia 410M\\n27.3\\n40.9\\n26.2\\n67.2\\n53.1\\nXGLM 564M\\n25.2\\n34.6\\n24.6\\n64.9\\n53.0\\nLamini-GPT-LM 0.59B\\n25.5\\n31.6\\n24.2\\n63.9\\n47.8\\nMobiLlama 0.5B\\n26.5\\n52.5\\n29.5\\n72.0\\n57.5\\nMobiLlama 0.8B\\n26.9\\n54.1\\n30.2\\n73.2\\n57.5\\n1B–3B\\nStableLM 2 1.6B\\n41.8\\n68.2\\n43.8\\n74.0\\n64.9\\nPythia 1B\\n24.3\\n44.7\\n33.1\\n69.1\\n53.3\\nTinyLLaMA 1.1B\\n25.02\\n61.4\\n32.6\\n73.5\\n59.4\\nOLMo 1B\\n24.23\\n62.9\\n34.5\\n75.1\\n59.9\\nOLMo 1B (0724)\\n25.45\\n66.9\\n36.5\\n74.9\\n61.4\\nBoomer 1B\\n25.4\\n31.6\\n22.3\\n58\\n51.0\\nPythia-Dedup 1B\\n24.3\\n49.6\\n29.1\\n70.2\\n54.0\\nFalcon-RW 1B\\n25.4\\n63.1\\n35.1\\n74.1\\n61.9\\nCerebras-GPT 1.3B\\n26.7\\n38.5\\n26.1\\n66.8\\n53.6\\nLamini 1.3B\\n28.5\\n38.1\\n26.6\\n67.9\\n50.6\\nOPT 1.3B\\n24.6\\n54.5\\n29.6\\n72.5\\n59.7\\nGPT-Neo 1.3B\\n24.8\\n48.5\\n31.3\\n71.1\\n57.1\\nPythia-Deduped 1.4B\\n25.5\\n55.0\\n32.6\\n–\\n56.9\\nMobiLlama 1.2B\\n24.8\\n63.0\\n34.6\\n–\\n62.0\\nOpenELM 1.1B\\n25.3\\n64.8\\n32.3\\n75.6\\n61.7\\nOLMo-1B-0724-hf 1.2B\\n25.5\\n66.1\\n32.3\\n75.1\\n61.7\\nAMD-OLMo-1B 1.2B\\n24.9\\n63.6\\n33.7\\n75.6\\n61.6\\nGemma 2 2B\\n57.8\\n61.1\\n76.7\\n81.2\\n72.3\\nLlama 3.2 1B\\n49.3\\n41.2\\n59.4\\n–\\n–\\nLlama 3.2 3B\\n63.4\\n69.8\\n78.6\\n–\\n–\\n3B–7B\\nPhi-3.5-mini 3.8B\\n69.0\\n81.4\\n87.4\\n–\\n–\\nPythia 6.9B\\n–\\n63.8\\n44.1\\n75.2\\n60.9\\nFalcon 7B\\n–\\n75.9\\n47.5\\n78.5\\n68.9\\nLLaMA 1 7B\\n35.1\\n76.2\\n44.5\\n77.2\\n70.5\\nLLaMA 2 7B\\n45.3\\n76.8\\n48.5\\n76.7\\n69.4\\nMPT 7B\\n30.2\\n77.6\\n46.5\\n77.3\\n69.9\\nRPJ-INCITE 7B\\n27.8\\n70.3\\n42.8\\n76.0\\n64.7\\nOLMo 7B\\n61.1\\n76.4\\n48.5\\n78.4\\n58.0\\n>7B\\nLLaMA 2 13B\\n55.6\\n80.7\\n48.8\\n80.8\\n72.9\\nLLaMA 2 70B\\n67.6\\n77.3\\n55.5\\n82.0\\n75.0\\nGemma 2 9B\\n70.6\\n87.3\\n89.5\\n86.1\\n78.8\\nGPT-3.5\\n86.4\\n90.1\\n87.4\\n89.3\\n82.7\\nClaude 2\\n89.3\\n92.1\\n88.0\\n91.5\\n85.5\\nGPT-4\\n90.6\\n93.5\\n94.0\\n92.7\\n87.2\\nData from MobiLlama [342], OLMo/OLMoE [118, 257], and public evaluations of GPT-3.5, Claude 2, and GPT-4.\\ncoefficient calculations for inference acceleration. In contrast, SwiGLU’s parameters are\\nlearned during training, allowing the model to dynamically adapt to diverse tasks and\\ndatasets, thereby enhancing model capabilities and establishing it as a state-of-the-art option.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 39}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:39\\nTable 10. Comparison of MobiLlama 0.5B with Larger Models (1.2B–180B) in Terms of Efficiency and\\nResource Consumption on Low-End Hardware Devices [342]\\nPlatform\\nModel\\n#Params\\nAvg Tokens/s\\nMemory (MB)\\nBattery /1kT (mAh)\\nRTX 2080Ti\\nGPT-4\\n>100B\\nN/A\\nN/A\\nN/A\\nClaude 2\\n>100B\\nN/A\\nN/A\\nN/A\\nFalcon\\n180B\\n<1.0\\n∼3,20,000\\nN/A\\nLLaMA 2\\n70B\\n∼5.8a\\n∼1,28,000\\nN/A\\nMixtral 8 ×7B\\n46.7B (MoE)\\n∼10–20b\\n∼90,000\\nN/A\\nLLaMA 2\\n13B\\n∼20.0\\n∼11,000\\n∼85\\nGemma 2\\n9B\\n∼24.0\\n∼8,000\\n∼80\\nMistral\\n7.3B\\n∼30.0\\n∼20,000\\n∼68\\nPhi-2\\n2.7B\\n32.19\\n12,071\\n59.1\\nMobiLlama Large\\n1.2B\\n50.61\\n6,254\\n18.9\\nMobiLlama\\n0.5B\\n63.38\\n3,046\\n8.2\\nCPU i7\\nGPT-4\\n>100B\\nN/A\\nN/A\\nN/A\\nClaude 2\\n>100B\\nN/A\\nN/A\\nN/A\\nFalcon\\n180B\\n∼0.4–1.1c\\n∼7,00,000\\nN/A\\nLLaMA 2\\n70B\\n∼0.1–1.0\\n∼2,56,000\\nN/A\\nMixtral 8 × 7B\\n46.7B (MoE)\\n∼1–2\\n∼90,000\\nN/A\\nLLaMA 2\\n13B\\n∼3.3\\n∼8,400\\n∼130\\nGemma 2\\n9B\\n∼4.5\\n∼5,500\\n∼97\\nMistral\\n7.3B\\n∼12.0\\n∼3,500\\n∼37\\nPhi-2\\n2.7B\\n22.14\\n1,972\\n27.4\\nMobiLlama Large\\n1.2B\\n29.23\\n1,163\\n10.8\\nMobiLlama\\n0.5B\\n36.32\\n799\\n4.9\\nSnapdragon\\nFalcon/GPT-4/Claude 2\\n>70B\\nN/A\\nN/A\\nN/A\\nMixtral TurboSparse\\n47B (MoE)\\n11.7\\n24,000\\nN/A\\nLLaMA 2\\n13B\\nN/A\\nN/A\\nN/A\\nGemma 2\\n9B\\nN/A\\nN/A\\nN/A\\nPhi-2\\n2.7B\\n2.88\\n1,893\\n14.6\\nMobiLlama Large\\n1.2B\\n6.69\\n780\\n6.0\\nMobiLlama\\n0.5B\\n7.02\\n770\\n5.3\\na: PowerInfer with CPU–GPU offloading (i7-12700K\\xa0+\\xa0RTX 2080 Ti). b: Mixtral acts like a ∼13B model in compute per\\ntoken; effective throughput depends on offloading. c: Falcon 180B 4–8\\u2009bit quantized on 256\\xa0GB server CPU (reddit).\\nSources: openreview.net, arxiv.org, reddit.com, news.ycombinator.com, mistral.Ai, anthropic.com, en.wikipedia.org.\\nSiLU, situated between these two, is favored for its balance of computational efficiency and\\nmodel performance.\\n(3) RMS normalization is commonly used than layer normalization due to its reduced computa-\\ntional demands.\\nA basic introduction to these options is provided in Section 2. Apart from component choices, there\\nare notable innovations in architecture for SLMs:\\n—Mobilellm [227] highlights that deeper models are more effective than wider ones for improving\\nperformance.\\n—Embedding sharing [445] is crucial, as embedding layers often constitute over 20% of a model’s\\nparameters—for example, with 512 dimensions and a 32k vocabulary, each layer holds 16M\\nparameters in a 125M-parameter model. Smaller models often reuse these weights for both\\ninput and output layers, enhancing efficiency and compactness.\\n—Layer sharing [227] increases hidden layers in small Transformer models without additional\\nstorage costs.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 40}, page_content='145:40\\nF. Wang et al.\\nTable 11. High-Level Overview and Training Details of Specific-Domain SLMs\\nModel\\n#Params\\nDate\\nBase Models\\nDomain\\nTraining Datasets\\nTrain Techniques\\nHippocrates [3]\\n7B\\n2024.4\\nInstruction\\nTuning\\n(LLaMA2\\n[345],\\nMistral [163])\\nHealthcare\\nMedical Guidelines,\\nPMC-Patients [456],\\nand\\nPubMedQA-\\ncontexts [166]\\nContinual pre-training,\\ninstruction tuning, RLHF\\nBioMedLM [35]\\n2.7B\\n2024.3\\nFrom\\nscratch\\nand\\nFine-tuning\\nHealthcare\\nPubMed [109]\\nFSDP\\nBioMistral [181]\\n7B\\n2024.2\\nMistral [163]\\nBiomedicine\\nPubMed [109]\\nContinual pretraining\\nMentaLLaMA\\n[411]\\n7B; 13B\\n2023.9\\nInstruction\\nTuning\\n(LLaMA2 [345])\\nHealthcare\\nIMHI dataset\\nRLHF; PEFT\\nAdaLM [419]\\n34M\\n2021.6\\nDistillation\\n(BERT\\n[86] or MiniLM [368])\\nHealthcare\\nPubMed [109]\\nContinual\\npretraining,\\nAdapt-and-Distill\\nRho-1 [215]\\n1B; 7B\\n2024.4\\nTinyLlama-1.1B [444],\\nMistral-7B [163]\\nScience\\n(Mathemat-\\nics)\\nOpenWebMath [274]\\nContinual pretraining\\nChemLLM [441]\\n7B\\n2024.4\\nInstruction\\nTuning\\n(InternLM2)\\nScience\\n(Chemistry)\\nChemData\\nContinual training and\\nfine-tuning\\nSciGLM [440]\\n6B\\n2024.3\\nInstruction\\nTuning\\n(ChatGLM-6B)\\nScience\\nSciInstruct\\nSelf-reflective\\ninstruc-\\ntion annotation\\nLlemma [23]\\n7B\\n2023.10\\nCode Llama 7B\\nScience\\n(Mathemat-\\nics)\\nProof-Pile-2 [23]\\nContinual pre-training\\nOceanGPT [31]\\n2B;\\n7B;\\n14B\\n2023.10\\nLLaMA2 [345]\\nScience\\n(Ocean)\\nOpen-access\\nlitera-\\nture, DoINSTRUCT\\nContinual pre-training,\\nInstruction tuning\\nAstroLLaMA\\n[265]\\n7B\\n2023.9\\nTuning (LLaMA-2-7B)\\nScience\\n(As-\\ntronomy)\\narXiv abstracts from\\nKaggle\\nContinual training\\nDARWIN [393]\\n7B\\n2023.8\\nLLaMa 7B\\nScience\\n(physics,\\nchemistry,\\nand material)\\nSciQ [380], Scientific\\npaper\\n[393],\\nFAIR\\n[393]\\nFine-tuning\\nMindLLM [417]\\n1.3B; 3B\\n2023.10\\nFrom-scratch and SFT\\nLaw, Finance\\nPile\\n[109],\\nWudao\\n[429], CBooks\\nTrain on Bilingual Mix-\\nture Data, SFT\\n—Shared FFNs [342] make up about 65% of all trainable parameters, with attention mechanisms\\nand heads accounting for the rest. Sharing FFN parameters across all transformer layers of an\\nSLM is proposed to increase efficiency.\\n—Architecture search ahead of pre-training. PhoneLM [422] proposes a principle for constructing\\non-device SLMs: searching for a resource-efficient architecture on a given hardware to optimize\\nthe speed-capacity trade-off before pretraining. This approach inspires the tailored selection of\\narchitectural components for on-device SLMs, based on specific compositional requirements\\nsuch as computing efficiency, model capability, and safety.\\nA detailed description of these architectural designs can be found in Section 3.1.\\n5.1.2\\nTraining Datasets. From Table 8, we can observe a set of widely used training datasets in\\nSLM development. We provide the details below:\\n—Pile [109]: It comprises 22 smaller, high-quality, diverse corpora from various domains, such\\nas Pile-CC, PubMed Central, ArXiv, GitHub, and FreeLaw, designed to offer a comprehensive\\nfoundation for language model training. The dataset contains 207 billion tokens and totals\\n825\\xa0GB.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 41}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:41\\n—C4 (Colossal Clean Crawled Corpus) [291]: This dataset includes 350 billion tokens, representing\\na cleaned version of the Common Crawl web corpus, intended to capture a wide snapshot of\\nthe internet.1\\n—The Stack [177]: It contains 6 trillion tokens of source code from over 300 programming\\nlanguages, useful for developing code-centric AI applications. Python-edu in smollm-corpus\\n[28] consists of Python files that are scored 4 or more by the educational code model and are\\nextracted from the stack-v2-train dataset.\\n—StarCoder [199]: It features 35 billion tokens, predominantly Python code, aimed at program-\\nming language understanding and generation.\\n—RedPajama [70]: This dataset encompasses 1.2 trillion tokens derived from over 100 billion\\ntext documents, processed using the CCNet pipeline to ensure a rich collection of web texts.\\n—RefinedWeb [276]: This dataset includes 5 trillion tokens of high-quality, extensively filtered\\nweb data, offering a valuable resource for training web-aware models.\\n—PushShift.io Reddit [25]: A resource around 5 billion tokens for social media data collection,\\nanalysis, and archiving, specifically of Reddit data, aiding research into social media dynamics.\\n—CulturaX [264]: It comprises 6.3 trillion tokens across 167 languages, supporting the develop-\\nment of models with extensive linguistic and cultural understanding.\\n—FineWeb [275]: A large-scale (15-trillion tokens, 44 TB disk space) dataset for LLM pretraining.\\nFineWeb is derived from 96 CommonCrawl snapshots. FineWeb-Edu is a subset of FineWeb\\nconstructed using scalable, automated, high-quality annotations for educational value.\\nFrom the analysis of these datasets, we can derive several critical insights regarding the development\\nof SLMs: (i) Data quality is crucial for training effective SLMs, involving sophisticated filtering\\nlike removing duplicates or irrelevant content, often with another LLM’s help. For example, the\\nTinyStories corpus [98] is tailored for simplicity, ideal for training models to handle straightforward\\nnarratives. RedPajama-V2 [70] uses the CCNet pipeline to process 30B documents, providing quality\\nsignals and IDs for creating a 20B deduplicated dataset. (ii) Code Data: Source code constitutes a\\nsignificant component of valuable data for training models, particularly because of its structured\\nnature and logical content. Training on code data enhances a model’s reasoning capabilities and\\nsupports its ability to generalize across multiple natural languages, which is crucial for applications\\nrequiring robust problem-solving and interpretation skills in diverse coding environments [18, 106,\\n122, 241].\\n5.1.3\\nTraining Algorithms. To enhance the alignment of SLMs with desirable properties such as\\nsafety and reasoning, training algorithms, particularly during the fine-tuning phase, are crucial in\\nevolving pre-trained SLMs.\\n—DPO [290] presents a simpler alternative to RLHF for optimizing language models based on\\nhuman preferences, preventing explicit reward modeling and reinforcement learning. Instead,\\nDPO modifies log probabilities of responses with a dynamic weighting mechanism to prevent\\nmodel degradation common in probability ratio-focused methods. The DPO loss function is:\\nL퐷푃푂(휋휃; 휋ref) = −E(푥,푦푤,푦푙)∼퐷\\n\\x14\\nlog휎\\n\\x12\\n훽log 휋휃(푦푤|푥)\\n휋ref(푦푤|푥) −훽log 휋휃(푦푙|푥)\\n휋ref(푦푙|푥)\\n\\x13\\x15\\n,\\nwhere 휋휃is the policy being optimized, 휋ref is the reference policy, 퐷includes tuples (푥,푦푤,푦푙),\\n휎is the sigmoid function, and 훽scales the log-ratios between 휋휃and 휋ref, guiding the model\\ntowards human-preferred outputs.\\n1Available at https://commoncrawl.org.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 42}, page_content='145:42\\nF. Wang et al.\\n—Reinforcement Learning from Contrast Distillation (RLCD) [410] aims to calibrate gen-\\nerative SLMs/LLMs towards embodying harmless and beneficial characteristics. The process\\nstarts with an unaligned LM and initial prompts, which are modified into two variants,\\n푝+ and 푝−, intended to promote and suppress, respectively, attributes like helpfulness and\\nharmlessness. Upon inputting these prompts, the LM generates outputs 표+ and 표−, with 표+\\nautomatically designated as the preferred response. This automation speeds up training by\\navoiding additional evaluative scoring. The training continues under the RLHF framework.\\n—Conditioned Reinforcement Learning Fine-Tuning (C-RLFT), by OpenChat [362], en-\\nhances model performance by incorporating low-quality data during SFT. C-RLFT leverages\\nvaried data qualities with simple rewards (e.g., expert data at 1 credit, sub-optimal at 0.1),\\nusing distinct prompt tokens to condition data sources, eliminating costly human feedback.\\nSimilarly, Data Mix [281] trains on English text in three stages, reducing noisy web data\\nprogressively in each stage in favor of higher-quality data.\\n—Explanation Tuning, proposed by Orca [258], addresses the limitations of standard instruction-\\nbased fine-tuning, which often restricts SLMs to style imitation rather than reasoning. It uses\\nsystem prompts with instructions to direct GPT-4 to produce detailed explanations or perform\\nstep-by-step reasoning. The resulting instructions and the responses are used as a dataset for\\nfine-tuning SLMs to have a better ability to reason.\\n—Prompt Erasing, introduced by Orac 2 [253], is a distillation strategy designed to enhance\\nthe independent reasoning capabilities of student SLMs. In this approach, a more capable\\nteacher LLM is given intricate prompts intended to elicit specific strategic behaviors and more\\nprecise outcomes. During the training phase, the SLM is exposed only to the task instruction\\nand the resulting behavior, without access to the original intricate prompts that initiate such\\nresponses. This technique, known as Prompt Erasing, positions the student model as a cautious\\nreasoner because it not only learns to perform specific reasoning steps but also develops\\nstrategies for approaching tasks at a higher level.\\n—Progressive Learning, proposed by Orca [258], aims to bridge the capability gap between Orca\\nand the more capable GPT-4. It starts with training on five million data points from ChatGPT,\\nfollowed by one million from GPT-4. Research suggests that an intermediate-level teacher can\\nimprove distillation effects, enabling a stepwise learning approach where students start with\\nsimpler examples and gradually move to more complex ones, receiving improved reasoning\\nand explanations from a more advanced teacher.\\n—Maximal Update Parameterization (휇P) optimizes control initialization, layer-wise learning\\nrates, and activation magnitudes to ensure stable training regardless of model layer widths.\\nThis method enhances training stability and allows the same optimizer settings, especially\\nlearning rates, to be used across different model scales. For instance, Cerebras-GPT [321]\\nemploys 휇P to train its models efficiently.\\n5.1.4\\nModel Performance. To compare the performance of SLMs, we have extracted experimental\\nresults from two recent and concurrent studies published in June 2024, OLMo [118] and MobiLlama\\n[342], and the recently proposed edge-device Llama 3.2 1B and 3B in September 2024.2 The extracted\\nresults are merged and shown in Table 9. From the table, we can find that the following evaluation\\nbenchmarks are commonly used:\\n(1) MMLU [137]: Evaluate broad knowledge across diverse fields such as humanities, science,\\ntechnology, engineering, and management. It includes multiple-choice questions covering\\n2https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 43}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:43\\n57 tasks ranging from elementary mathematics to US history, computer science, law, and\\nbeyond, with a total of 14K test samples.\\n(2) HellaSwag [434]: Assesses the model’s ability to select the correct ending to scenarios from\\nmultiple options, testing common sense reasoning, including 10K test samples.\\n(3) ARC [68]: The AI2’s Reasoning Challenge (ARC) dataset features multiple-choice science\\nexam questions for grades 3 to 9, divided into Easy and Challenge partitions, with the latter\\ncontaining more complex questions necessitating reasoning. Most questions offer four answer\\nchoices. ARC includes a supporting knowledge base of 14.3M unstructured text passages,\\nwith 1.17K test samples in ARC_Challenge and 2.25K in ARC_Easy.\\n(4) PIQA [33]: A commonsense reasoning dataset designed to evaluate the physical knowledge\\nof NLP models. It presents questions (goals) that require physical commonsense for correct\\nresolution, alongside two detailed response options (sol1 and sol2). The dataset comprises\\n3,000 test samples.\\n(5) Winogrande [301]: a dataset structured as a fill-in-the-blank task with binary options, designed\\nto assess commonsense reasoning. The dataset includes 1,767 test samples by default splits.\\nAccuracy is used as the evaluation metric in the table. Open Language Model (OLMo) [118]\\nis publicly available with its training data and code.3 MobiLlama [342] is a general-purpose SLM\\ndesigned from scratch, available in 0.5B and 0.8B versions. It adopts a unique approach by using\\na shared FFN across all transformer blocks, enhancing efficiency. MobiLlama also shows high\\nefficiency on diverse hardware (Table 10).\\nFrom Tables 9 and 10, we can conclude that: (1) MobiLlama 0.5B and 0.8B demonstrate that a\\nshared FFN design can facilitate excellent performance in SLMs with fewer than 1B parameters,\\neven rivaling some models in the 1B-3B range. (2) The performance of MobiLlama 1.2B and OLMo\\n1.2B illustrates that advanced SLM architectures incorporating high-quality data, SwiGLU, non-\\nparametric layer normalization, RoPE, BPE tokenization, and a shared FFN design can achieve\\ncompetitive results among models with 1B-3B parameters. (3) Popular techniques such as pruning,\\nquantization, distillation, SFT, and DPO, utilized in Llama 3.2, have substantially enhanced SLM\\nperformance. (4) MobiLlama demonstrates that SLMs can significantly reduce resource consumption\\non low-end hardware devices, achieving comparable performance while using a smaller proportion\\nof battery power and memory.\\nInsights: We draw several key insights from the development of generic-domain SLMs:\\n—Typical SLM architectures generally incorporate features such as GQA, gated FFN with SiLU activations,\\nRMS normalization, deep and thin architectures, embedding sharing, layer sharing, and shared FFNs.\\n—Although these components are widely used, current research has not yet thoroughly explored their\\nspecific contributions within SLMs.\\n—The importance of data quality in SLM research is increasingly emphasized, often considered more critical\\nthan the quantity of data and model architectural configurations.\\n—Post-pretraining, meticulous fine-tuning is often required to enhance the safety of SLMs, involving strate-\\ngies to distill capabilities from LLMs better. Common strategies include explanatory tuning, progressive\\nlearning, and prompt erasing.\\n5.2\\nDomain-Specific SLMs\\nOverview. The capability of LLMs to generate human-like text has significantly captured pub-\\nlic interest, highlighting their potential in the field of general artificial intelligence. However,\\n3https://allenai.org/olmo.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 44}, page_content='145:44\\nF. Wang et al.\\ninefficiencies persist when integrating LLMs into specialized applications due to resource con-\\nstraints. Unlike the need for extensive general knowledge and capabilities, domain-specific SLMs\\nshould focus on well-defined tasks and expertise pertinent to specific fields. For instance, specialized\\nmodels can significantly impact biomedical research and healthcare by fine-tuning for interpretable\\nmental health analysis or assisting humans in legal dialogues and financial tasks through instruction\\ntuning, showcasing their potential transformative influence. Given the limited number of SLMs\\nspecialized in specific domains, we demonstrate some existing SLMs individually across healthcare,\\nscience, finance, and law domains.\\n5.2.1\\nSLMs for Healthcare. Hippocrates [3] is an open-source medical language model framework\\nwith free access to its data, codebase, checkpoints, and protocols.4 It utilizes a medical pre-training\\ncorpus from Medical Guidelines, PMC-Patients [456], and PubMedQA-contexts [166], totaling about\\n300M tokens. The Hippo series, a 7B model, undergoes continuous pre-training, instruction tuning,\\nand RLHF. Fine-tuned on Mistral and Llama-2, it rivals 70B models in some evaluations. For example,\\nHippo-Mistral 7B scores 59.9% on MedQA, outperforming Meditron 70B [57] at 58.5%. BioMedLM\\n[35], a 2.7B GPT-style model trained on PubMed content [109], excels in biomedical QA after fine-\\ntuning, achieving 57.3% on MedMCQA (dev) and 69.0% on MMLU medical genetics exams. Available\\non Hugging Face Hub.5 AdaLM [419] enhances domain-specific SLMs by continuing training on a\\nmedical-focused SLM atop a general pre-trained model. It empirically validates that adaptation-\\nthen-distillation is the most effective way. AdaLM modified a BERT_base model (12 layers, 768\\nhidden size) [86] with a 16\\xa0GB PubMed6 abstracts corpus. MentalLLaMA [411] introduces the first\\nIMHI dataset for mental health analysis and the first open-source LM for explainable analysis\\non social media. The IMHI is compiled from ten sources, totaling 105K samples. Expert-designed\\nmental health analysis prompts are employed via ChatGPT for explanations. Based on Llama-2-7B,\\nMentalLLaMA is instruction-tuned on this data and matches top methods in accuracy on the IMHI\\ntest set. Project code is available at https://github.com/SteveKGYang/MentaLLaMA.\\n5.2.2\\nSLMs for Science. SciGLM [440] is a collegiate-level scientific language model overcom-\\ning data scarcity with a self-reflective instruction annotation framework. Utilizing GPT-4 [2], it\\ngenerates detailed reasoning for unlabeled scientific problems through three steps with designed\\nprompts in Table 12: (i) CoT prompt for step-by-step answers (Prompt 1), (ii) reflective prompt\\nfor correcting errors (Prompt 2), and (iii) integrating the correct answer for clarity (Prompt 3).\\nThe SciInstruct dataset spans physics, chemistry, math, and proofs, tuning ChatGLM-6B’s [95]\\nreasoning abilities. SciGLM boosts the base model’s (ChatGLM3-6B-Base) scientific QA accuracy\\nby 3.06% on benchmarks such as CEval-Hard [152], CEval-Sci [152], MMLU-Sci [137], SciEval\\n[325], and SciBench [369]. Llemma [23], an SLM derived from CodeLlama [299], specializes in\\nmathematical reasoning. By continual pre-training, its 7B model is evolved on 55B tokens from\\nthe newly created Proof-Pile-2 dataset, which includes scientific papers, math web content, and\\nmathematical code up until April 2023, to enhance few-shot capabilities. It excels in mathematical\\nbenchmarks like MATH [137], GSM8k [69], OCWCourses [189], MMLU-STEM [137], and SAT,\\nsurpassing all comparable open-weight models. ChemLLM [441] is a chemistry-focused language\\nmodel that utilizes its proposed ChemData, a dataset designed for instruction tuning that transforms\\nchemical data into dialogue format for training. ChemLLM is based on InternLM2-Base-7B [41],\\ninitially enhancing its language skills with a multi-corpus of 1.7 million Q&A pairs from Hugging\\nFace, then fine-tuning using ChemData and the multi-corpus to maintain its general capabilities.\\n4https://cyberiada.github.io/Hippocrates/.\\n5https://huggingface.co/stanford-crfm/BioMedLM.\\n6https://pubmed.ncbi.nlm.nih.gov/.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 45}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:45\\nTable 12. Prompts for Self-Reflective Instruction Annotation Framework\\nCoT\\n[Prompt 1] The following input consists of a science problem. Please generate an elaborate step-by-step solution\\nto the problem.\\nReflective Generation\\n[Prompt 2] The following input comprises a science problem and a corresponding solution. However, this\\nsolution is incorrect. Please reflect on its errors and then generate a correct step-by-step solution to the problem.\\nPrompt Answer\\n[Prompt 3] The following input consists of a science problem, a corresponding solution, and the real answer.\\nThe given solution is incorrect. Please reflect on its errors and then generate a correct step-by-step solution to\\nthe problem based on the real answer.\\nChemLLM excels in interdisciplinary chemical tasks within the proposed ChemBench, achieving\\nresults comparable to GPT-4 [2] and outperforming GPT-3.5 with a score of 92.6 in Mol2caption,\\nslightly below that of GPT-4. AstroLLaMA [265] introduces an astronomy-focused language model.\\nBased on Llama-2-7B [345] and enhanced via continual pre-training, it has been developed using\\nover 300\\u2009K astronomy abstracts from arXiv.7 AstroLLaMA achieves 30% lower perplexity than\\nLlama-2-7B, indicating substantial improvements in domain adaptability. AstroLLaMA is avail-\\nable8 for tasks such as automated paper summarization and conversational agent development in\\nastronomy.\\n5.2.3\\nSLMs for Finance and Law. MindLLM [417] introduces a bilingual (Chinese and English)\\nSLM, pretrained on the Pile dataset [109] for English and WuDao [429], CBook, and various\\nChinese web content for Chinese. Bilingual training enhances capacity and prevents catastrophic\\nforgetting. It explores specific domains such as law and finance through SFT. In law, it utilizes\\npublicly available legal data, scenario-based Q&A from LaW-GPT [142], and NLP-based legal tasks\\nfrom DISC-LawLLM [432]. In finance, EastMoney9 is selected as the data source.\\nInsights: We draw several key insights from the development of domain-specific SLMs:\\n—Adapting SLMs to domain-specific data is a common practice for acquiring domain-specific SLMs,\\nprompting many to create their datasets [265, 411, 440, 441]. These datasets are often annotated\\nusing LLMs like GPT-4 and used to continual pre-train or fine-tune general models such as LLaMa-\\n2-7B [3, 35]. To ensure the data quality, specialized annotation frameworks are developed, such as\\nSciGLM [440].\\n—In domains with abundant corpora, training a general model from scratch and fine-tuning it using\\nSFT [417] is practical. Bilingual settings during training can prevent catastrophic forgetting.\\n—Distilling general capabilities from LLMs while integrating domain-specific knowledge from corpora\\nis another method for developing domain-specific SLMs [419].\\n6\\nSLMs for LLMs\\nIn this section, we provide a comprehensive review of how SLMs enhance LLMs. While LLMs are\\nrobust, they face challenges such as latency during inference, labor-intensive fine-tuning, noise\\nfiltration issues in retrieval, suboptimal zero-shot performance, copyright infringement risks, and\\nevaluation difficulties. SLMs can help LLMs to alleviate these issues. Research in this field can\\nbe categorized into five primary areas: (i) using SLMs for reliable LLM generation; (ii) extracting\\nprompts for LLMs using SLMs; (iii) fine-tuning LLMs with SLMs; (iv) applying SLMs in LLM\\napplications; (v) utilizing SLMs as guardians; and (vi) evaluating LLMs using SLMs. A summary\\n7https://www.kaggle.com/Cornell-University/arxiv.\\n8https://huggingface.co/universeTBD/astrollama.\\n9https://www.eastmoney.com/default.html.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 46}, page_content='145:46\\nF. Wang et al.\\nTable 13. SLMs Help LLMs in Different Aspects\\nAspect\\nRepresentative Work\\nKey Point\\nSLM for reliable\\nLLM generations\\nAPRICOT [349]\\nTrains a small auxiliary model to predict LLM’s confidence using only\\ntextual inputs and outputs.\\nPOLAR [454]\\nUsing a BERT model to calibrate LLM responses.\\nHallucination Detector in NMT\\n[404]\\nUsing lightweight classifiers to detect hallucinations in Neural Machine\\nTranslation.\\nSAPLMA [22]\\nUsing a BERT SLM as a classifier to assess the truthfulness of statements\\naccurately.\\nQuestion Decomposer [388]\\nDistilled SLM decomposes complex questions to aid reasoning.\\nSuperICL [398]\\nSLM Plug-ins provide confidence and prediction for contextual exem-\\nplars to aid in-context learning.\\nSuperContext [413]\\nSpecific SLM enhances ICL by providing confidence and predictions to\\novercome out-of-domain challenges.\\nSelf-RAG [19]\\nA proxy model labels special tokens during RAG data generation for\\nfine-tuning.\\nSKR [371]\\nTraining a small model to detect its self-knowledge for better use of\\nexternal knowledge.\\nSlimPLM [331]\\nDetecting missing knowledge in LLMs with a slim proxy model, en-\\nhancing the LLM’s knowledge integration.\\nIn-Context RALM [294]\\nTraining a RoBERTa-based reranker for top-k BM25 documents using\\nLM signals to enhance LM gains.\\nCRAG [406]\\nTraining a lightweight evaluator to assess document quality and trigger\\nactions based on confidence levels.\\nGSR [151]\\nTraining a Generative Sub-graph Retriever (GSR) for relation chain in\\nRAG when retrieving from knowledge graphs.\\nSLM for\\nextracting LLM\\nprompts\\nPrompt Extraction [448]\\nSmall model trained to predict confidence of extracted system prompt\\nfrom adversarial prompts.\\nPrompt Stealing Attacks [304]\\nUsing small models fine-tuned as parameter extractors to facilitate\\nhierarchical prompt reconstruction.\\nOutput2prompt [438]\\nUsing a sparse encoder-decoder-based T5 small model to reverse-\\nengineer LLM inputs from outputs.\\nModel Purifying [201]\\nUsing SLMs to ensemble with LLMs, mitigating negative effects from\\nuncurated data.\\nSLM for\\nFine-tuning\\nLLMs\\nLP [247]\\nLearning Percentage as a difficulty metric.\\nEmulated Fine-tuning [252]\\nEmulating pre-training and fine-tuning at different scales by summing\\nbase log probabilities with behavior deltas.\\nCROSSLM [82]\\nSLMs enhance LLMs by generating task-specific high-quality data.\\nWeak-to-Strong Search [457]\\nFraming LLM alignment as a test-time greedy search to maximize the\\nlog-probability difference between tuned and untuned SLMs.\\nSLM for LLM\\napplications\\nSLCoLM [333]\\nUsing SLM predictions to guide the LLM generation process in Chinese\\nEntity Relation Extraction.\\nHEF [418]\\nUsing SLMs as plugins to improve LLM’s nuanced understanding.\\nContrastive decoding [202]\\nEnhancing text quality by maximizing the difference between expert\\nand amateur log probabilities.\\nSLM for LLM\\nsafety\\nLlama Guard [156]\\nAn LLM-based input–output safeguard model geared towards Hu-\\nman–AI conversation use cases.\\nSLM as Guardian [180]\\nA smaller LLM for both harmful query detection and safeguard response\\ngeneration.\\nSLM for LLM\\nevaluation\\nSLIDE [453]\\nUtilizing SLMs trained via contrast learning to distinguish and score\\nresponses in dialogue scenarios effectively.\\nKuhn et al. [178]\\nAn SLM is used as the natural language inference classifier.\\nSelfCheckGPT [245]\\nAn SLM is used to calculate BERTScore.\\nFactscore [250]\\nAn SLM functions as the natural language inference classifier.\\nof representative work in each category along with their key point is given in Table 13. Next, we\\nintroduce each category in detail.\\n6.1\\nSLM for Reliable LLM Generation\\nAlthough LLMs generally produce fluent and convincing text, they can occasionally generate\\nerroneous responses [162, 355]. Additionally, LLMs are susceptible to privacy breaches from\\nuntrusted data collection, which can erode user trust or cause harm. To address these issues, recent\\nstudies have focused on using SLMs to calibrate LLM confidence, detect hallucinations, and improve\\nretrieval-augmented LLMs and their reasoning capabilities.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 47}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:47\\nFig. 20. Architectures of Enhancing Calibration and Hallucination Detection of LLMs.\\nEnhancing Calibration and Hallucination Detection of LLMs. As illustrated in Figure 20(a), to\\ncalibrate LLMs, an SLM processes both questions and LLM-generated answers to predict calibrated\\nconfidence. This training involves minimizing the discrepancy between estimated calibration\\nerror and predicted confidence score. For instance, APRICOT [349] uses an auxiliary DeBERTaV3\\nmodel [133] to assess LLM confidence in open-question scenarios, aiming to improve uncertainty\\nexpression and response adjustment. Similarly, POLAR [454] has developed a self-supervised\\napproach that generates risk scores for each response to calibrate LLM confidence, utilizing a small\\nBERT model [86] to synchronize LLM outputs with other weak supervision sources. As shown\\nin Figure 20(b), for hallucination detection, an SLM analyzes LLM internal states to output the\\nlikelihood of hallucination. This process uses supervised data obtained by testing the knowledge\\nboundaries of the LLM. In neural machine translation, Xu et al. [404] develop a lightweight detector\\nthat analyzes token contributions to hallucinations, outperforming both model-free baselines and\\nquality estimation classifiers. Furthermore, SAPLMA [22] found that LLM internal states can signal\\nthe truthfulness of statements, with a small BERT classifier trained to differentiate correct from\\nincorrect predictions, achieving accuracies of 71–83%.\\nEnhancing RAG. Generally, as shown in Figure 21, SLMs can also serve as proxy models to\\nevaluate the familiarity of LLMs with user queries, determining whether LLMs need to retrieve\\nadditional information or can respond directly. For example, SlimPLM [331] is a small proxy\\nFig. 21. Architecture of SLM as a Heuristic RAG prober.\\nmodel that assesses the necessity for LLM re-\\ntrieval by generating heuristic answers. High-\\nquality responses indicate that LLMs can han-\\ndle queries independently, whereas lower-\\nquality outputs require further retrieval. Ad-\\nditionally, Self-Knowledge Guided Retrieval\\n(SKR) [371] enables SLMs to autonomously de-\\ncide when LLMs should operate independently,\\nbased on their self-assessment of knowledge\\nlimitations. Further, SELF-RAG [19] improves the factual accuracy and quality of LLM outputs\\nthrough on-demand retrieval and self-reflection. This method employs a small critical language\\nmodel to issue reflective markers and make binary decisions regarding the need for further infor-\\nmation retrieval. Moreover, some studies utilize SLMs to evaluate the relevance of retrieved documents.\\nLongLLMLingua [165] employs SLMs to calculate the relevance of documents to a query 푥푞푢푒using\\nperplexity, as formalized by the equation:\\n푟푘= −1\\n푁푐\\nÕ\\n푖\\nlog푝SLM(푥푞푢푒\\n푖\\n|푥푑표푐\\n푘\\n),\\n푘∈{1, 2, . . . , 퐾},\\n(2)\\nwhere 푥푞푢푒\\n푖\\nis the 푖th token in the query sequence, 푥푑표푐\\n푘\\nis the retrieved document, and 푁푐is the\\ntotal number of tokens in the query. 푝SLM represents the probability generated by an SLM. CRAG\\n[406] employs SLMs as evaluators of document relevance in the same way. RA-ISF [224] trains a\\nSLM that checks the base LLM in self-knowledge, relevance judgment, and question decomposition.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 48}, page_content='145:48\\nF. Wang et al.\\nIn addition, some research employs SLMs as re-rankers to refine the order of documents provided\\nby initial retrieval efforts such as BM25 [298]. In-Context RALM [294] positions SLMs as rankers,\\noptimizing the document sequence with a fine-tuning process on RoBERTa [223] as defined by the\\nloss function:\\nmin\\n푟푎푛푘푒푟\\n푘\\nÕ\\n푖=1\\n−log푝rank(푑푖|푥≤푠푗) · 푝휃(푦|푑푖;푥≤푠푗),\\n(3)\\nwhere 푥≤푠푖is a prefix sampled from the training data, 푦= 푥푠푖+1, . . . ,푥푠푖+푠represents the text to\\nbe generated in the next stride, 푝휃(푦|푑푖;푥≤푠푖) denotes the probability of the LLM generating 푦\\ngiven 푑푖and 푥≤푠푖, and 푝rank(푑푖|푥≤푠푗) is the ranking score of 푑푖. Lastly, some studies leverage SLMs to\\nretrieve subgraphs when utilizing knowledge graphs as external sources. Huang et al. [151] introduce\\nthe Generative Sub-graph Retriever (GSR), which employs SLMs to predict relation chains\\nfor answering questions, offering a cost-effective alternative to training LLMs. Specifically, it\\nuses customized T5 (220M, 770M, and 3B) [291] as retrievers to enhance LLM readers, including\\nLlama2-chat-7B [345] and Llama3-instruct-8B [96], on the WebQSP [423] and CWQ [330] datasets.\\nFig. 22. SLM transfers knowledge into ICL.\\nEnhancing Reasoning Capabilities of\\nLLMs. As illustrated in Figure 22, SLMs\\nenhance LLMs’ reasoning by transferring\\ntask knowledge to in-context examples, ef-\\nfectively reducing hallucinations. While\\nIn-context Learning (ICL) generally han-\\ndles few-shot learning with 16 to 32 ex-\\namples, it struggles when faced with extensive supervised data. SLMs, specialized in task-specific\\ntraining, complement the broader domain knowledge of extensively pre-trained LLMs. For example,\\nSuperICL [398] incorporates SLMs as plugins for efficiently executing supervised tasks. It predicts\\nlabels for contextual examples and integrates these predictions with the input text and actual\\nlabels to enhance knowledge transfer, thereby boosting the understanding and responsiveness\\nof LLMs. SuperContext [413] tackles challenges that LLMs encounter with new tasks and out-of-\\ndistribution data in natural language understanding by synergizing SLM outputs with LLM prompts\\nduring inference. This integration merges model predictions with their confidence levels, effectively\\nleveraging SLM task-specific knowledge and LLM domain expertise. Furthermore, SLMs efficiently\\ndecompose complex reasoning by breaking tasks into simpler components, as demonstrated in [388].\\nThis strategy increases efficiency and reduces deployment costs when SLMs and LLMs are used\\ncollaboratively, transforming complex tasks into manageable segments.\\nFig. 23. Architecture of SLM-based data protection.\\nAlleviate Copyright and\\nPrivacy Issues of LLMs.\\nLLMs pose significant secu-\\nrity risks due to their ten-\\ndency to memorize train-\\ning data, leading to poten-\\ntial privacy breaches and\\ncopyright infringement. As\\ndepicted in Figure 23, SLMs\\ncan assist LLMs in address-\\ning copyright and privacy\\nconcerns arising from online data collection. By training on selectively curated data subsets, SLMs\\neffectively reduce copyright infringement and privacy risks, although they are less effective than\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 49}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:49\\nFig. 24. SLM for LLM prompt extraction paradigm. 푀푆denotes SLMs, and 푀퐿denotes LLMs. (a) SLM-based\\nprompt estimation tries various attack prompts; 푀푆selects the most likely extracted one. (b) SLM-based\\nparameter extractor identifies the type of input prompt. (c) SLM-based model inversion uses 푀푆to invert the\\nLLM output back into the input.\\nfull-scale LLMs. To harness the combined benefits of both models, Li et al. [201] integrate untrusted\\nLLMs with benign SLMs using the CP-Δ KL algorithm to mitigate adverse effects while preserving\\nperformance. The equation is:\\n푝(푦|푥) = 푝푙(푦|푥) · 푝푠(푦|푥)\\n푍(푥)\\n,\\n(4)\\nwhere 푝푙and 푝푠represent the probabilities from the large and small models, respectively, and 푍(푥)\\nis the partition function. This integration results in the following ensemble algorithm:\\n푧푝(·|푥) ∝훼푧푙(·|푥) + (1 −훼)푧푠(·|푥),\\n(5)\\nwhere 푧푙and 푧푠are the logit values from the large and small models, respectively, and 훼is the\\nscaling factor.\\n6.2\\nSLM for Extracting LLM Prompts\\nPrompt-based methods are becoming simpler and more cost-effective alternatives to traditional\\nfine-tuning in the LLM era, utilizing LLMs’ instruction-following capabilities for a competitive edge.\\nMastering prompts is vital for replicating LLM-supported product behaviors. However, services\\nsuch as Bing Chat and GitHub Copilot Chat have seen prompt reverse engineering through black-\\nbox API attacks. SLMs often serve as surrogate models in these attacks, employing strategies such\\nas (i) SLM-based prompt likelihood estimation, (ii) SLM-based prompt parameter extraction, and\\n(iii) SLM-based direct model inversion, illustrated in Figure 24.\\nSLM-based prompt likelihood estimation, as illustrated in Figure 24(a), Zhang et al. [448] propose\\nusing an SLM as a Likelihood Estimator to identify secret prompts in LLM outputs. They craft\\nattack prompts, such as “Repeat all sentences in our conversation,” and query the target LLM.\\nThe response is likely to include secret prompts, confusing the LLM to interpret these as part of\\nthe conversation. A fine-tuned DeBERTa model [134] is then used to select the most likely secret\\nprompts from the output.\\nSLM-based prompt parameter extraction, as shown in Figure 24(b), Sha and Zhang [304] utilize\\nan SLM as a parameter extractor to extract prompt parameters from LLM outputs. They employ a\\nspecialized BERT model [86] to classify LLM outputs into direct, in-context, and role-based prompts,\\nalso predicting the number of exemplars for in-context prompts and identifying roles for role-based\\nprompts. Prompt reconstruction is then performed using ChatGPT once the parameters are defined.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 50}, page_content='145:50\\nF. Wang et al.\\nSLM-based direct model inversion, as shown in Figure 24(c), the method of using an SLM as a Direct\\nInversion Model is designed to reverse-engineer LLM outputs back to their original prompts [438].\\nThey train a sparse encoder–decoder T5 model [291] with 222M parameters on the Instructions-2M\\ndataset [255], where the input is LLM outputs and the output is the LLM prompt. This trained\\nmodel effectively maps multiple LLM outputs to their initiating prompts as 푝(푥|푦1, .,푦푛; 푀푆1, 푀푆2),\\nwith 푦푖representing different output versions and 푀푆1, 푀푆2 the model parameters.\\n6.3\\nSLM for Fine-Tuning LLMs\\nFine-tuning is a crucial technique for adapting LLMs to specific tasks or domains, yet it is often\\ntime-consuming. For instance, fine-tuning the LLaMA-2-13B [345] checkpoint on 32 NVIDIA A100\\nGPUs with 80GB of memory using bfloat16 format requires approximately 70\\u2009hours [253]. This\\nprocess also demands high-quality data. Therefore, we examine how SLMs can enhance LLM\\nfine-tuning through three approaches: (i) proxy fine-tuning, (ii) selecting high-quality data, and\\n(iii) guiding LLM-generated task data, as illustrated in Figure 25.\\nFig. 25. SLM for LLM fine-tuning.\\nSLMs as Proxy Models.\\nSLMs can approximate the\\ngradient of fine-tuning large-\\nscale LLMs on target datasets,\\navoiding the costly fine-\\ntuning process in terms of\\ntime and computational re-\\nsources. As shown in Fig-\\nure 25(a), Emulated Fine-\\nTuning (EFT) [252] simu-\\nlates both unsupervised pre-\\ntraining and supervised fine-\\ntuning stages across different\\nscales by manipulating log\\nprobabilities. EFT, for exam-\\nple, combines base log prob-\\nabilities from a 70B model\\nwith behavioral deltas from a 7B model—these deltas represent differences between fine-tuned and\\nunfine-tuned SLMs, effectively emulating outcomes for the Llama-2 series. This method allows\\nfine-tuning on smaller models such as Falcon-7B [11] while capturing most benefits of fine-tuning\\nlarger models such as Falcon-180B, benefiting applications such as dialogue, QA, and code genera-\\ntion. Similarly, Proxy-tuning [217] adjusts LLM predictions by adding the differences between the\\noutputs of a fine-tuned small model and its untuned version to the LLM’s output vocabulary during\\ndecoding, maintaining the advantages of large-scale pre-training while integrating small-scale\\nfine-tuning benefits. Moreover, SLMs can act as proxies for approximate LLM fine-tuning during\\ndecoding. The Weak-to-Strong Search [457] strategy frames the alignment of LLMs as a test-time\\ngreedy search, aiming to maximize the log-probability difference between small tuned and un-\\ntuned models while sampling from the frozen large model. This approach serves as a dual-purpose\\nmethod: (1) a compute-efficient model upscaling strategy that circumvents direct tuning of the\\nlarge model and (2) an instance of weak-to-strong generalization that bolsters a strong model with\\nweak test-time guidance.\\nSLMs Play a Role in Selecting High-Quality Fine-Tuning Data for LLMs. Figure 25(b) illustrates\\nhow SLMs within the same family as the LLM can identify training samples that are likely to\\nbe challenging, enhancing the training efficiency and generalization capability of the LLM. As\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 51}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:51\\ndemonstrated by Swayamdipta et al. [329] and further advanced by Mekala et al. [247], the learning\\npercentage 퐿푃(푖) is a metric used to curate high-quality datasets with hard samples: 퐿푃(푖) = 푃푖−1−푃푖\\n푃0−푃푛,\\nwhere 푃푖represents the perplexity at the end of epoch-푖, and 푃0 is the initial perplexity. A higher\\n퐿푃(푖) early in training indicates significant learning in the initial epochs, highlighting the potential\\nof these samples to enhance LLMs. SmallToLarge (S2L) [416] utilizes training loss trajectories\\nfrom smaller models to guide data selection for larger models’ fine-tuning. Experimental results\\ndemonstrate that S2L significantly enhances data efficiency in SFT for mathematical problem-\\nsolving, reducing the required training data to just 11% of the original MathInstruct dataset [433]\\nto achieve performance comparable to that obtained using the full dataset.\\nSLMs Enhance the Quality of LLM-generated Data for Specific Tasks. As depicted in Figure 25(c),\\nCROSSLM [82] promotes the local training of SLMs on client-specific private data to mitigate\\nprivacy risks associated with server-based LLMs. An SLM trained in this manner can guide the\\nserver-side LLM in producing high-quality synthetic datasets. Feedback from SLMs regarding the\\nquality of this synthetic data serves as a supervisory signal, enhancing both the quality of LLM\\noutputs and the utility of the data for further training.\\n6.4\\nSLM for LLM Applications\\nLLMs are utilized across various applications due to their open-ended generation capabilities, yet\\nthey often lack specialized knowledge and have other generation issues. SLMs can supplement this\\nby providing task-specific knowledge or reflecting weaknesses. Therefore, we explore how SLMs\\nenhance the performance of LLMs in specific applications, focusing on open-ended generation,\\nknowledge integration, relation extraction, and empathetic response.\\nFig. 26. Contrastive decoding [202].\\nIn open-ended text generation—such as writ-\\ning assistance and story creation—LLMs of-\\nten suffer from issues such as incoherence and\\nthematic drift over extended sequences. Due\\nto more frequent failure patterns observed in\\nSLMs, such as short, repeated, and irrelevant\\nstrings, these patterns serve as negative exam-\\nples for LLM decoding. Contrastive Decod-\\ning (CD) [202] improves coherence and lexical\\ndiversity by leveraging the differential capa-\\nbilities between a large model, OPT-13B [445],\\nand a smaller model, OPT-125M. As illustrated\\nin Figure 26, CD improves content quality by\\nsampling generation based on the difference in\\nlog probabilities, log푝퐸푋푃−log푝퐴푀퐴, between\\nan expert LM and an amateur LM, rather than\\nrelying solely on the expert LM’s log probability. This approach effectively reduces generative\\nfailures, including repetition.\\nIn knowledge injection, general LLMs may lack domain-specific expertise for specialized tasks like\\nlaw or medicine. Domain-specific SLMs can supply crucial knowledge in a format suitable for LLMs.\\nTo this end, BLADE [192] integrates black-box LLMs with small domain-specific models. BLADE\\ncombines the comprehensive language capabilities of LLMs with the specialized knowledge of small\\nLMs. As shown in Figure 27, BLADE’s process includes: (1) pre-training SLMs on domain-specific\\ndata, (2) fine-tuning with knowledge instruction to meet task-specific needs, and (3) using joint\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 52}, page_content='145:52\\nF. Wang et al.\\nBayesian optimization to enhance synergy between the LLM and the small LM, boosting overall\\nperformance.\\nFig. 27. BLADE framework [192].\\nIn relation extraction, a field limited\\nby scarce labeled data and prevalent\\nlong-tail categories, the “Train-Guide-\\nPredict”\\nframework [333] employs\\nSLMs to learn task-specific knowledge\\nfor dominant categories. SLMs strug-\\ngle with rare categories, whereas LLMs\\nmanage these effectively due to their\\nextensive pre-trained text. Therefore,\\nthis framework leverages the strengths\\nof both models: it utilizes SLMs to acquire task knowledge and guide the LLM’s generative process\\nwith initial SLM predictions, enhancing the LLM’s handling of underrepresented categories.\\nIn generating empathetic responses, LLMs excel in expressiveness but struggle with nuanced\\nemotions and cognition. HEF [418] addresses this by incorporating Small Empathy Models (SEMs)\\nto enhance LLMs’ emotional and cognitive depth. This framework employs a two-tiered emotion\\nprediction method: SEMs identify primary emotions, directing LLMs to concentrate on these\\nemotions and their triggers, resulting in more accurate and empathetic responses.\\n6.5\\nSLM for LLM Safety\\nAs demonstrated by various works [256, 309, 431, 464], LLMs are vulnerable to adversarial attacks\\nand jailbreaking. For example, Wang et al. [364] show that ChatGPT’s performance on adversarial\\ndatasets is still far from perfect, indicating that potential risks of adversarial vulnerability remain.\\nAnother example includes jailbreaking ChatGPT by asking it to “pretend to be a sarcastic mean girl.”\\nUsing such techniques, it has been shown that even the most advanced LLMs are far from being\\nsafe against generating potentially harmful content. Hence, the widely adopted LLM-based services\\nto generate are at high risk of being misused for nefarious purposes. Consequently, resources such\\nas the Llama 2 Responsible Use Guide10 strongly advocate for implementing robust guardrails in\\nproducts that utilize Generative AI. These guardrails are specifically designed to mitigate risks\\nassociated with both inputs to and outputs from the model, ensuring safeguards against the\\ngeneration of high-risk or policy-violating content, as well as protecting against adversarial inputs\\nand attempts to compromise the model. In addition to developing trustworthy LLMs, adopting\\nSLMs for LLM safety [156, 180] has also attracted increasing attention. For example, Llama Guard\\n[156], fine-tuned on Llama2-7B, has publicly released an input–output safeguard tool specifically for\\nclassifying safety risks in prompts and responses within conversational AI applications. However,\\nthis tool is limited to assessing the harmfulness of questions and answers and does not facilitate\\nthe generation of fluent, safe responses. In response to this limitation, Kwon et al. [180] fine-tune a\\nspecialized SLM with harmful query detection and safeguard answer generation tasks to accurately\\ndetect harmful user queries and generate appropriate safeguard explanations, thereby enhancing\\nthe safety measures in conversational AI.\\n6.6\\nSLM for LLM Evaluation\\nSLMs can also enhance the evaluation of LLMs. In dialog evaluation, generating dialog reference\\nresponses is computationally complex, making accurate assessment difficult due to the multiple\\nplausible but semantically different responses possible for a single dialog context. Relying on\\n10https://ai.meta.com/static-resource/responsible-use-guide/.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 53}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:53\\nLLM prompting for evaluation can lead to problems such as dependency on prompt wording and\\ninconsistent results. One solution involves training specialized SLMs to evaluate LLMs, as these\\nSLMs can be fine-tuned more quickly and generate outputs faster during inference, owing to\\ntheir reduced number of parameters. For example, SLIDE [453] employs contrastive learning to\\nfine-tune an SLM to effectively distinguish between positive and negative responses. Based on\\nits observation that SLMs are more accurate in identifying positive responses and LLMs excel at\\nclassifying negative ones, the trained SLM is subsequently integrated with an LLM to assign a score\\nto each response. The scoring method used is formalized as follows:\\n푠푐표푟푒=\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f3\\n푠푐표푟푒푆퐿푀,\\nif 푠푐표푟푒푆퐿푀≥0.5\\n푠푐표푟푒퐿퐿푀,\\nelif 푠푐표푟푒퐿퐿푀< 0.5\\n푠푐표푟푒푆퐿푀+푠푐표푟푒퐿퐿푀\\n2\\n,\\notherwise\\n.\\n(6)\\nThis equation allows for adaptive response evaluation, leveraging the strengths of both models to\\nensure a more reliable and consistent assessment across varying dialogue contexts. In the natural\\nlanguage generation task, Kuhn et al. [178] design a novel entropy to evaluate the uncertainty of\\nLLMs. It aims to tackle the challenge of semantic equivalence [178]. For instance, A’s son is B and B\\nis A’s son are semantically equivalent. It should not be considered uncertain if an LLM is unsure\\nabout which of the two previously mentioned sentences to generate due to semantic equivalence.\\nA\\xa0DeBERTa-Large [134] fine-tuned on the MNLI [383] dataset serves as the classifier guided by\\nsemantic equivalence in the clustering stage. SelfCheckGPT [245] proposes a black-box hallucination\\ndetection method for LLMs. The core idea is to leverage uncertainty derived from sampled outputs.\\nTo be specific, Manakul et al. [245] claim that an LLM trained on a concept generates responses that\\nare similar and factually consistent. One of the five variants of SelfCheckGPT uses BertScore to\\nachieve it. A DeBERTa-Large [223] is utilized to calculate the BERTScore. Factscore [250] is proposed\\nto evaluate the factuality of LM-generated long-form content. It divides the generated long content\\ninto multiple short texts, enabling a more precise assessment of factual accuracy. In addition to\\nmanual evaluation, Min et al. [250] also propose an automated evaluation framework to estimate\\nFactscore, which can reduce costs. LLaMa 7B [344], fine-tuned on Super-NaturalInstructions [373]\\nis one of the LMs employed as an evaluation assistant and shows promising performance. They\\nalso employ Generalizable T5-based dense retrievers [266] to facilitate passage retrieval.\\nInsights: SLMs can improve LLMs in various aspects, including enhancing the reliability of LLM\\ngeneration, extracting prompts, fine-tuning, application, and evaluation. This discussion seeks to\\nanswer when SLMs should be utilized to augment LLMs. We identify several suitable scenarios:\\n—Adapting LLMs to specific tasks can require substantial computational resources and time. In\\nsuch cases, a smaller model could be fine-tuned instead to serve functions such as hallucination\\ndetection.\\n—SLMs can outperform LLMs in certain aspects; hence combining SLMs with LLMs can create a\\nmore powerful model, e.g., SLMs typically have fewer security issues than LLMs, and integrating\\nboth can generate a model that is both powerful and secure.\\n—SLMs, despite their limitations, can alert LLMs to these issues, such as the tendency to produce\\nrepetitive vocabulary. Designing contrastive losses can help LLMs overcome these issues by learning\\nfrom the nuanced feedback of SLMs.\\n—The fast inference speed and certain characteristics of SLMs can emulate and thus enhance the\\nbehavior of LLMs, acting as effective proxies. For example, the training data selection for LLMs\\ncan be guided by the difficulty metrics assessed by SLMs, and the parameter adjustments during\\nthe fine-tuning of SLMs can also approximate the fine-tuning processes of LLMs.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 54}, page_content='145:54\\nF. Wang et al.\\nTable 14. Synergy between SLMs and LLMs\\nSynergy\\nRepresentative Work\\nKey Point\\nCloud-Edge\\nSynergy\\n(Inference)\\nCoGenesis [442]\\nDivide user instructions into general parts by LLMs and private parts\\nby SLMs.\\nXu et al. [402]\\nIntroduce split learning in 6\\u2009G to distribute LLM agents.\\nLLM-to-SLM [29]\\nEncode prompts with server-side LLM and decode with edge-side SLM.\\nSynergy of Thoughts [305]\\nSLMs suggest reasoning paths; LLMs correct contradictions.\\nHao et al. [129]\\nSLM generates local tokens; LLM checks and corrects complex tokens.\\nLLMCad [399]\\nCombine lightweight and high-precision LLMs for on-device inference.\\nKhattab et al. [170], Ma et al. [239]\\nFocus on LLM’s reasoning and SLM’s efficient decoding.\\nCloud-Edge\\nSynergy\\n(Training)\\nCROSSLM [82]\\n\\xa0\\n\\xa0\\nPreserve client data privacy by training SLM locally and LLM remotely;\\nmutual improvement using SLM-labeled data from LLM outputs.\\n\\xa0\\nTask-Centric\\nSynergy\\n훼-UMi [314]\\nBreak down a single LLM into specialized agents.\\nSynCID [208]\\nMerge LLM’s semantics with SLM’s speed; refine labels via contrastive\\nlearning.\\nFilter-then-rerank [239]\\nSLMs process simple samples and flag complex ones for LLM reranking.\\nData Shunt+ (DS+) [47]\\nProcess easy samples with SLMs and delegate hard samples to LLMs.\\n7\\nSynergy between SLMs and LLMs\\nThe synergy between SLMs and LLMs leverages the unique strengths of each to enhance overall\\nsystem performance and efficiency. SLMs, being lightweight and resource-efficient, are ideal for\\ndeployment on edge devices, enabling rapid responses and low latency for straightforward tasks.\\nLLMs, on the other hand, possess greater computational power and a deeper understanding of\\ncomplex language patterns, allowing them to handle more intricate and nuanced tasks. By integrat-\\ning SLMs and LLMs, systems can dynamically allocate tasks based on complexity, ensuring that\\nsimple queries are processed quickly on the edge while more demanding requests are escalated\\nto the cloud. This collaborative approach optimizes resource usage, reduces operational costs,\\nand maintains high-quality outputs across a diverse range of applications. The synergy between\\nSLMs and LLMs can be categorized into two parts: cloud-edge synergy and task-centric synergy.\\nCloud-edge synergy refers to a setup where SLMs operate on edge devices, while LLMs reside\\non the server. When the SLM is not powerful enough, the LLM compensates by handling more\\ncomplex tasks and providing additional support. Task-centric synergy refers to the scenario where\\nSLMs and LLMs leverage their respective strengths to improve task-oriented efficiency. Table 14\\nsummarizes representative work in each category and their key points. Next, we introduce each\\ncategory in detail.\\n7.1\\nCloud-Edge Synergy\\nThe current utilization of LLMs typically involves uploading private data to the cloud for response.\\nFine-tuning LLMs usually also requires uploading data to clouds for computing. However, this\\nraises privacy concerns, as the collection and usage of private data are constrained by personal\\nprivacy awareness and legal regulations [354]. Consequently, the cloud-edge synergy between\\nSLMs and LLMs is proposed to alleviate this issue, i.e., SLMs handle privacy-sensitive data locally,\\nLLMs handle de-identified or non-privacy-sensitive data, and these two models collaborate. This\\nsection discusses such cloud-edge synergy, dividing it into two categories: cloud-edge synergy\\nduring inference and cloud-edge synergy during training, as shown in Figure 28.\\nCloud-edge Synergy during Inference. CoGenesis [442] breaks down the user instruction into\\na general section and a personal section. The LLM generates replies solely based on general\\ninstruction, and the SLM considers both user instruction and additional personal context for its\\noutput generation. A fusion strategy blends the output of LLM and SLM synergistically. Xu et al.\\n[402] introduce a split learning system for LLM agents in 6\\u2009G networks, optimizing mobile device and\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 55}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:55\\nFig. 28. Could-edge synergy between LLMs and SLMs.\\ncloud server collaboration. Mobile devices operate lightweight SLMs with 0–10B parameters for real-\\ntime tasks, while cloud servers handle larger LLMs with over 10B parameters for complex reasoning\\nand planning. This setup allows efficient local task management on mobile devices and offloads\\nheavy operations to cloud servers. The system’s architecture features three modules—perception,\\ngrounding, and alignment—facilitating effective communication to meet the sophisticated needs of\\n6\\u2009G networks.\\nBesides these frameworks, more specific models are proposed to facilitate the cloud-edge synergy.\\nA common strategy is to use SLM’s fast decoding ability. LLM-to-SLM [29] proposes a framework\\nin which the pre-trained frozen encoder–decoder LLM resides on the server and computes a high-\\nquality representation of the prompt for the planning of an appropriate response. The SLM residing\\non the edge device, conditioned on this representation, decodes the response efficiently. Some\\nvariants put more emphasis on the reasoning ability of LLMs [170, 239, 305]. In Synergy of Thoughts\\n[305], the SLMs generate multiple low-cost reasoning paths. If these paths conflict, the larger LLMs\\nare invoked to provide reflective reasoning and correct any intuitive errors. Hao et\\xa0al. [129] propose\\na framework in which an SLM residing on the edge devices generates tokens, calling LLMs to\\nverify and correct threshold-gated “harder” tokens, to achieve a controllable tradeoff between\\ninference quality and cost. LLMCad [399] presents an on-device inference engine addressing\\nmemory and latency issues in deploying LLMs on mobile devices. It combines a lightweight LM for\\ntoken generation with a high-precision LLM for verification, leveraging a token tree structure and\\nspeculative generation for efficiency. Tested on devices such as Jetson TX2, it achieves up to 9.3×\\nspeedup for LLMs with over 10 billion parameters while maintaining accuracy.\\nCloud-edge Synergy During Training. CROSSLM [82] introduces a client-server collaborative\\ntraining framework that preserves data privacy by having clients locally train SLMs instead of\\nfine-tuning LLMs. The framework enables mutual enhancement through a feedback loop where\\nSLMs evaluate LLM-generated synthetic data and provide feedback to improve the LLM’s generative\\ncapabilities, ensuring high-quality and task-specific data. Concurrently, the synthetic data trains the\\nSLMs, boosting their performance. This cyclical exchange fosters cloud-edge synergy and mutual\\nmodel improvement.\\n7.2\\nTask-Centric Synergy\\nThe advent of LLMs has significantly propelled various natural language processing tasks and\\ninspired research into their synergistic interactions with SLMs to enhance the performance of\\nmodels tailored for specific tasks. This section introduces scenarios where SLMs exhibit specialized\\ncapabilities after fine-tuning and discusses how combining their unique strengths with the versatile\\nabilities of LLMs can yield superior performance on specific tasks. For example, LLMs excel at\\nhandling difficult examples or can rewrite content to eliminate task-irrelevant redundancy, thereby\\nenhancing overall task performance, as illustrated in Figures 29–31.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 56}, page_content='145:56\\nF. Wang et al.\\nFig. 29. Synergizing SLMs and LLMs in tool learning.\\n훼-UMi [314] introduces a multi-agent frame-\\nwork to enhance tool learning by overcoming the\\nlimitations of single-LLM approaches for complex\\ntasks. It utilizes three specialized LMs—planner,\\ncaller, and summarizer—as depicted in Figure 29\\n—each handling specific subtasks such as plan-\\nning, tool invocation, and summarization. This\\nmodular design allows the use of small and large open-source LLMs (e.g., LLaMa-7B/12B) and\\nsupports easy tool updates. Evaluated on benchmarks like ToolBench [286] and ToolAlpaca [332],\\n훼-UMi outperforms traditional single-LLM methods and even exceeds GPT-4 in tool learning\\nperformance.\\nSynCID [208] focuses on Conversational Intent Discovery (CID), a task where both known\\nand new intents must be identified from user utterances in an open-world setting. SynCID combines\\nFig. 30. Synergizing SLMs and LLMs in conversa-\\ntional intent detection.\\nLLMs’ deep semantic insights with SLMs’ agility\\nand specialized capabilities. As illustrated in Fig-\\nure 30, the framework uses LLM prompting to\\nrefine discourse and intent labels, enhancing se-\\nmantic accuracy and assigning new labels to un-\\nlabeled data. SLMs are trained via contrastive\\nlearning to align the semantic spaces of discourse\\nand intent descriptors, reducing clustering distor-\\ntion and improving new intent detection. Tested\\non BANKING [44], CLINC [182], and StackOver-\\nflow [401], SynCID outperforms CID baselines\\nsignificantly.\\nFig. 31. Synergizing SLMs and LLMs in information extraction.\\nFilter-then-rerank\\n[239] addresses LLMs’\\npoor performance\\non simpler IE tasks\\nby integrating LLMs\\nand SLMs. SLMs\\nact as filters, pre-\\ndicting and identi-\\nfying difficult sam-\\nples, while LLMs rerank the top N candidate labels for these cases. As illustrated in Figure 31,\\nSLM predictions are final for non-difficult samples, minimizing reliance on LLMs and reducing\\nlatency and costs; for those difficult samples, the top N predicted candidate labels from the SLM\\nare passed to the LLM for reranking (predicting). Tested on small-sample IE tasks, this approach\\nimproves performance by an average of 2.4% compared to previous methods. Data Shunt+ (DS+)\\n[47] introduces a framework to reduce costs by minimizing large model queries during inference\\nand boosting LLM performance with SLMs for tasks like sentiment analysis and image processing.\\nDS+ uses SLMs for “easy” samples within the main training distribution and LLMs for “hard”\\noutliers or boundary cases, maintaining accuracy while reducing LLM use. It incorporates S4L and\\nL4S modules with Prompt Pruning (PP) and 2-stage Confidence Distillation (2CD) for better input\\nprocessing and knowledge transfer. Tests show DS+ outperforms fine-tuning in accuracy and cost\\nefficiency, significantly cutting down on LLM queries.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 57}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:57\\nFig. 32. Scenarios we discuss in this section. The taxonomy is inspired by previous works [326, 357]. Please\\nnote that the trustworthy scenarios listed here are not exhaustive.\\n8\\nTrustworthiness in SLMs\\nLanguage models have become ubiquitous in our daily lives, and we increasingly rely on them.\\nHowever, they pose risks regarding their limitations in trustworthy dimensions like privacy and\\nfairness. These concerns are especially critical in high-stakes domains such as healthcare [132]\\nand finance [206]. Consequently, numerous studies have emerged to evaluate the trustworthiness\\nof LMs [91, 97, 141, 179, 254, 261, 280, 357, 376, 430]. In this section, we consider the works that\\nbenchmark various LMs’ trustworthiness and omit the specific attack methods [42, 55, 153, 465]\\nor work [412] that only focuses on early pre-trained LMs like BERT [86] as they are already\\ncovered in previous survey papers [81, 117, 125, 295]. Inspired by previous works [326, 357], we\\ndiscuss the following five key trustworthy scenarios: robustness, privacy, reliability, safety, and\\nfairness, as shown in Figure 32. We consider two dimensions for robustness: Adversarial (Adv)\\nRobustness [358] and Out-of-Distribution (OOD) Robustness [39, 218]. For safety, we explore two\\nkey concerns: Misinformation [350] and Toxicity [379]. For reliability, we focus on Hallucination\\n[149] and Sycophancy [308]. Please note that these are just the aspects we are focusing on, and\\ntherefore, this is not a comprehensive classification or taxonomy. For example, robustness also\\ncontains robustness to adversarial demonstration.\\nThough there are a lot of works benchmarking LMs’ trustworthiness, their main focus is on\\nLLMs. Therefore, we survey some representative works evaluating the trustworthiness of LMs,\\nfocusing specifically on those that include SLMs of around 7B parameters or smaller. We also\\nsummarize these works in Table 15. Next, we briefly introduce them.\\nHolistic Evaluation of Language Models (HELM) [209] benchmarks a large number of LMs\\nfrom various aspects, including a lot of metrics related to trustworthiness, such as robustness\\nand fairness. Do-Not-Answer [370] introduces a dataset to evaluate how LMs act when they face\\ncontent that should not be answered. Wang et al. [370] also label the output of several LMs on their\\ndataset and then uses the labeled data to train some classifiers. PromptRobust [460] constructs\\ntwo kinds of adversarial prompts to evaluate LMs’ robustness: One kind is designed under non-\\nadversarial settings with semantic integrity, while another category is created under adversarial\\nsettings. Their results show that LMs perform poorly under such prompts. HaluEval [194] builds a\\ndataset comprising both the samples generated by their proposed framework and human-labeled\\nhallucinations. It facilitates analysis of when LMs produce hallucinated output and how well\\nthey detect hallucinated content. Then they use some strategies, such as knowledge retrieval, to\\nhelp LMs better recognize hallucinations. Mo et al. [254] evaluate the trustworthiness of open-\\nsource LMs, presenting a variety of scenarios such as fairness and privacy. Results show that\\nsmaller LMs sometimes outperform larger ones in terms of trustworthiness. PrivLM-Bench [193] is\\ndesigned to evaluate the privacy issues in LMs. It enables a fair comparison of privacy-preserving\\nLMs by considering more than just differential privacy parameters. FFT [74] introduces around\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 58}, page_content='145:58\\nF. Wang et al.\\nTable 15. Comparison of Different Works That Evaluate the\\nTrustworthiness Issues in LMs\\nPaper\\nAdv Robustness\\nOOD Robustness\\nToxicity\\nMisinformation\\nHallucination\\nSycophancy\\nPrivacy\\nFairness\\nHave Compressed SLMs\\nHELM [209]\\n✓\\n×\\n✓\\n✓\\n×\\n×\\n×\\n✓\\n×\\nDo-Not-Answer [370]\\n×\\n×\\n✓\\n✓\\n×\\n×\\n✓\\n✓\\n×\\nPromptRobust [460]\\n✓\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\nHaluEval [194]\\n×\\n×\\n×\\n×\\n✓\\n×\\n×\\n×\\n×\\nMo et al. [254]\\n✓\\n×\\n✓\\n×\\n✓\\n✓\\n✓\\n✓\\n×\\nPrivLM-Bench [193]\\n×\\n×\\n×\\n×\\n×\\n×\\n✓\\n×\\n×\\nFFT [74]\\n×\\n×\\n✓\\n✓\\n✓\\n×\\n×\\n✓\\n×\\nROBBIE [100]\\n×\\n×\\n✓\\n×\\n×\\n×\\n×\\n✓\\n×\\nTrustLLM [326]\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n×\\nRAmBLA [36]\\n✓\\n×\\n×\\n×\\n✓\\n×\\n×\\n×\\n×\\nJailbreakBench [46]\\n×\\n×\\n✓\\n✓\\n×\\n×\\n✓\\n×\\n×\\nXie et al. [395]\\n×\\n×\\n✓\\n×\\n✓\\n×\\n×\\n×\\n×\\nOR-Bench [73]\\n×\\n×\\n✓\\n✓\\n×\\n×\\n✓\\n×\\n×\\nSORRY-Bench [392]\\n×\\n×\\n✓\\n✓\\n×\\n×\\n✓\\n×\\n×\\nBeHonest [61]\\n×\\n×\\n×\\n✓\\n✓\\n✓\\n×\\n×\\n×\\nHong et al. [141]\\n✓\\n✓\\n✓\\n×\\n×\\n×\\n✓\\n✓\\n✓\\nRUPBench [376]\\n✓\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\nNakka et al. [261]\\n×\\n×\\n✓\\n×\\n×\\n×\\n✓\\n✓\\n×\\ntwo thousand crafted examples to evaluate LMs’ performances on three trustworthy dimensions:\\nfactuality, fairness, and toxicity. Their results suggest that larger LMs do not always show better\\nharmlessness. ROBBIE [100] first benchmarks various series of LMs using a lot of datasets, including\\ntwo newly introduced datasets developed by ROBBIE. It also evaluates mitigation techniques\\ndesigned to reduce bias and toxicity. TrustLLM [326] is a comprehensive benchmark that contains a\\nlarge number of datasets and various well-designed metrics to systematically evaluate various LMs\\nacross multiple trustworthy dimensions, including truthfulness, safety, fairness, robustness, privacy,\\nand machine ethics. They also carefully design specific subcategories for each dimension. RAmBLA\\n[36] evaluates the trustworthiness of four LMs as biomedical assistants from three dimensions:\\nRobustness, High Recall, and Hallucination. RAmBLA suggests LMs with more parameters are\\nless likely to cause hallucinations and may choose to reject providing an answer in uncertain\\nsituations. JailbreakBench [46] constructs a jailbreaking dataset named JBB-Behaviors and jailbreak\\nartifacts to evaluate current LMs’ performance regarding jailbreaking. It also proposes a unified\\nevaluation pipeline that can incorporate new jailbreak defense techniques. Xie et al. [395] test\\nonline safety analysis methods, filling the gap where no methods focus on the generation phase.\\nOR-Bench [73] constructs three datasets: OR-Bench-80K, OR-Bench-Hard-1K, and OR-Bench-Toxic,\\nto systematically evaluate over-refusal problems in LMs, emphasizing the challenge of balancing\\nsafety alignment with the models’ usefulness. SORRY-Bench [392] systematically tests 43 different\\nLMs to see how they perform when facing requests that should be refused. They also collect more\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 59}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:59\\nthan annotations created by humans and find that fine-tuned 7B LMs can achieve performance\\ncomparable to GPT-4 scale LMs as evaluators. BeHonest [61] evaluates the honesty of LMs from\\nthree aspects: Self-Knowledge, Non-Deceptiveness, and Consistency. They use many different\\nmetrics for each aspect. For example, the sycophancy rate and the lying rate are adopted in Non-\\nDeceptiveness. The results in both the Self-Knowledge and Consistency parts reveal that larger\\nmodel sizes generally bring improved performance for the Llama-2 [345] and Llama-3 [96] series.\\nHong et al. [141] examine the effects of compression methods, including quantization and pruning,\\non the trustworthiness of language models. They find that pruning and extreme quantization\\nsignificantly affect the trustworthiness of LMs. RUPBench [376] comprises 15 reasoning datasets\\ndesigned to assess the performance of LMs both in normal conditions and under various adversarial\\nperturbations. Their results indicate that larger LMs generally demonstrate better resilience to\\nperturbations. Nakka et al. [261] investigate the trust and ethical implications of SLMs deployed on\\npersonal devices. It reveals the vulnerabilities of on-device SLMs compared with their on-server\\ncounterparts.\\nPlease note that the dimensions discussed in this section reflect only those relevant to our\\ncurrent focus; additional dimensions may be discussed in those works but not listed in Table 15.\\nFor example, TrustLLM [326] also explores Machine Ethics.\\n9\\nFuture Directions\\nIn this section, we offer insights into several promising future research directions that could inspire\\nand motivate the community to address existing gaps in the development of SLMs.\\n9.1\\nDeveloping Efficient SLM Model Architecture\\nAlthough Transformers [352] are foundational in most language models, they face significant\\ncomputational and memory challenges that worsen with model size, impacting training and autore-\\ngressive decoding. Recently, Mamba [119] has emerged as a promising alternative, adapting SSMs\\nto dynamically select inputs based on demands, thereby enhancing efficiency. Thereafter, xLSTM\\n[26] demonstrates that an improved LSTM could function as an LLM, revealing the potential of\\ntraditional SSMs. The integration of global static information captured by SSMs with the dynamic\\ninformation processing of Transformers could complement each other, leading to new architectures\\nthat balance effectiveness and efficiency.\\n9.2\\nAddressing SLM Training Inefficiencies\\nOne study [88] explores the disparate learning dynamics between SLMs and LLMs. Utilizing the\\nPythia model suite, the research demonstrates that layers’ activations in larger models converge\\nmore rapidly and monotonically to their final states. This phenomenon is associated with a higher\\nproportional effective rank (PER) in the parameters and gradients of larger models. The analysis\\nenhances our understanding of training inefficiencies in small models and provides insights for\\nfuture efforts, such as developing methods to increase the PER of layers’ parameters.\\n9.3\\nExpanding Domain-Specific SLMs\\nDomain-specific SLMs, which are tailored for specific fields, can provide a stronger foundation\\nfor relevant downstream tasks than general-purpose models. Currently, these models primarily\\nfocus on scientific and healthcare domains. However, there is significant potential for expansion\\ninto other key areas such as law, finance, education, telecommunications, and transportation. The\\nscarcity of SLMs that cater to these domains presents an urgent call for research into developing\\nmore specialized models.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 60}, page_content='145:60\\nF. Wang et al.\\n9.4\\nEstablishing Benchmarking and Leaderboard Platforms for SLMs\\nSeveral compelling reasons justify the establishment of benchmarking and leaderboard platforms for\\nSLMs. Firstly, most state-of-the-art SLMs are trained on proprietary datasets, which may include test\\nsets from existing evaluation tasks, presenting challenges for fair capability comparisons. Secondly,\\nmany SLMs are designed for specific device applications, significantly differing from general\\nopen-domain tasks. Thus, there is a lack of comprehensive benchmarks that accurately reflect\\nSLM performance in specific device applications. For example, SLMs deployed on smartphones\\noften handle tasks sensitive to user data, such as auto-replies based on historical chat texts or GUI\\ncontext understanding—tasks not typically included in current benchmarks, potentially leading to\\nan underestimation of their importance. Finally, current evaluation tasks focus primarily on metrics\\nlike accuracy. Evaluating on-device SLMs involves balancing multiple factors, including overall\\ncapabilities, response times, storage and memory usage, power consumption, CPU utilization,\\nadditional fine-tuning requirements, and context window constraints, making comprehensive and\\ndetailed assessments essential.\\n9.5\\nEnhancing SLM Performance and Efficiency\\nIn terms of enhancing SLM performance and efficiency, the efficiency of using teacher LLMs via\\ninstruction tuning can be further developed, such as Efficient Instruction Tuning of SLMs from\\nLLMs-generated data, Optimizing Teacher LLM Selection for SLM Learning, and Applying Emerging\\nTechniques from LLMs to SLMs.\\n—Efficient Instruction Tuning of SLMs from LLMs-Generated Data. Enhancing the specialization\\nof SLMs through instruction tuning from LLMs-generated data is crucial, yet finding the most\\ncost-effective instructional strategies remains an underexplored area. Some key areas for\\nexploration are:\\n(1) Instruction Design Adaptability: The performance of LLMs and SLMs varies significantly\\nwith changes in instructions. Therefore, designing tailored instructions that effectively\\nactivate relevant sub-competencies and reasoning pathways in SLMs for specific tasks is\\ncrucial. This approach would optimize their ability to utilize instructional data, repre-\\nsenting a significant future research direction.\\n(2) SLM Capability Adaptability: Given that SLMs exhibit diverse capabilities across domains,\\nsimply supplying extensive data samples for instruction tuning is often inefficient, as\\nSLMs may spend excessive time processing unnecessary data. To optimize efficiency\\nwhen adapting to specific domains, we suggest first assessing the intrinsic capabilities\\nof an SLM within those domains. Subsequently, one could select appropriate data and\\nactivate essential fine-grained capabilities to effectively adapt to domain shifts. This\\ntargeted approach ensures efficient and domain-specific instruction tuning.\\n(3) Optimizing Data Efficiency: SLMs may possess missing or latent domain knowledge,\\nand activating this latent knowledge may not require substantial data. Thus, identifying\\ninherent knowledge within SLMs and determining the minimal data necessary for effec-\\ntive fine-tuning is a future direction. This research aims to optimize performance while\\nminimizing training resources.\\n—Optimizing Teacher LLM Selection for SLM Learning. Teacher LLMs with different abilities\\nand knowledge facilitate diverse applications for SLM training, including data rewriting and\\ngeneration. Selecting the appropriate teacher model based on specific use cases is crucial.\\nThis process requires evaluating the teacher’s capabilities and knowledge to ensure optimal\\napplication. For example, GPT-4 excels in generating domain-specific data, outperforming\\nChatGPT, which may produce inferior outcomes. Strategic selection of teacher LLMs is\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 61}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:61\\nessential for future work to ensure their strengths are effectively utilized to enhance SLM\\nperformance.\\n—Applying Emerging Techniques from LLMs to SLMs. To improve LLM performance, techniques\\nsuch as RAG and Mixture of Experts (MoE) are employed. The adoption of RAG in SLMs shows\\nsignificant promise [220], suggesting benefits from further tailoring retrieved information for\\nSLMs. Future research should account for SLMs’ constraints, such as limited context windows,\\nand customize RAG accordingly. MoE uses multiple experts to enhance learning without\\nincreasing active neurons, but its storage demands pose challenges for SLM deployment,\\nmaking this a promising area for exploration. Additionally, the application of LLM techniques,\\nsuch as in-context learning and prompt engineering to maximize SLM performance, while\\naccounting for SLMs’ constraints, warrants further investigation.\\n9.6\\nApplications of SLMs\\nIn real-world applications, SLMs often need to provide personalized services and need to be updated\\nperiodically to reflect new needs and new knowledge. Hence, there are several promising directions\\nin terms of the real-world application of SLMs, which are listed as follows:\\n—LoRA for Personalized Services. Companies often provide personalized services, but user-\\nspecific complexities can render simple rules ineffective. Training a separate SLM for each\\nuser is impractical. LoRA suggests a method of separable training weights alongside fixed\\noriginal weights, enabling scalable customization. For instance, RecLoRA [459] integrates\\npersonalized knowledge into SLMs/LLMs tailored for recommendation tasks by maintaining\\na set of parallel, independent LoRA weights. This approach effectively customizes language\\nmodel parameters to align with individual user preferences. This approach is a promising\\ndirection that inspires further investigation.\\n—Lifelong On-device Learning for Knowledge Injection. SLMs on devices can access local data\\nwithout risking data privacy through two main methods. The first method uses retrieval-\\naugmented generation to integrate local data into prompts, requiring SLMs with advanced\\nprocessing and reasoning capabilities. The second method fine-tunes SLMs with local data,\\nintegrating customized knowledge into the model’s weights. However, this approach demands\\nsignificant device resources, including memory and energy. A promising solution is lifelong\\nlearning, where SLMs continuously learn and adapt while in use.\\n—Strategic Use of SLMs and LLMs in Multi-Agent Systems. LLMs can function as agents; however,\\ntheir extensive capabilities are often underutilized in many scenarios, leading to resource\\nwastage. Consequently, strategically routing to appropriately capable SLMs and LLMs within\\nmulti-agent systems can optimize cost and functionality.\\n9.7\\nMultimodal SLMs\\nResearch on SLMs also includes multimodal data. For example, SmolVLM [101] is a compact model\\nthat handles image and text inputs to produce text outputs, suitable for on-device use and various\\nmultimodal tasks. SOLO [56] integrates vision and language processing in a single 7B Transformer\\nmodel. The limited scope of existing research on multimodal SLMs provides a compelling impetus\\nfor researchers to investigate the integration of various modalities, including audio and graphs.\\n9.8\\nSLMs Assisting LLMs\\nIn Section 6, we introduced existing works on the use of SLMs to assist LLMs. For instance, EFT\\n[252] emulates fine-tuning on LLMs by leveraging behavior deltas between SLMs’ pre-trained and\\nfine-tuned weights to alleviate the time-cost issues associated with fine-tuning LLMs; SlimPLM\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 62}, page_content='145:62\\nF. Wang et al.\\n[331] detects missing knowledge in LLMs using a slim proxy SLM to accelerate knowledge injection;\\nContrastive Decoding [202] enhances text quality by maximizing the difference between the log\\nprobabilities of an expert LLM and an amateur SLM to mitigate issues of low-quality generation.\\nThe research on adopting SLMs to assist LLMs is still in its early stages, with many promising\\ndirections yet to be explored. We list some as follows:\\n—Enhancing LLM Performance Across Broader Tasks through SLM Integration. SLMs can outper-\\nform LLMs in certain scenarios. For example, SLMs often present fewer security vulnerabilities\\ncompared to their larger counterparts and demonstrate superior performance on easier sam-\\nples in specific tasks [201, 239]. Therefore, integrating SLMs with LLMs can promote the\\ndevelopment of models that are not only more robust but also inherently safer. Currently,\\nresearch in this domain is relatively sparse, suggesting that this collaborative framework\\ncould potentially be applied to a wider array of tasks.\\n—Efficient Enhancement of LLMs through Proxy SLMs. Existing research [19, 217, 252, 331]\\nindicates that SLMs, owing to their accelerated fine-tuning and inference speeds, can effectively\\nmimic the behaviors of LLMs, thereby serving as efficient proxies for optimization. However,\\nthe application of SLMs as operational proxies for LLMs is currently underexplored. This\\nmimicry could potentially be expanded to include various aspects of LLM functionality, such\\nas the optimization of prompts, the filtration and integration of supplementary knowledge,\\nand the management of additional knowledge repositories.\\n—SLMs Assist in Managing Data Quality. LLMs tend to produce hallucinations and toxic content\\ndue to low-quality real-world training data. One solution is to remove these low-quality data\\n[361]. However, directly eliminating low-quality content can diminish certain functionalities\\nof LLMs, such as versatility [362]. Therefore, it is crucial to define more refined data quality\\nassessment criteria across dimensions such as factuality, safety, and diversity [382] for real-\\nworld data. Researching efficient data selection methods using small models represents a\\nvaluable research direction. Additionally, while synthetic data serves as a vital complement\\nto scarce human-generated data [228], the potential for small models to effectively manage\\nsynthetic data remains largely unexplored.\\n—SLMs Assist in LLM Assessment. LLMs are producing vast amounts of increasingly complex\\ntexts, such as specialized code and scientific papers, presenting challenges not only for human\\nevaluators but also for traditional assessment metrics. Consequently, developing effective\\nevaluators to assess various aspects of generated content, including factuality, safety, and\\nuncertainty, becomes crucial. Given their proficiency in handling specific tasks, exploring the\\npotential of SLMs to evaluate LLM outputs is a promising research direction.\\n—SLMs Optimize Query and Reduce Noise for LLM RAG. For RAG using LLMs, differing query\\nrequirements between LLMs and search engines pose a challenge. The query for LLMs is\\noften abstract and difficult for search engines to handle, so they require more detailed query\\nkeywords. Moreover, LLMs may not need all the information related to a query because they\\nonly require partial additional knowledge. Thus, intermediate agents are crucial to adapting\\nLLM queries for search engines by clarifying the required detailed keywords that can search\\nfor necessary extra knowledge. Additionally, search engine outputs contain noise, requiring\\nrefinement to boost LLM efficiency. SLMs, skilled in a single task, are ideal for optimizing\\nquery rewriting and noise reduction in RAG systems, making their application in LLM RAG a\\npromising research area.\\n—SLMs Safeguard LLM. Resources such as the Llama 2 Responsible Use Guide strongly advocate\\nfor the implementation of robust guardrails in products that utilize Generative AI. SLMs can\\nbe strategically designed to serve as such guardrails, mitigating risks associated with both\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 63}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:63\\ninputs and outputs from the model. This approach ensures safeguards against the generation\\nof high-risk or policy-violating content and provides protection against adversarial inputs and\\nattempts to compromise the model. Future research could explore how SLMs can enhance the\\nsafety of LLMs by providing protection against various threats, including adversarial attacks,\\njailbreak attacks, and backdoor attacks.\\n9.9\\nSynergy between SLMs and LLMs\\nIn Section 7, we discussed how SLMs and LLMs can complement each other. For example, CoGenesis\\n[442] integrates SLMs for private data and LLMs for broader context, while Synergy of Thoughts\\n[305] uses SLMs for initial reasoning and LLMs for conflict resolution. CROSSLM [82] shows how\\nprivacy can be preserved by training SLMs locally to support LLMs without data exposure. Research\\nin this area is still evolving, and we outline several promising future directions below:\\n—Refined Cloud-Edge Division of Labor. Current research mainly focuses on splitting tasks be-\\ntween edge-based SLMs and cloud-based LLMs along privacy-sensitive and non-sensitive\\ndata boundaries. A potential future direction involves more granular task partitioning: deter-\\nmining which subtasks should be handled locally by SLMs (e.g., initial data filtering, quick\\nsemantic parsing) and which should be delegated to the cloud-based LLM (e.g., advanced\\nreasoning, complex generation). This approach can further optimize latency, privacy, and\\nresource utilization.\\n—Adaptive On-Device Specialization for Dynamic Environments. Although SLMs have shown the\\nability to handle private or personalized data locally, continuous changes in user preferences,\\napplication requirements, and data distributions pose challenges. Future work can explore\\nadaptive strategies where edge-based SLMs dynamically specialize or update their parameters,\\nguided by the cloud-based LLM. For instance, the LLM can periodically distill new knowledge\\ninto the SLM or provide feedback signals to help the SLM adapt to evolving scenarios.\\n9.10\\nTrustworthy SLMs\\nAs SLMs are playing crucial roles in various aspects, understanding and improving the trustwor-\\nthiness of SLMs are essential. Hence, two promising directions are:\\n—A Comprehensive Evaluation of SLMs’ Trustworthiness. While numerous studies address trust-\\nworthiness issues in LLMs, research on SLMs remains sparse. Most existing literature focuses\\non models with at least 7 billion parameters, leaving a gap in the comprehensive analysis of\\nSLMs’ trustworthiness. Current evaluations typically cover only a fraction of the necessary\\naspects. Therefore, a systematic assessment, such as TrustLLM [326], is essential to thor-\\noughly evaluate the trustworthiness of SLMs and understand their reliability across various\\napplications.\\n—Developing Trustworthy SLMs. Developing trustworthy SLMs is crucial, with three key research\\ndirections: (i) Training SLMs to be trustworthy from scratch; (ii) Ensuring SLMs retain or gain\\ntrustworthiness when compressed from LLMs—maintaining trustworthiness if the LLM is\\ntrustworthy and instilling trustworthiness if it is not; and (iii) Fine-tuning non-trustworthy\\nSLMs to enhance their robustness.\\n10\\nConclusion\\nThis article provides a comprehensive survey of SLMs with up to 7 billion parameters. Initially, we\\naddress the need to clearly define SLMs due to existing ambiguities in their characterization. We\\nthen present the foundational concepts essential for constructing SLMs. The survey progresses to\\nexplore enhancement techniques, including KD and quantization, as well as strategies for adapting\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 64}, page_content='145:64\\nF. Wang et al.\\nLLMs to SLM contexts. We survey representative SLMs, both general-domain and domain-specific,\\ndiscussing their preferred datasets and architectural decisions. We also assess their applications\\nacross various tasks and deployment strategies on devices. Further, we investigate their role in\\naugmenting the capabilities of LLMs, serving as proxies for fine-tuning and facilitating two types\\nof synergies: cloud-local and task-centric. Additionally, we discuss the critical aspect of their\\ntrustworthiness. The article concludes with key insights aimed at guiding future research on SLMs.\\nReferences\\n[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen\\nBach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. 2024. Phi-3 technical report: A highly capable language\\nmodel locally on your phone. arXiv:2404.14219. Retrieved from https://arxiv.org/abs/2404.14219\\n[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 Technical Report. arXiv:2303.08774. Retrieved\\nfrom https://arxiv.org/abs/2303.08774\\n[3] Emre Can Acikgoz, Osman Batur İnce, Rayene Bench, Arda Anıl Boz, İlker Kesen, Aykut Erdem, and Erkut Erdem.\\n2024. Hippocrates: An open-source framework for advancing large language models in healthcare. arXiv:2404.16621.\\nRetrieved from https://arxiv.org/abs/2404.16621\\n[4] Harshavardhan Adepu, Zhanpeng Zeng, Li Zhang, and Vikas Singh. 2024. FrameQuant: Flexible low-bit quantization\\nfor transformers. arXiv:2403.06082. Retrieved from https://arxiv.org/abs/2403.06082\\n[5] Abien Fred Agarap. 2018. Deep learning using rectified linear units (ReLU). arXiv:1803.08375. Retrieved from\\nhttp://arxiv.org/abs/1803.08375\\n[6] Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos Garea, Matthieu Geist, and Olivier\\nBachem. 2024. On-policy distillation of language models: Learning from self-generated mistakes. In The Twelfth\\nInternational Conference on Learning Representations (ICLR ’24).\\n[7] Meta AI. 2024. Llama 3.2: Revolutionizing edge AI and vision with open, customizable models. Retrieved from\\nhttps://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/. Accessed: September 25, 2024.\\n[8] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. 2023. GQA:\\nTraining generalized multi-query transformer models from multi-head checkpoints. arXiv:2305.13245. Retrieved\\nfrom https://arxiv.org/abs/2305.13245\\n[9] Ali Al-Lawati, Jason Lucas, Zhiwei Zhang, Prasenjit Mitra, and Suhang Wang. 2025. Graph-based molecular in-context\\nlearning grounded on morgan fingerprints. arXiv:2502.05414. Retrieved from https://arxiv.org/abs/2502.05414\\n[10] Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Leandro von Werra, and Thomas Wolf. 2024. SmolLM - blazingly\\nfast and remarkably powerful. arXiv:2409.00286v1. Retrieved from https://arxiv.org/abs/2409.00286v1\\n[11] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane\\nDebbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. 2023. The falcon series of open\\nlanguage models. arXiv:2311.16867. Retrieved from https://arxiv.org/abs/2311.16867\\n[12] Guilherme F. C. F. Almeida, José Luiz Nunes, Neele Engelmann, Alex Wiegmann, and Marcelo de Araújo. 2024.\\nExploring the psychology of LLMs’ moral and legal reasoning. Artif. Intell. 333, (2024), 104145. DOI: https://doi.org/\\n10.1016/j.artint.2024.104145\\n[13] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji\\nRuwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. 2022. Deepspeed-inference: Enabling efficient inference\\nof transformer models at unprecedented scale. In SC22: International Conference for High Performance Computing,\\nNetworking, Storage and Analysis. IEEE, 1–15.\\n[14] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang. 2024. Fluctuation-based adaptive structured pruning for\\nlarge language models. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38, AAAI, 10865–10873.\\n[15] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri,\\nEmanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 Technical Report. arXiv:2305.10403. Retrieved from\\nhttps://arxiv.org/abs/2305.10403\\n[16] AI Anthropic. 2024. The Claude 3 model family: Opus, Sonnet, Haiku. Claude-3 Model Card, 1, (2024). Retrieved from\\nhttps://api.semanticscholar.org/CorpusID:268232499\\n[17] David Anugraha, Genta Indra Winata, Chenyue Li, Patrick Amadeus Irawan, and En-Shiun Annie Lee. 2024. ProxyLM:\\nPredicting language model performance on multilingual tasks via proxy models. arXiv:2406.09334. Retrieved from\\nhttps://arxiv.org/abs/2406.09334\\n[18] Viraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr Locatelli, Marzieh Fadaee, Ahmet\\nÜstün, and Sara Hooker. 2024. To code, or not to code? Exploring impact of code in pre-training. arXiv:2408.10914.\\nRetrieved from https://arxiv.org/abs/2408.10914\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 65}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:65\\n[19] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-RAG: Learning to retrieve,\\ngenerate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations.\\nRetrieved from https://openreview.net/forum?id=hSyW5go0v8\\n[20] Saleh Ashkboos, Maximilian L. Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. 2024.\\nSliceGPT: Compress large language models by deleting rows and columns. In The Twelfth International Conference on\\nLearning Representations. Retrieved from https://openreview.net/forum?id=vXxardq6db\\n[21] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang,\\nCarrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv:2108.07732.\\nRetrieved from https://arxiv.org/abs/2108.07732\\n[22] Amos Azaria, and Tom Mitchell. 2023. The internal state of an LLM knows when it’s lying. In Findings of the\\nAssociation for Computational Linguistics: EMNLP ’23. Association for Computational Linguistics, 967–976.\\n[23] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng,\\nStella Biderman, and Sean Welleck. 2023. Llemma: An open language model for mathematics. arXiv:2310.10631.\\nRetrieved from https://arxiv.org/abs/2310.10631\\n[24] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et\\nal. 2023. Qwen Technical Report. arXiv:2309.16609. Retrieved from https://arxiv.org/abs/2309.16609\\n[25] Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. 2020. The pushshift\\nreddit dataset. In Proceedings of the International AAAI Conference on Web and Social Media, Vol.14, AAAI, 830–839.\\n[26] Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael K. Kopp,\\nGünter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. 2024. xLSTM: Extended long Short-Term memory.\\nIn The Thirty-Eighth Annual Conference on Neural Information Processing Systems. Retrieved from https://openreview.\\nnet/forum?id=ARAxPPIAhq\\n[27] Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan, James\\nBaicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, et al. 2024. Stable LM 2 1.6B Technical Report. arXiv:2402.17834.\\nRetrieved from https://arxiv.org/abs/2402.17834\\n[28] Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. 2024. SmolLM-Corpus.\\nRetrieved from https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus\\n[29] Benjamin Bergner, Andrii Skliar, Amelie Royer, Tijmen Blankevoort, Yuki Asano, and Babak Ehteshami Bejnordi.\\n2024. Think big, generate quick: LLM-to-SLM for fast autoregressive decoding. arXiv:2402.16844. Retrieved from\\nhttps://arxiv.org/abs/2402.16844\\n[30] Milan Bhan, Jean-Noel Vittaut, Nicolas Chesneau, and Marie-Jeanne Lesot. 2024. Self-AMPLIFY: Improving small\\nlanguage models with self post Hoc explanations. arXiv:2402.12038. Retrieved from https://arxiv.org/abs/2402.12038\\n[31] Zhen Bi, Ningyu Zhang, Yida Xue, Yixin Ou, Daxiong Ji, Guozhou Zheng, and Huajun Chen. 2023. OceanGPT: A\\nlarge language model for ocean science tasks. arXiv:2310.02031. Retrieved from https://arxiv.org/abs/2310.02031\\n[32] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad\\nAflah Khan, Shivanshu Purohit, Usvsn Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar\\nvan der Wal. 2023. Pythia: A suite for analyzing large language models across training and scaling. arXiv:230401373.\\nRetrieved from https://arxiv.org/abs/2304.01373\\n[33] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. 2020. Piqa: Reasoning about physical commonsense in\\nnatural language. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34, AAAI, 7432–7439.\\n[34] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive\\nLanguage Modeling with Mesh-Tensorflow. https://doi.org/10.5281/zenodo.5297715.\\n[35] Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Roxana Daneshjou,\\nJonathan Frankle, Percy Liang, Michael Carbin, et al. 2024. BioMedLM: A 2.7 b parameter language model trained on\\nbiomedical text. arXiv:2403.18421. Retrieved from https://arxiv.org/abs/2403.18421\\n[36] William James Bolton, Rafael Poyiadzi, Edward R. Morrell, Gabriela van Bergen, Gonzalez Bueno, and Lea Goetz. 2024.\\nRAmBLA: A framework for evaluating the reliability of LLMs as assistants in the biomedical domain. arXiv:2403.14578.\\nRetrieved from https://arxiv.org/abs/2403.14578\\n[37] Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022. Inpars: Data augmentation for\\ninformation retrieval using large language models. arXiv:2202.05144. Retrieved from https://arxiv.org/abs/2202.05144\\n[38] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\\nPranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. In Advances in\\nNeural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol.\\n33. Curran Associates, Inc., 1877–1901. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2020/file/\\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\\n[39] Saikiran Bulusu, Bhavya Kailkhura, Bo Li, Pramod K. Varshney, and Dawn Song. 2020. Anomalous example detection\\nin deep learning: A survey. IEEE Access 8, (2020), 132330–132347. DOI: https://doi.org/10.1109/ACCESS.2020.3010274\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 66}, page_content='145:66\\nF. Wang et al.\\n[40] Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. 2025. A survey on mixture of experts.\\nIEEE Transactions on Knowledge & Data Engineering 37, 7 (2025), 3896–3915.\\n[41] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei\\nChu, et al. 2024. InternLM2 Technical Report. arXiv:2403.17297. Retrieved from https://arxiv.org/abs/2403.17297\\n[42] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts,\\nTom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In 30th\\nUSENIX Security Symposium (USENIX Security ’21). JMLR, 2633–2650.\\n[43] Samuel Carreira, Tomás Marques, José Ribeiro, and Carlos Grilo. 2023. Revolutionizing mobile interaction: Enabling\\na 3 billion parameter GPT LLM on mobile. arXiv:231001434. Retrieved from https://arxiv.org/abs/2310.01434\\n[44] Iñigo Casanueva, Tadas Temčinas, Daniela Gerz, Matthew Henderson, and Ivan Vulić. 2020. Efficient intent detection\\nwith dual sentence encoders. In Proceedings of the 2nd Workshop on Natural Language Processing for Conversational\\nAI. Association for Computational Linguistics, Online, 38–45.\\n[45] Wei-Cheng Chang, X. Yu Felix, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. 2020. Pre-training tasks for\\nembedding-based large-scale retrieval. In International Conference on Learning Representations.\\n[46] Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag,\\nEdgar Dobriban, Nicolas Flammarion, George J. Pappas, Florian Tramer, et al. 2024. Jailbreakbench: An open\\nrobustness benchmark for jailbreaking large language models. arXiv:2404.01318. Retrieved from https://arxiv.org/\\nabs/2404.01318\\n[47] Dong Chen, Shuo Zhang, Yueting Zhuang, Siliang Tang, Qidong Liu, Hua Wang, and Mingliang Xu. 2024. Improving\\nlarge models with small models: Lower costs and better performance. arXiv:2406.15471. Retrieved from https:\\n//arxiv.org/abs/2406.15471\\n[48] Dong Chen, Yueting Zhuang, Shuo Zhang, Jinfeng Liu, Su Dong, and Siliang Tang. 2024. Data shunt: Collaboration\\nof small and large models for lower costs and better performance. In Proceedings of the AAAI Conference on Artificial\\nIntelligence, Vol. 38, AAAI, 11249–11257.\\n[49] Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. 2025. SFT or\\nRL? an early investigation into training R1-like reasoning large vision-language models. arXiv:2504.11468. Retrieved\\nfrom https://arxiv.org/abs/2504.11468\\n[50] Hongzhan Chen, Siyue Wu, Xiaojun Quan, Rui Wang, Ming Yan, and Ji Zhang. 2023. MCC-KD: Multi-CoT consistent\\nknowledge distillation. In Findings of the Association for Computational Linguistics: EMNLP ’23. Association for\\nComputational Linguistics, 6805–6820.\\n[51] Lihu Chen, and Gaël Varoquaux. 2024. What is the role of small models in the LLM era: A survey. arXiv:2409.06857.\\nRetrieved from https://arxiv.org/abs/2409.06857\\n[52] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri\\nEdwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on\\ncode. arXiv:2107.03374. Retrieved from https://arxiv.org/abs/2107.03374\\n[53] Tianyi Chen, Tianyu Ding, Badal Yadav, Ilya Zharkov, and Luming Liang. 2023. Lorashear: Efficient large language\\nmodel structured pruning and knowledge recovery. arXiv:2310.18356. Retrieved from https://arxiv.org/abs/2310.18356\\n[54] Wei Chen, Zhiyuan Li, and Mingyuan Ma. 2024. Octopus: On-device language model for function calling of software\\nAPIs. arXiv:240401549. Retrieved from https://arxiv.org/abs/2404.01549\\n[55] Yangyi Chen, Fanchao Qi, Hongcheng Gao, Zhiyuan Liu, and Maosong Sun. 2022. Textual backdoor attacks can Be\\nmore harmful via two simple tricks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\\nProcessing, EMNLP. Springer-Verlag, 11215–11221.\\n[56] Yangyi Chen, Xingyao Wang, Hao Peng, and Heng Ji. 2024. A single transformer for scalable vision-language\\nmodeling. arXiv:2407.06438. Retrieved from https://arxiv.org/abs/2407.06438\\n[57] Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi,\\nMatteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, et al. 2023. MEDITRON-70B: Scaling medical\\npretraining for large language models. arXiv:2311.16079. Retrieved from https://arxiv.org/abs/2311.16079\\n[58] Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane,\\nTing-Hao Huang, Bryan R. Routledge, et al. 2021. FinQA: A dataset of numerical reasoning over financial data. In\\nProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. ACM, 3697–3711.\\n[59] Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang. 2022. ConvFinQA:\\nExploring the chain of numerical reasoning in conversational finance question answering. In Proceedings of the\\n2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,\\n6279–6292.\\n[60] Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Hongzhi Zhang, Fuzheng Zhang, Di Zhang, Kun Gai, and Ji-Rong Wen.\\n2024. Small agent can also rock! Empowering small language models as hallucination detector. arXiv:2406.11277.\\nRetrieved from https://arxiv.org/abs/2406.11277\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 67}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:67\\n[61] Steffi Chern, Zhulin Hu, Yuqing Yang, Ethan Chern, Yuan Guo, Jiahe Jin, Binjie Wang, and Pengfei Liu. 2024. BeHonest:\\nBenchmarking honesty of large language models. arXiv:2406.13261. Retrieved from https://arxiv.org/abs/2406.13261\\n[62] Yew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. 2024. InstructEval: Towards holistic evaluation of\\nInstruction-Tuned large language models. In Proceedings of the First Edition of the Workshop on the Scaling Behavior\\nof Large Language Models (SCALE-LLM 2024). Association for Computational Linguistics, 35–64.\\n[63] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang,\\nYonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing\\nGPT-4 with 90%* ChatGPT Quality. Retrieved from https://lmsys.org/blog/2023-03-30-vicuna/\\n[64] Yae Jee Cho, Luyang Liu, Zheng Xu, Aldi Fahrezi, and Gauri Joshi. 2024. Heterogeneous LoRA for federated fine-tuning\\nof on-device foundation models. arXiv:240106432. Retrieved from https://arxiv.org/abs/2401.06432\\n[65] Xiaokai Chu, Jiashu Zhao, Lixin Zou, and Dawei Yin. 2022. H-ERNIE: A multi-granularity pre-trained language\\nmodel for web search. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in\\nInformation Retrieval. ACM, 1478–1489.\\n[66] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang,\\nMostafa Dehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models. J. Mach. Learn.\\nRes. 25, 70 (2024), 1–53. DOI: https://doi.org/10.5555/3722577.3722647\\n[67] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.\\n2019. BoolQ: Exploring the surprising difficulty of natural yes/no questions. arXiv:1905.10044. Retrieved from\\nhttps://arxiv.org/abs/1905.10044\\n[68] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.\\n2018. Think you have solved question answering? Try ARC, the AI2 reasoning challenge. arXiv:1803.05457. Retrieved\\nfrom https://arxiv.org/abs/1803.05457\\n[69] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry\\nTworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv:2110.14168.\\nRetrieved from https://arxiv.org/abs/2110.14168\\n[70] Together Computer. 2023. RedPajama: An Open Dataset for Training Large Language Models. Retrieved from\\nhttps://github.com/togethercomputer/RedPajama-Data\\n[71] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Za-\\nharia, and Reynold Xin. 2023. Free Dolly: Introducing the World’s First Truly Open Instruction-Tuned LLM. Retrieved\\nfrom https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm\\n[72] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021. Overview of the TREC 2020 deep learning\\ntrack. arXiv:2102.07662. Retrieved from https://arxiv.org/abs/2102.07662\\n[73] Justin Cui, Wei-Lin Chiang, Ion Stoica, and Cho-Jui Hsieh. 2024. OR-bench: An over-refusal benchmark for large\\nlanguage models. arXiv:2405.20947. Retrieved from https://arxiv.org/abs/2405.20947\\n[74] Shiyao Cui, Zhenyu Zhang, Yilong Chen, Wenyuan Zhang, Tianyun Liu, Siqi Wang, and Tingwen Liu. 2023. Fft:\\nTowards harmlessness evaluation and analysis for LLMS with factuality, fairness, toxicity. arXiv:2311.18580. Retrieved\\nfrom https://arxiv.org/abs/2311.18580\\n[75] Luigi Daniele, and Suphava Deeprasit. 2023. Amplify-Instruct: Synthetically generated diverse multi-turn conversa-\\ntions for efficient LLM training. arXiv Preprint. Retrieved from https://huggingface.co/datasets/LDJnr/Capybara\\n[76] Tri Dao. 2024. FlashAttention-2: Faster attention with better parallelism and work partitioning. In The Twelfth\\nInternational Conference on Learning Representations.\\n[77] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. Flashattention: Fast and memory-efficient\\nexact attention with io-awareness. Adv. Neural Inf. Process. Syst. 35, (2022), 16344–16359.\\n[78] Tri Dao, and Albert Gu. 2024. Transformers are SSMs: Generalized models and efficient algorithms through structured\\nstate space duality. arXiv:2405.21060. Retrieved from https://arxiv.org/abs/2405.21060\\n[79] Rocktim Jyoti Das, Liqun Ma, and Zhiqiang Shen. 2023. Beyond size: How gradients shape pruning decisions in\\nlarge language models. arXiv:2311.04902. Retrieved from https://arxiv.org/abs/2311.04902\\n[80] Anirban Dasgupta, Ravi Kumar, and Tamás Sarlós. 2011. Fast locality-sensitive hashing. In Proceedings of the 17th\\nACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 1073–1081.\\n[81] Pieter Delobelle, Ewoenam Kwaku Tokpo, Toon Calders, and Bettina Berendt. 2022. Measuring fairness with biased\\nrulers: A comparative study on bias metrics for pre-trained language models. In Proceedings of the 2022 Conference\\nof the North American Chapter of the Association for Computational Linguistics. Association for Computational\\nLinguistics, 1693–1706.\\n[82] Yongheng Deng, Ziqing Qiao, Ju Ren, Yang Liu, and Yaoxue Zhang. 2023. Mutual enhancement of large and small\\nlanguage models with cross-silo knowledge transfer. arXiv:2312.05842. Retrieved from https://arxiv.org/abs/2312.\\n05842\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 68}, page_content='145:68\\nF. Wang et al.\\n[83] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. GPT3.int8(): 8-bit Matrix Multiplication for\\nTransformers at Scale. In Advances in Neural Information Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle\\nBelgrave, and Kyunghyun Cho (Eds.). Retrieved from https://openreview.net/forum?id=dXiGWqBoxaD.\\n[84] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2024. Qlora: Efficient finetuning of quantized\\nllms. Adv. Neural Inf. Process. Syst. 36, (2024). DOI: https://doi.org/10.48550/arXiv.2305.14314\\n[85] Tim Dettmers, and Luke Zettlemoyer. 2023. The case for 4-bit precision: K -bit inference scaling laws. In International\\nConference on Machine Learning. PMLR, 7750–7774.\\n[86] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional\\ntransformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human Language Technologies, Vol. 1 (Long and Short Papers). Association\\nfor Computational Linguistics, 4171–4186.\\n[87] Nolan Dey, Gurpreet Gosal, Zhiming Chen, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, and\\nJoel Hestness. 2023. Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale\\nCluster. CoRR abs/2304.03208.\\n[88] Richard Diehl Martinez, Pietro Lesci, and Paula Buttery. 2024. Tending Towards Stability: Convergence Challenges\\nin Small Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2024, Yaser Al-\\nOnaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, 3275–3286. https:\\n//doi.org/10.18653/v1/2024.findings-emnlp.187\\n[89] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen\\nZhou. 2023. Enhancing chat language models by scaling high-quality instructional conversations. arXiv:2305.14233.\\nRetrieved from https://arxiv.org/abs/2305.14233\\n[90] Tinghe Ding. 2024. MobileAgent: Enhancing mobile control via human-machine interaction and SOP integration.\\narXiv:240104124. Retrieved from https://arxiv.org/abs/2401.04124\\n[91] Ricardo Dominguez-Olmedo, Moritz Hardt, and Celestine Mendler-Dünner. 2023. Questioning the survey responses\\nof large language models. arXiv:2306.07951. Retrieved from https://arxiv.org/abs/2306.07951\\n[92] Qian Dong, Yiding Liu, Qingyao Ai, Haitao Li, Shuaiqiang Wang, Yiqun Liu, Dawei Yin, and Shaoping Ma. 2023. I3\\nretriever: Incorporating implicit interaction in pre-trained language models for passage retrieval. In Proceedings of\\nthe 32nd ACM International Conference on Information and Knowledge Management. ACM, 441–451.\\n[93] Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Shih-Yang Liu,\\nMatthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, et al. 2024. Hymba: A hybrid-head architecture for small\\nlanguage models. arXiv:2411.13676. Retrieved from https://arxiv.org/abs/2411.13676\\n[94] Xinrun Du, Zhouliang Yu, Songyang Gao, Ding Pan, Yuyang Cheng, Ziyang Ma, Ruibin Yuan, Xingwei Qu, Jiaheng\\nLiu, Tianyu Zheng, Xinchen Luo, Guorui Zhou, Wenhu Chen, and Ge Zhang. 2024. Chinese tiny LLM: Pretraining a\\nChinese-centric large language model. arXiv:240404167. Retrieved from https://arxiv.org/abs/2404.04167\\n[95] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General\\nlanguage model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the\\nAssociation for Computational Linguistics Vol. 1 Long Papers, Association for Computational Linguistics, 320–335.\\n[96] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil\\nMathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The Llama 3 herd of models. arXiv:2407.21783. Retrieved\\nfrom https://arxiv.org/abs/2407.21783\\n[97] Kazuki Egashira, Mark Vero, Robin Staab, Jingxuan He, and Martin Vechev. 2024. Exploiting LLM quantization.\\narXiv:2405.18137. Retrieved from https://arxiv.org/abs/2405.18137\\n[98] Ronen Eldan, and Yuanzhi Li. 2023. Tinystories: How small can language models be and still speak coherent English?.\\narXiv:2305.07759. Retrieved from https://arxiv.org/abs/2305.07759\\n[99] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. 2018. Sigmoid-weighted linear units for neural network function\\napproximation in reinforcement learning. Neural Netw.: Off. J. Int. Neural Netw. Soc. 107, (2018), 3–11. DOI: https:\\n//doi.org/10.1016/j.neunet.2017.12.012\\n[100] David Esiobu, Xiaoqing Tan, Saghar Hosseini, Megan Ung, Yuchen Zhang, Jude Fernandes, Jane Dwivedi-Yu, Eleonora\\nPresani, Adina Williams, and Eric Smith. 2023. ROBBIE: Robust bias evaluation of large generative language models.\\nIn Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 3764–3814. Retrieved\\nfrom https://aclanthology.org/2023.emnlp-main.230.\\n[101] Hugging Face. 2024. SmolVLM - small yet mighty Vision Language Model. Retrieved from https://huggingface.co/\\nblog/smolvlm. Accessed: November 26, 2024.\\n[102] William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with\\nsimple and efficient sparsity. J. Mach. Learn. Res. 23, 120 (2022), 1–39. DOI: https://doi.org/10.48550/arXiv.2101.03961\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 69}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:69\\n[103] Shangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov. 2023. Knowledge\\ncard: Filling LLMs’ knowledge gaps with plug-in specialized language models. arXiv:2305.09955. Retrieved from\\nhttps://arxiv.org/abs/2305.09955\\n[104] Elias Frantar, and Dan Alistarh. 2023. SparseGPT: Massive language models can be accurately pruned in one-shot. In\\nInternational Conference on Machine Learning. PMLR, 10323–10337.\\n[105] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2023. GPTQ: Accurate post-training quantization\\nfor generative pre-trained transformers. In The Eleventh International Conference on Learning Representations.\\n[106] Hao Fu, Yao Peng, and Tushar Khot. 2022. How does GPT Obtain its Ability? Tracing Emergent Abilities of Language\\nModels to their Sources. Yao Fu’s Notion (Dec. 2022) Retrieved from https://yaofu.notion.site/How-does-GPT-Obtain-\\nits-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1\\n[107] Yao Fu, Hao Peng, and Litu Ou. Ashish sabharwal, and tushar khot. 2023. Specializing smaller language models\\ntowards multi-step reasoning. In International Conference on Machine Learning. PMLR, 10421–10430.\\n[108] Chongming Gao, Shiqi Wang, Shijun Li, Jiawei Chen, Xiangnan He, Wenqiang Lei, Biao Li, Yuan Zhang, and Peng\\nJiang. 2023. CIRS: Bursting filter bubbles by counterfactual interactive recommender system. ACM Trans. Inf. Syst.\\n42, 1 (2023), 1–27. DOI: https://doi.org/10.1145/3594871\\n[109] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish\\nThite, Noa Nabeshima, et al. 2020. The Pile: An 800GB dataset of diverse text for language modeling. arXiv:2101.00027.\\nRetrieved from https://arxiv.org/abs/2101.00027\\n[110] Luyu Gao, and Jamie Callan. 2022. Unsupervised corpus aware language model pre-training for dense passage\\nretrieval. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, Vol. 1 Long\\nPapers, Association for Computational Linguistics, 2843–2853.\\n[111] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding,\\nJeffrey Hsu, Alain Le Noac’h, et al. 2024. The Language Model Evaluation Harness. DOI: https://doi.org/10.5281/\\nzenodo.12608602\\n[112] Shangqian Gao, Chi-Heng Lin, Ting Hua, Zheng Tang, Yilin Shen, Hongxia Jin, and Yen-Chang Hsu. 2024. DISP-LLM:\\nDimension-Independent structural pruning for large language models. In The Thirty-Eighth Annual Conference on\\nNeural Information Processing Systems. Retrieved from https://openreview.net/forum?id=YxaY6tHgg0\\n[113] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. 2024. Model tells you what to discard:\\nAdaptive KV cache compression for LLMs. In The Twelfth International Conference on Learning Representations.\\nRetrieved from https://openreview.net/forum?id=uNrFpDPMyo\\n[114] Alex Gichamba, Tewodros Kederalah Idris, Brian Ebiyau, Eric Nyberg, and Teruko Mitamura. 2024. ColBERT\\nretrieval and ensemble response scoring for language model question answering. arXiv:240810808. Retrieved from\\nhttps://arxiv.org/abs/2408.10808\\n[115] Karan Goel. 2024. The OnDevice Intelligence Update. Retrieved from https://www.cartesia.ai/blog/on-device\\n[116] Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. 2019. Openwebtext corpus.\\n[117] Shreya Goyal, Sumanth Doddapaneni, Mitesh M. Khapra, and Balaraman Ravindran. 2023. A survey of adversarial\\ndefenses and robustness in nlp. Comput. Surveys 55, 14s (2023), 1–39.\\n[118] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha,\\nHamish Ivison, Ian Magnusson, Yizhong Wang, et al. 2024. OLMo: Accelerating the science of language models.\\narXiv:2402.00838. Retrieved from https://arxiv.org/abs/2402.00838\\n[119] Albert Gu, and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. arXiv:2312.00752.\\nRetrieved from https://arxiv.org/abs/2312.00752\\n[120] Naibin Gu, Peng Fu, Xiyu Liu, Bowen Shen, Zheng Lin, and Weiping Wang. 2024. Light-PEFT: Lightening Parameter-\\nEfficient Fine-Tuning via Early Pruning. In Findings of the Association for Computational Linguistics: ACL 2024,\\nLun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, 7528–7541. DOI:\\nhttps://doi.org/10.18653/v1/2024.findings-acl.447\\n[121] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2024. MiniLLM: Knowledge distillation of large language models.\\nIn The Twelfth International Conference on Learning Representations.\\n[122] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan\\nJavaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang,\\nSébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. 2023. Textbooks are all you need.\\narXiv:230611644. Retrieved from https://arxiv.org/abs/2306.11644\\n[123] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, Y. K. Li,\\net al. 2024. DeepSeek-Coder: When the Large Language Model Meets Programming–The Rise of Code Intelligence.\\narXiv preprint arXiv:2401.14196.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 70}, page_content='145:70\\nF. Wang et al.\\n[124] Jinyang Guo, Jianyu Wu, Zining Wang, Jiaheng Liu, Ge Yang, Yifu Ding, Ruihao Gong, Haotong Qin, and Xianglong\\nLiu. 2024. Compressing large language models by joint sparsification and quantization. In Forty-First International\\nConference on Machine Learning.\\n[125] Shangwei Guo, Chunlong Xie, Jiwei Li, Lingjuan Lyu, and Tianwei Zhang. 2022. Threats to pre-trained language\\nmodels: Survey and taxonomy. arXiv preprint arXiv:2202.06862.\\n[126] Song Guo, Jiahang Xu, Li Lyna Zhang, and Mao Yang. 2023. Compresso: Structured pruning with collaborative\\nprompting learns compact large language models. arXiv preprint arXiv:2310.05015.\\n[127] Zhen Guo, Peiqi Wang, Yanwei Wang, and Shangdi Yu. 2023. Improving small language models on PubMedQA via\\ngenerative data augmentation. arXiv:230507804. Retrieved from https://arxiv.org/abs/2305.07804\\n[128] Song Han, Jeff Pool, John Tran, and William Dally. 2015. Learning both weights and connections for efficient neural\\nnetwork. Adv. Neural Inf. Process. Syst., 28 (2015), 1135–1143.\\n[129] Zixu Hao, Huiqiang Jiang, Shiqi Jiang, Ju Ren, and Ting Cao. 2024. Hybrid SLM and LLM for edge-cloud collaborative\\ninference. In Proceedings of the Workshop on Edge and Mobile Foundation Models. ACM, 36–41.\\n[130] Tim Hartill, Diana Benavides-Prado, Michael Witbrock, and Patricia J. Riddle. 2023. Answering unseen questions\\nwith smaller language models using rationale generation and dense retrieval. arXiv:230804711. Retrieved from\\nhttps://arxiv.org/abs/2308.04711\\n[131] Tim Hartill, Neset Tan, Michael Witbrock, and Patricia J. Riddle. 2023. Teaching smaller language models to generalise\\nto unseen compositional questions. arXiv:230800946. Retrieved from https://arxiv.org/abs/2308.00946\\n[132] Kai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng, and Erik Cambria. 2023. A survey of large lan-\\nguage models for healthcare: From data, technology, and applications to accountability and ethics. arXiv:2310.05694.\\nRetrieved from https://arxiv.org/abs/2310.05694\\n[133] Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023. DeBERTaV3: Improving DeBERTa using ELECTRA-Style\\nPre-Training with Gradient-Disentangled embedding sharing. In The Eleventh International Conference on Learning\\nRepresentations. Retrieved from https://openreview.net/forum?id=sE7-XhLxHA\\n[134] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. Deberta: Decoding-enhanced bert with\\ndisentangled attention. arXiv:2006.03654. Retrieved from https://arxiv.org/abs/2006.03654\\n[135] Narges Heidari, Parham Moradi, and Abbas Koochari. 2022. An attention-based deep learning method for solving\\nthe cold-start and sparsity issues of recommender systems. Knowl.-Based Syst. 256 (2022), 109835. DOI: https:\\n//doi.org/10.1016/j.knosys.2022.109835\\n[136] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020.\\nMeasuring massive multitask language understanding. arXiv:2009.03300. Retrieved from https://arxiv.org/abs/2009.\\n03300\\n[137] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021.\\nMeasuring massive multitask language understanding. In International Conference on Learning Representations.\\nRetrieved from https://openreview.net/forum?id=d7KBjmI3GmQ\\n[138] Dan Hendrycks, and Kevin Gimpel. 2016. Gaussian error linear units (gelus). arXiv:1606.08415. Retrieved from\\nhttps://arxiv.org/abs/1606.08415\\n[139] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv:1503.02531.\\nRetrieved from https://arxiv.org/abs/1503.02531\\n[140] Sepp Hochreiter, and Jürgen Schmidhuber. 1996. LSTM can solve hard long time lag problems. Adv. Neural Inf.\\nProcess. Syst. 9, (1996), 473–479. DOI: https://doi.org/10.5555/2998981.2999048\\n[141] Junyuan Hong, Jinhao Duan, Chenhui Zhang, Zhangheng Li, Chulin Xie, Kelsey Lieberman, James Diffenderfer, Brian\\nR. Bartoldson, Ajay Kumar Jaiswal, Kaidi Xu, et al. 2024. Decoding compressed trust: Scrutinizing the trustworthiness\\nof efficient LLMs under compression. In Proceedings of the Forty-First International Conference on Machine Learning,\\nICML. Retrieved from https://openreview.net/forum?id=e3Dpq3WdMv\\n[142] Yutong Meng Yuhao Wang Hongcheng Liu, and Yusheng Liao. 2023. XieZhi: Chinese law large language model.\\nRetrieved from https://github.com/LiuHC0428/LAW_GPT\\n[143] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo,\\nMona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In International Conference\\non Machine Learning. PMLR, 2790–2799.\\n[144] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna,\\nChen-Yu Lee, and Tomas Pfister. 2023. Distilling step-by-step! outperforming larger language models with less\\ntraining data and smaller model sizes. In Findings of the Association for Computational Linguistics: ACL ’23. Association\\nfor Computational Linguistics, 8003–8017.\\n[145] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\n2021. Lora: Low-rank adaptation of large language models. arXiv:2106.09685. Retrieved from https://arxiv.org/abs/\\n2106.09685\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 71}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:71\\n[146] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang,\\nWeilin Zhao, et al. 2024. Minicpm: Unveiling the potential of small language models with scalable training strategies.\\narXiv:2404.06395. Retrieved from https://arxiv.org/abs/2404.06395\\n[147] Xing Hu, Yuan Chen, Dawei Yang, Sifan Zhou, Zhihang Yuan, Jiangyong Yu, and Chen Xu. 2024. I-LLM: Efficient\\ninteger-only inference for fully-quantized low-bit large language models. arXiv:2405.17849. Retrieved from https:\\n//arxiv.org/abs/2405.17849\\n[148] Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2023. Large\\nlanguage models can Self-Improve. In The 2023 Conference on Empirical Methods in Natural Language Processing.\\n[149] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng,\\nXiaocheng Feng, Bing Qin, et al. 2023. A survey on hallucination in large language models: Principles, taxonomy,\\nchallenges, and open questions. arXiv:2311.05232. Retrieved from https://arxiv.org/abs/2311.05232\\n[150] Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong Liu, Michele Magno, and Xiaojuan\\nQi. 2024. Billm: Pushing the limit of post-training quantization for LLMS. arXiv:2402.04291. Retrieved from https:\\n//arxiv.org/abs/2402.04291\\n[151] Wenyu Huang, Guancheng Zhou, Hongru Wang, Pavlos Vougiouklis, Mirella Lapata, and Jeff Z. Pan. 2024. Less is\\nMore: Making Smaller Language Models Competent Subgraph Retrievers for Multi-hop KGQA. In Findings of the\\nAssociation for Computational Linguistics: EMNLP 2024, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.).\\nAssociation for Computational Linguistics, 15787–15803. https://doi.org/10.18653/v1/2024.findings-emnlp.927\\n[152] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv,\\nYikai Zhang, Yao Fu, et al. 2024. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models.\\nAdv. Neural Inf. Process. Syst. 36 (2024), 62991–63010.\\n[153] Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. 2023. Catastrophic jailbreak of open-source\\nLLMs via exploiting generation. arXiv:2310.06987. Retrieved from https://arxiv.org/abs/2310.06987\\n[154] Yuheng Huang, Jiayang Song, Zhijie Wang, Shengming Zhao, Huaming Chen, Felix Juefei-Xu, and Lei Ma. 2023. Look\\nbefore you leap: An exploratory study of uncertainty measurement for large language models. arXiv:2307.10236.\\nRetrieved from https://arxiv.org/abs/2307.10236\\n[155] Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. 2020. Poly-encoders: Architectures and\\npre-training strategies for fast and accurate multi-sentence scoring. In International Conference on Learning Repre-\\nsentations.\\n[156] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing\\nHu, Brian Fuller, Davide Testuggine, et al. 2023. Llama guard: LLM-based input-output safeguard for human-AI\\nconversations. arXiv:2312.06674. Retrieved from https://arxiv.org/abs/2312.06674\\n[157] Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. 1991. Adaptive mixtures of local\\nexperts. Neural Comput. 3, 1 (1991), 79–87.\\n[158] Mojan Javaheripi, Sébastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio César Teodoro Mendes,\\nWeizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth Gopi, et al. 2023. Phi-2: The surprising power of small\\nlanguage models. In Microsoft Research Blog.\\n[159] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Hwang, and Jong C. Park. 2023. Test-Time Self-Adaptive small\\nlanguage models for question answering. In Findings of the Association for Computational Linguistics: EMNLP ’23.\\nAssociation for Computational Linguistics, 15459–15469.\\n[160] Ananya Harsh Jha, Tom Sherborne, Evan Pete Walsh, Dirk Groeneveld, Emma Strubell, and Iz Beltagy. 2024. Just\\nCHOP: Embarrassingly simple LLM compression. arXiv:230514864. Retrieved from https://arxiv.org/abs/2305.14864\\n[161] Yixin Ji, Yang Xiang, Juntao Li, Wei Chen, Zhongyi Liu, Kehai Chen, and Min Zhang. 2024. Feature-based low-\\nrank compression of large language models via Bayesian optimization. arXiv:2405.10616. Retrieved from https:\\n//arxiv.org/abs/2405.10616\\n[162] Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. 2023. Towards mitigating LLM halluci-\\nnation via self reflection. In Findings of the Association for Computational Linguistics: EMNLP ’23. Association for\\nComputational Linguistics, 1827–1843.\\n[163] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las\\nCasas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7B. arXiv:2310.06825.\\nRetrieved from https://arxiv.org/abs/2310.06825\\n[164] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, De-\\nvendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts.\\narXiv:2401.04088. Retrieved from https://arxiv.org/abs/2401.04088\\n[165] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2024. LongLLM-\\nLingua: Accelerating and enhancing LLMs in long context scenarios via prompt compression. In ICLR 2024 Workshop\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 72}, page_content='145:72\\nF. Wang et al.\\non Mathematical and Empirical Understanding of Foundation Models. Retrieved from https://openreview.net/forum?\\nid=9YvfRrpmyw\\n[166] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. PubMedQA: A dataset for\\nbiomedical research question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural\\nLanguage Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),\\n2567–2577.\\n[167] Rudolph Emil Kalman. 1960. A new approach to linear filtering and prediction problems. Trans. ASME, D 82 (1960),\\n35–44.\\n[168] Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, and Tuo Zhao. 2024. Gear:\\nAn efficient KV cache compression recipefor near-lossless generative inference of LLM. arXiv:2403.05527. Retrieved\\nfrom https://arxiv.org/abs/2403.05527\\n[169] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec\\nRadford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv:2001.08361. Retrieved\\nfrom https://arxiv.org/abs/2001.08361\\n[170] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful\\nHaq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, et al. 2023. DSPy: Compiling declarative language model\\ncalls into self-improving pipelines. arXiv:2310.03714. Retrieved from https://arxiv.org/abs/2310.03714\\n[171] Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, and Dongsoo Lee.\\n2023. Memory-efficient fine-tuning of compressed large language models via Sub-4-bit integer quantization. Adv.\\nNeural Inf. Process. Syst. 36 (2023), 36187–36207.\\n[172] Minsoo Kim, Sihwa Lee, Janghwan Lee, Sukjin Hong, Du-Seong Chang, Wonyong Sung, and Jungwook Choi. 2023.\\nToken-scaled logit distillation for ternary weight generative language models. Adv. Neural Inf. Process. Syst. 36 (2023),\\n42097–42118.\\n[173] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W. Mahoney, and Kurt\\nKeutzer. 2023. Squeezellm: Dense-and-sparse quantization. arXiv:2306.07629. Retrieved from https://arxiv.org/abs/\\n2306.07629\\n[174] Yoon Kim, and Alexander M. Rush. 2016. Sequence-Level knowledge distillation. In Proceedings of the 2016 Conference\\non Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 1317–1327.\\n[175] Young Jin Kim, Raffy Fahim, and Hany Hassan Awadalla. 2023. Mixture of quantized experts (MoQE): Complementary\\neffect of low-bit quantization and robustness. arXiv:2310.02410. Retrieved from https://arxiv.org/abs/2310.02410\\n[176] Jongwoo Ko, Sungnyun Kim, Tianyi Chen, and Se-Young Yun. 2024. DistiLLM: Towards streamlined distillation for\\nlarge language models. arXiv:2402.03898. Retrieved from https://arxiv.org/abs/2402.03898\\n[177] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite,\\nMargaret Mitchell, Sean Hughes, Thomas Wolf, et al. 2022. The stack: 3 TB of permissively licensed source code.\\narXiv:2211.15533. Retrieved from https://arxiv.org/abs/2211.15533\\n[178] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for uncertainty\\nestimation in natural language generation. In Proceedings of the Eleventh International Conference on Learning\\nRepresentations. Retrieved from https://openreview.net/forum?id=VD-AYtP0dve\\n[179] Divyanshu Kumar, Anurakt Kumar, Sahil Agarwal, and Prashanth Harshangi. 2024. Fine-tuning, quantization, and\\nLLMs: Navigating unintended outcomes. arXiv:2404.04392. Retrieved from https://arxiv.org/abs/2404.04392\\n[180] Ohjoon Kwon, Donghyeon Jeon, Nayoung Choi, Gyu-Hwung Cho, Hwiyeol Jo, Changbong Kim, Hyunwoo Lee,\\nInho Kang, Sun Kim, and Taiwoo Park. 2024. SLM as guardian: Pioneering AI safety with small language model.\\nIn Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, Franck\\nDernoncourt, Daniel Preoţiuc-Pietro, and Anastasia Shimorina (Eds.). Association for Computational Linguistics,\\n1333–1350. DOI: https://doi.org/10.18653/v1/2024.emnlp-industry.99\\n[181] Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, and Richard Dufour. 2024.\\nBiomistral: A collection of open-source pretrained large language models for medical domains. arXiv:2402.10373.\\nRetrieved from https://arxiv.org/abs/2402.10373\\n[182] Stefan Larson, Anish Mahendran, Joseph J. Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan K. Kum-\\nmerfeld, Kevin Leach, Michael A. Laurenzano, Lingjia Tang, and Jason Mars. 2019. An Evaluation Dataset for Intent\\nClassification and Out-of-Scope Prediction. In Emnlp-Ijcnlp 2019, Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun\\nWan (Eds.). Association for Computational Linguistics, 1311–1316. DOI: https://doi.org/10.18653/v1/D19-1131\\n[183] Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao,\\nLeandro Von Werra, Chenghao Mou, Eduardo González Ponferrada, Huu Nguyen, et al. 2022. The bigscience roots\\ncorpus: A 1.6 tb composite multilingual dataset. Adv. Neural Inf. Process. Syst. 35 (2022), 31809–31826.\\n[184] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra\\nSasha Luccioni, François Yvon, Matthias Gallé, et al. 2023. Bloom: A 176b-parameter open-access multilingual language\\nmodel. arxiv:2211.05100. Retrieved from https://arxiv.org/abs/2211.05100\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 73}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:73\\n[185] Hojae Lee, Junho Kim, and SangKeun Lee. 2024. Mentor-KD: Making small language models better multi-step\\nreasoners. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, Yaser Al-\\nOnaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, 17643–17658. DOI:\\nhttps://doi.org/10.18653/v1/2024.emnlp-main.977\\n[186] Jooyoung Lee, Fan Yang, Thanh Tran, Qian Hu, Emre Barut, and Kai-Wei Chang. 2024. Can small language models\\nhelp large language models reason better?: LM-Guided chain-of-thought. In Proceedings of the 2024 Joint International\\nConference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING ’24), 2835–2843.\\n[187] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu,\\nJieru Hu, Marta Tintore, Susan Zhang, et al. 2022. xFormers: A modular and hackable transformer modelling library.\\nRetrieved from https://github.com/facebookresearch/xformers\\n[188] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer normalization. arXiv:1607.06450. Retrieved\\nfrom https://arxiv.org/abs/1607.06450v1\\n[189] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose\\nSlone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with\\nlanguage models. Adv. Neural Inf. Process. Syst. 35, (2022), 3843–3857.\\n[190] Chenglin Li, Qianglong Chen, Liangyue Li, Caiyu Wang, Yicheng Li, Zulong Chen, and Yin Zhang. 2023. Mixed\\ndistillation helps smaller language model better reasoning. arXiv:2312.10730. Retrieved from https://arxiv.org/abs/\\n2312.10730\\n[191] Guangyan Li, Yongqiang Tang, and Wensheng Zhang. 2024. LoRAP: Transformer sub-layers deserve differentiated\\nstructured compression for large language models. arXiv:2404.09695. Retrieved from https://arxiv.org/abs/2404.09695\\n[192] Haitao Li, Qingyao Ai, Jia Chen, Qian Dong, Zhijing Wu, Yiqun Liu, Chong Chen, and Qi Tian. 2024. BLADE:\\nEnhancing black-box large language models with small domain-specific models. arXiv:2403.18365. Retrieved from\\nhttps://arxiv.org/abs/2403.18365\\n[193] Haoran Li, Dadi Guo, Donghao Li, Wei Fan, Qi Hu, Xin Liu, Chunkit Chan, Duanyi Yao, Yuan Yao, and Yangqiu Song.\\n2024. PrivLM-Bench: A multi-level privacy evaluation benchmark for language models. In Proceedings of the 62nd\\nAnnual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 54–73.\\n[194] Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. HaluEval: A Large-Scale hallucination\\nevaluation benchmark for large language models. In Proceedings of the 2023 Conference on Empirical Methods in\\nNatural Language Processing. Association for Computational Linguistics, 6449–6464. DOI: https://doi.org/10.18653/\\nv1/2023.emnlp-main.397\\n[195] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick\\nKeh, Kushal Arora, et al. 2024. DataComp-LM: In search of the next generation of training sets for language models.\\narXiv:2406.11794. Retrieved from https://arxiv.org/abs/2406.11794\\n[196] Luchang Li, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, and Qin Xie. 2024. Transformer-lite: high-efficiency deploy-\\nment of large language models on mobile phone GPUs. arXiv:2403.20041. Retrieved from https://arxiv.org/abs/2403.\\n20041\\n[197] Pingzhi Li, Xiaolong Jin, Yu Cheng, and Tianlong Chen. 2024. Examining post-training quantization for mixture-of-\\nexperts: A benchmark. arXiv:2406.08155. Retrieved from https://arxiv.org/abs/2406.08155\\n[198] Quan Li, Tianxiang Zhao, Lingwei Chen, Junjie Xu, and Suhang Wang. 2024. Enhancing graph neural networks with\\nlimited labeled data by actively distilling knowledge from large language models. arXiv:2407.13989. Retrieved from\\nhttps://arxiv.org/abs/2407.13989\\n[199] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone,\\nChristopher Akiki, Jia Li, Jenny Chim, et al. 2023. StarCoder: May the source be with you! arXiv:2305.06161. Retrieved\\nfrom https://arxiv.org/abs/2305.06161\\n[200] Shengrui Li, Xueting Han, and Jing Bai. 2024. Nuteprune: Efficient progressive pruning with numerous teachers for\\nlarge language models. arXiv:2402.09773. Retrieved from https://arxiv.org/abs/2402.09773\\n[201] Tianlin Li, Qian Liu, Tianyu Pang, Chao Du, Qing Guo, Yang Liu, and Min Lin. 2024. Purifying large language models\\nby ensembling a small language model. arXiv:2402.14845. Retrieved from https://arxiv.org/abs/2402.14845\\n[202] Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori B. Hashimoto, Luke Zettlemoyer,\\nand Mike Lewis. 2023. Contrastive decoding: Open-ended text generation as optimization. In Proceedings of the 61st\\nAnnual Meeting of the Association for Computational Linguistics, Vol. 1 Long Papers, Association for Computational\\nLinguistics, 12286–12312.\\n[203] Xiang Lisa Li, and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. arXiv:2101.00190.\\nRetrieved from https://arxiv.org/abs/2101.00190\\n[204] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023. Textbooks\\nare all you need II: Phi-1.5 Technical Report. arXiv:2309.05463. Retrieved from https://arxiv.org/abs/2309.05463\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 74}, page_content='145:74\\nF. Wang et al.\\n[205] Yun Li, Lin Niu, Xipeng Zhang, Kai Liu, Jianchen Zhu, and Zhanhui Kang. 2023. E-Sparse: Boosting the large language\\nmodel inference through entropy-based N:M sparsity. arXiv:2310.15929. Retrieved from https://arxiv.org/abs/2310.\\n15929\\n[206] Yinheng Li, Shaofei Wang, Han Ding, and Hang Chen. 2023. Large language models in finance: A survey. In Proceedings\\nof the Fourth ACM International Conference on AI in Finance, 374–382.\\n[207] Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet, and Vong, Teknium. 2023.\\nSlimOrca: An Open Dataset of GPT-4 Augmented FLAN Reasoning Traces, with Verification. Retrieved from\\nhttps://https://huggingface.co/Open-Orca/SlimOrca\\n[208] Jinggui Liang, Lizi Liao, Hao Fei, and Jing Jiang. 2024. Synergizing large language models and Pre-Trained smaller\\nmodels for conversational intent discovery. In Findings of the Association for Computational Linguistics ACL ’24.\\nAssociation for Computational Linguistics, 14133–14147.\\n[209] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak\\nNarayanan, Yuhuai Wu, Ananya Kumar, et al. 2023. Holistic evaluation of language models. In TMLR ’23. Retrieved\\nfrom https://openreview.net/forum?id=iO4LZibEqW\\n[210] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom,\\nYonatan Belinkov, Shai Shalev-Shwartz, et al. 2024. Jamba: A hybrid transformer-Mamba language model.\\narXiv:2403.19887. Retrieved from https://arxiv.org/abs/2403.19887\\n[211] Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, and Weinan\\nZhang. 2024. Rella: Retrieval-enhanced large language models for lifelong sequential behavior comprehension in\\nrecommendation. In Proceedings of the ACM on Web Conference 2024, 3497–3508.\\n[212] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang,\\nChuang Gan, and Song Han. 2024. AWQ: Activation-aware weight quantization for on-device LLM compression and\\nacceleration. Proc. Mach. Learn. Syst. 6 (2024), 87–100.\\n[213] Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring how models mimic human falsehoods.\\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, Vol. 1 Long Papers,\\nAssociation for Computational Linguistics, 3214–3252.\\n[214] Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman\\nGoyal, Shruti Bhosale, Jingfei Du, et al. 2022. Few-shot learning with multilingual generative language models.\\nIn EMNLP-Main 2022, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational\\nLinguistics, 9019–9052. DOI: https://doi.org/10.18653/v1/2022.emnlp-main.616\\n[215] Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan\\nDuan, et al. 2024. Rho-1: Not all tokens are what you need. arXiv:2404.07965. Retrieved from https://arxiv.org/abs/\\n2404.07965\\n[216] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai\\nDai, Daya Guo, et al. 2024. DeepSeek-v2: A strong, economical, and efficient mixture-of-experts language model.\\narXiv:2405.04434. Retrieved from https://arxiv.org/abs/2405.04434\\n[217] Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, and Noah A. Smith. 2024. Tuning language\\nmodels by proxy. arXiv:2401.08565. Retrieved from https://arxiv.org/abs/2401.08565\\n[218] Jiashuo Liu, Zheyan Shen, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. 2021. Towards out-of-\\ndistribution generalization: A survey. arXiv:2108.13624. Retrieved from https://arxiv.org/abs/2108.13624\\n[219] Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. 2024. Once: Boosting content-based recommendation\\nwith both open-and closed-source large language models. In Proceedings of the 17th ACM International Conference on\\nWeb Search and Data Mining. ACM, 452–461.\\n[220] Suqing Liu, Zezhu Yu, Feiran Huang, Yousef Bulbulia, Andreas Bergen, and Michael Liut. 2024. Can small language\\nmodels with retrieval-augmented generation replace large language models when learning computer science?. In\\nProceedings of the 2024 on Innovation and Technology in Computer Science Education, Vol. 1, ACM, 388–393.\\n[221] Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. 2023. What makes good data for alignment? a\\ncomprehensive study of automatic data selection in instruction tuning. arXiv:2312.15685. Retrieved from https:\\n//arxiv.org/abs/2312.15685\\n[222] Yinpeng Liu, Jiawei Liu, Xiang Shi, Qikai Cheng, and Wei Lu. 2024. Let’s learn step by step: Enhancing in-context\\nlearning ability with curriculum learning. arXiv:2402.10738. Retrieved from https://arxiv.org/abs/2402.10738\\n[223] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer,\\nand Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. arXiv:1907.11692. Retrieved\\nfrom https://arxiv.org/abs/1907.11692\\n[224] Yanming Liu, Xinyue Peng, Xuhong Zhang, Weihao Liu, Jianwei Yin, Jiannan Cao, and Tianyu Du. 2024. RA-ISF:\\nLearning to answer and understand from retrieval augmentation via iterative self-feedback. arXiv:2403.06840.\\nRetrieved from https://arxiv.org/abs/2403.06840\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 75}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:75\\n[225] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and\\nAnshumali Shrivastava. 2023. Scissorhands: Exploiting the persistence of importance hypothesis for LLM KV cache\\ncompression at test time. Adv. Neural Inf. Process. Syst. 36 (2023), 52342–52364.\\n[226] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman\\nKrishnamoorthi, and Vikas Chandra. 2023. LLM-QAT: Data-free quantization aware training for large language\\nmodels. arXiv:2305.17888. Retrieved from https://arxiv.org/abs/2305.17888\\n[227] Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang,\\nYangyang Shi, Raghuraman Krishnamoorthi, et al. 2024. Mobilellm: Optimizing sub-billion parameter language\\nmodels for on-device use cases. arXiv:2402.14905. Retrieved from https://arxiv.org/abs/2402.14905\\n[228] Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao Ding, Gang Chen, and Haobo Wang. 2024. On LLMs-Driven\\nsynthetic data generation, curation, and evaluation: A survey. In Findings of the Association for Computational\\nLinguistics ACL ’24. Association for Computational Linguistics, 11065–11082.\\n[229] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret\\nZoph, Jason Wei, et al. 2023. The flan collection: Designing data and methods for effective instruction tuning. In\\nInternational Conference on Machine Learning. PMLR, 22631–22648.\\n[230] Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William Brannon, Niklas\\nMuennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, et al. 2023. The data provenance initiative: A large scale\\naudit of dataset licensing & attribution in AI. arXiv:2310.16787. Retrieved from https://arxiv.org/abs/2310.16787\\n[231] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang,\\nDmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. 2024. StarCoder 2 and the stack v2: The next generation.\\narXiv:2402.19173. Retrieved from https://arxiv.org/abs/2402.19173\\n[232] Wenhao Lu, Jian Jiao, and Ruofei Zhang. 2020. Twinbert: Distilling knowledge to twin-structured compressed bert\\nmodels for large-scale retrieval. In Proceedings of the 29th ACM International Conference on Information & Knowledge\\nManagement. ACM, 2645–2652.\\n[233] Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas D. Lane, and Mengwei\\nXu. 2024. Small language models: Survey, measurements, and insights. arXiv:2409.15790. Retrieved from https:\\n//arxiv.org/abs/2409.15790\\n[234] Haitong Luo, Xuying Meng, Suhang Wang, Tianxiang Zhao, Fali Wang, Hanyun Cao, and Yujun Zhang. 2024. Enhance\\ngraph alignment for large language models. arXiv:2410.11370. Retrieved from https://arxiv.org/abs/2410.11370\\n[235] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. 2022. BioGPT: Generative\\npre-trained transformer for biomedical text generation and mining. Briefings in Bioinformatics 23, 6 (2022). bbac409.\\n[236] Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang,\\nJilong Xue, and Furu Wei. 2024. The era of 1-bit LLMS: All large language models are in 1.58 bits. arXiv:2402.17764.\\nRetrieved from https://arxiv.org/abs/2402.17764\\n[237] Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. Llm-pruner: On the structural pruning of large language models.\\nAdv. Neural Inf. Process. Syst. 36 (2023), 21702–21720.\\n[238] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query rewriting in Retrieval-Augmented\\nlarge language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing.\\nAssociation for Computational Linguistics, 5303–5315.\\n[239] Yubo Ma, Yixin Cao, YongChing Hong, and Aixin Sun. 2023. Large language model is not a good few-shot information\\nextractor, but a good reranker for hard samples! arXiv:2303.08559. Retrieved from https://arxiv.org/abs/2303.08559\\n[240] Yuhan Ma, Chenyou Fan, and Haiqi Jiang. 2023. Sci-cot: Leveraging large language models for enhanced knowledge\\ndistillation in small models for scientific QA. In 2023 9th International Conference on Computer and Communications\\n(ICCC). IEEE, 2394–2398.\\n[241] Yingwei Ma, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, and Shanshan Li. 2024. At which training\\nstage does code data help LLMs reasoning? In The Twelfth International Conference on Learning Representations.\\nRetrieved from https://openreview.net/forum?id=KIPJKST4gw\\n[242] Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya Harsh Jha, Oyvind Tafjord, Dustin\\nSchwenk, Evan Pete Walsh, Yanai Elazar, Kyle Lo, et al. 2023. Paloma: A benchmark for evaluating language model\\nfit. arXiv:2312.10523. Retrieved from https://arxiv.org/abs/2312.10523\\n[243] Dakota Mahan, Ryan Carlow, Louis Castricato, Nathan Cooper, and Christian Laforte. [n.\\u2009d.] Stable beluga models.\\nRetrieved from https://huggingface.co/stabilityai/StableBeluga2\\n[244] Vladimir Malinovskii, Denis Mazur, Ivan Ilin, Denis Kuznedelev, Konstantin Burlachenko, Kai Yi, Dan Alistarh, and Pe-\\nter Richtarik. 2024. PV-tuning: Beyond straight-through estimation for extreme LLM compression. arXiv:2405.14852.\\nRetrieved from https://arxiv.org/abs/2405.14852\\n[245] Potsawee Manakul, Adian Liusie, and Mark Gales. 2023. SelfCheckGPT: Zero-Resource Black-Box hallucination\\ndetection for generative large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 76}, page_content='145:76\\nF. Wang et al.\\nLanguage Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics,\\n9004–9017. DOI: https://doi.org/10.18653/v1/2023.emnlp-main.557\\n[246] Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Seyed Iman\\nMirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, et al. 2024. OpenELM: An efficient language model\\nfamily with open training and inference framework. In Workshop on Efficient Systems for Foundation Models II (ICML\\n’24).\\n[247] Dheeraj Mekala, Alex Nguyen, and Jingbo Shang. 2024. Smaller language models are capable of selecting instruction-\\ntuning training data for larger language models. arXiv:2402.10430. Retrieved from https://arxiv.org/abs/2402.10430\\n[248] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen.\\n2024. ShortGPT: Layers in large language models are more redundant than you expect. arXiv:2403.03853. Retrieved\\nfrom https://arxiv.org/abs/2403.03853\\n[249] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models.\\narXiv:1609.07843. Retrieved from https://arxiv.org/abs/1609.07843\\n[250] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-Tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer,\\nand Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text\\ngeneration. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda\\nBouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 12076–12100. https://doi.org/\\n10.18653/v1/2023.emnlp-main.741\\n[251] Go Min-Su. 2024. Deep Learning Bible - 8. Large Language Models. WikiDocs. Retrieved from https://wikidocs.net/\\n237419\\n[252] Eric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn, and Christopher D. Manning. 2024. An emulator for\\nfine-tuning large language models using small language models. In The Twelfth International Conference on Learning\\nRepresentations. Retrieved from https://openreview.net/forum?id=Eo7kv0sllr\\n[253] Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi Chen,\\nAnastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, et al. 2023. Orca 2: Teaching small language models how to\\nreason. arXiv:2311.11045. Retrieved from https://arxiv.org/abs/2311.11045\\n[254] Lingbo Mo, Boshi Wang, Muhao Chen, and Huan Sun. 2024. How trustworthy are Open-Source LLMs? An assessment\\nunder malicious demonstrations shows their vulnerabilities. In Proceedings of the 2024 Conference of the North\\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol. 1 Long Papers,\\nAssociation for Computational Linguistics, 2775–2792.\\n[255] John Xavier Morris, Wenting Zhao, Justin T. Chiu, Vitaly Shmatikov, and Alexander M. Rush. 2024. Language model\\ninversion. In The Twelfth International Conference on Learning Representations. Retrieved from https://openreview.\\nnet/forum?id=t9dWHpGkPj\\n[256] Maximilian Mozes, Xuanli He, Bennett Kleinberg, and Lewis D. Griffin. 2023. Use of LLMS for illicit purposes: Threats,\\nprevention measures, and vulnerabilities. arXiv:2308.12833. Retrieved from https://arxiv.org/abs/2308.12833\\n[257] Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Evan Pete\\nWalsh, Oyvind Tafjord, Nathan Lambert, et al. 2025. OLMoE: Open mixture-of-experts language models. In The\\nThirteenth International Conference on Learning Representations. Retrieved from https://openreview.net/forum?id=\\nxXTkbTBmqq\\n[258] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah.\\n2023. Orca: Progressive learning from complex explanation traces of GPT-4. arXiv:2306.02707. Retrieved from\\nhttps://arxiv.org/abs/2306.02707\\n[259] Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Moham-\\nmad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. 2024. Compact language models via pruning and\\nknowledge distillation. arXiv:2407.14679. Retrieved from https://arxiv.org/abs/2407.14679\\n[260] Rithesh Murthy, Liangwei Yang, Juntao Tan, Tulika Manoj Awalgaonkar, Yilun Zhou, Shelby Heinecke, Sachin\\nDesai, Jason Wu, Ran Xu, Sarah Tan, Jianguo Zhang, Zhiwei Liu, Shirley Kokane, Zuxin Liu, Ming Zhu, Huan Wang,\\nCaiming Xiong, and Silvio Savarese. 2024. MobileAIBench: Benchmarking LLMs and LMMs for on-device use cases.\\narXiv:240610290. Retrieved from https://arxiv.org/abs/2406.10290\\n[261] Kalyan Nakka, Jimmy Dani, and Nitesh Saxena. 2024. Is on-device AI broken and exploitable? Assessing the trust\\nand ethics in small language models. arXiv:2406.05364. Retrieved from https://arxiv.org/abs/2406.05364\\n[262] Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, and Brad Myers. 2024. Using an LLM to help\\nwith code understanding. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering.\\nACM, 1–13.\\n[263] Piotr Nawrot, Adrian Łańcucki, Marcin Chochowski, David Tarjan, and Edoardo Ponti. 2024. Dynamic memory\\ncompression: Retrofitting LLMs for accelerated inference. In Forty-First International Conference on Machine Learning.\\nRetrieved from https://openreview.net/forum?id=tDRYrAkOB7\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 77}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:77\\n[264] Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi,\\nand Thien Huu Nguyen. 2024. CulturaX: A cleaned, enormous, and multilingual dataset for large language models\\nin 167 languages. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language\\nResources and Evaluation (LREC-COLING ’24). Association for Computational Linguistics, 4226–4237.\\n[265] Tuan Dung Nguyen, Yuan-Sen Ting, Ioana Ciuca, Charles O’Neill, Ze-Chang Sun, Maja Jabłońska, Sandor Kruk,\\nErnest Perkowski, Jack Miller, Jason Jason Jingsh Li, et al. 2023. AstroLLaMA: Towards specialized foundation\\nmodels in astronomy. In Proceedings of the Second Workshop on Information Extraction from Scientific Publications.\\nAssociation for Computational Linguistics, 49–55.\\n[266] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall,\\nMing-Wei Chang, and Yinfei Yang. 2022. Large dual encoders are generalizable retrievers. In Proceedings of the\\n2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,\\n9844–9855. DOI: https://doi.org/10.18653/v1/2022.emnlp-main.669\\n[267] Rodrigo Nogueira, and Kyunghyun Cho. 2019. Passage re-ranking with BERT. arXiv:1901.04085. Retrieved from\\nhttps://arxiv.org/abs/1901.04085\\n[268] A. Noorian. 2024. A BERT-based sequential POI recommender system in social media. Comput. Stand. Interf . 87\\n(2024), 103766. DOI: https://doi.org/10.1016/j.csi.2023.103766\\n[269] OpenAI. 2024. GPT-4o mini: Advancing cost-efficient intelligence. Retrieved from https://openai.com/index/gpt-4o-\\nmini-advancing-cost-efficient-intelligence/. Accessed: July 18, 2024.\\n[270] OpenAI. 2024. Hello GPT-4o. Retrieved from https://openai.com/index/hello-gpt-4o/. Accessed: May 13, 2024.\\n[271] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\\nAgarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback.\\nAdv. Neural Inf. Process. Syst. 35, (2022), 27730–27744.\\n[272] Shankar Padmanabhan, Yasumasa Onoe, Michael Zhang, Greg Durrett, and Eunsol Choi. 2023. Propagating knowledge\\nupdates to lms through distillation. Adv. Neural Inf. Process. Syst. 36 (2023), 47124–47142.\\n[273] Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subramanian, Dan Su, Chen\\nZhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush Dattagupta, et al. 2024. Nemotron-4 15B Technical Report.\\narXiv:2402.16819. Retrieved from https://arxiv.org/abs/2402.16819\\n[274] Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. 2024. OpenWebMath: An open dataset of\\nHigh-Quality mathematical web text. In The Twelfth International Conference on Learning Representations. Retrieved\\nfrom https://openreview.net/forum?id=jKHmjlpViu\\n[275] Guilherme Penedo, Hynek Kydlíček, Loubna Ben Allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro\\nVon Werra, and Thomas Wolf. 2024. The fineweb datasets: Decanting the web for the finest text data at scale.\\narXiv:2406.17557. Retrieved from https://arxiv.org/abs/2406.17557\\n[276] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobei-\\ndli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb dataset for Falcon LLM:\\nOutperforming curated corpora with web data, and web data only. arXiv:2306.01116. Retrieved from https:\\n//arxiv.org/abs/2306.01116\\n[277] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng,\\nMichael Chung, Leon Derczynski, et al. 2023. RWKV: Reinventing RNNs for the Transformer Era. In Findings of the\\nAssociation for Computational Linguistics: EMNLP 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association\\nfor Computational Linguistics, 14048–14077. DOI: https://doi.org/10.18653/v1/2023.findings-emnlp.936\\n[278] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with GPT-4.\\narXiv:2304.03277. Retrieved from https://arxiv.org/abs/2304.03277\\n[279] Zhiyuan Peng, Xuyang Wu, Qifan Wang, and Yi Fang. 2023. Soft prompt tuning for augmenting dense retrieval with\\nlarge language models. arXiv:2307.08303. Retrieved from https://arxiv.org/abs/2307.08303\\n[280] Ethan Perez, Sam Ringer, Kamile Lukosiute, Karina Nguyen, Edwin Chenandet al. 2023. Discovering language\\nmodel behaviors with Model-Written evaluations. In Findings of ACL ’23. Association for Computational Linguistics,\\n13387–13434.\\n[281] Pascal Pfeiffer, Philipp Singer, and Yauhen Babakhin, Gabor Fodor, Nischay Dhankhar, and Sri Satish Ambati. 2024.\\nH2O-Danube3 Technical Report. arXiv:2407.09276. Retrieved from https://arxiv.org/abs/2407.09276\\n[282] Karmvir Singh Phogat, Sai Akhil Puranam, Sridhar Dasaratha, Chetan Harsha, and Shashishekar Ramakrishna.\\n2024. Fine-tuning Smaller Language Models for Question Answering over Financial Documents. In Findings of the\\nAssociation for Computational Linguistics: EMNLP 2024, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.).\\nAssociation for Computational Linguistics, 10528–10548. https://doi.org/10.18653/v1/2024.findings-emnlp.617\\n[283] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao,\\nShivani Agrawal, and Jeff Dean. 2023. Efficiently scaling transformer inference. Proc. Mach. Learn. Syst. 5, 606–624.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 78}, page_content='145:78\\nF. Wang et al.\\n[284] Ofir Press, Noah Smith, and Mike Lewis. 2022. Train short, test long: Attention with linear biases enables input\\nlength extrapolation. In International Conference on Learning Representations. Retrieved from https://openreview.\\nnet/forum?id=R8sQPpGCv0\\n[285] Ruiyang Qin, Jun Xia, Zhenge Jia, Meng Jiang, Ahmed Abbasi, Peipei Zhou, Jingtong Hu, and Yiyu Shi. 2023. Enabling\\non-device large language model personalization with self-supervised data selection and synthesis. arXiv:2311.12275.\\nRetrieved from https://arxiv.org/abs/2311.12275\\n[286] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et\\nal. 2024. ToolLLM: Facilitating large language models to master 16000+ real-world APIs. In The Twelfth International\\nConference on Learning Representations.\\n[287] Haohao Qu, Liangbo Ning, Rui An, Wenqi Fan, Tyler Derr, Hui Liu, Xin Xu, and Qing Li. 2024. A survey of Mamba.\\narXiv:2408.01129. Retrieved from https://arxiv.org/abs/2408.01129\\n[288] Haohao Qu, Yifeng Zhang, Liangbo Ning, Wenqi Fan, and Qing Li. 2024. SSD4Rec: A structured state space duality\\nmodel for efficient sequential recommendation. arXiv:2409.01192. Retrieved from https://arxiv.org/abs/2409.01192\\n[289] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are\\nunsupervised multitask learners. OpenAI Blog 1, 8 (2019), 9.\\n[290] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. 2023.\\nDirect preference optimization: Your language model is secretly a reward model. Adv. Neural Inf. Process. Syst. 36,\\n(2023), 53728–53741.\\n[291] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\\nPeter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn.\\nRes. 21, 140 (2020), 1–67. DOI: https://doi.org/10.5555/3455716.3455856\\n[292] Mohammad Wali Ur Rahman, Murad Mehrab Abrar, Hunter Gibbons Copening, Salim Hariri, Sicong Shao,\\nPratik Satam, and Soheil Salehi. 2023. Quantized transformer language model implementations on edge devices.\\narXiv:2310.03971. Retrieved from https://arxiv.org/abs/2310.03971\\n[293] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward\\ntraining trillion parameter models. In SC20: International Conference for High Performance Computing, Networking,\\nStorage and Analysis. IEEE, 1–16.\\n[294] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.\\n2023. In-Context Retrieval-Augmented language models. Trans. Assoc. Comput. Linguist. 11, (2023), 1316–1331. DOI:\\nhttps://doi.org/10.1162/tacl_a_00605\\n[295] Krithika Ramesh, Arnav Chavan, Shrey Pandit, and Sunayana Sitaram. 2023. A comparative study on the impact\\nof model compression techniques on fairness in language models. In Proceedings of the 61st Annual Meeting of the\\nAssociation for Computational Linguistics, Vol. 1 Long Papers, Association for Computational Linguistics, 15762–15782.\\nRetrieved from https://aclanthology.org/2023.acl-long.878\\n[296] Al Mamunur Rashid, George Karypis, and John Riedl. 2008. Learning preferences of new users in recommender\\nsystems: An information theoretic approach. ACM SIGKDD Explor. Newsl. 10, 2 (2008), 90–100. DOI: https://doi.org/\\n10.1145/1540276.1540302\\n[297] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. 2024. Androidinthewild: A\\nlarge-scale dataset for android device control. Adv. Neural Inf. Process. Syst. 36,. 2024.\\n[298] Stephen Robertson, and Hugo Zaragoza. 2009. The probabilistic relevance framework: BM25 and beyond. Found.\\nTrends Inf. Ret. 3, 4 (2009), 333–389. DOI: https://doi.org/10.1561/1500000019\\n[299] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal\\nRemez, Jérémy Rapin, et al. 2023. Code Llama: Open foundation models for code. arXiv:2308.12950. Retrieved from\\nhttps://arxiv.org/abs/2308.12950\\n[300] Caitlin Sadowski, and Greg Levin. 2007. Simhash: Hash-Based Similarity Detection. Technical report, Google.\\n[301] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd\\nschema challenge at scale. Commun. ACM 64, 9 (2021), 99–106. DOI: https://doi.org/10.1145/3474381\\n[302] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer,\\nNicola Cancedda, and Thomas Scialom. 2024. Toolformer: Language models can teach themselves to use tools. Adv.\\nNeural Inf. Process. Syst. 36, (2024). DOI: https://doi.org/10.18653/v1/2025.realm-1.14\\n[303] Rico Sennrich, Jannis Vamvas, and Alireza Mohammadshahi. 2023. Mitigating hallucinations and off-target machine\\ntranslation with source-contrastive and language-contrastive decoding. arXiv:2309.07098. Retrieved from https:\\n//arxiv.org/abs/2309.07098\\n[304] Zeyang Sha, and Yang Zhang. 2024. Prompt stealing attacks against large language models. arXiv:2402.12959.\\nRetrieved from https://arxiv.org/abs/2402.12959\\n[305] Yu Shang, Yu Li, Fengli Xu, and Yong Li. 2024. Synergy-of-thoughts: Eliciting efficient reasoning in hybrid language\\nmodels. arXiv:2402.02563. Retrieved from https://arxiv.org/abs/2402.02563\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 79}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:79\\n[306] Yuzhang Shang, Zhihang Yuan, Qiang Wu, and Zhen Dong. 2023. PB-LLM: Partially binarized large language models.\\narXiv:231000034. Retrieved from https://arxiv.org/abs/2310.00034\\n[307] Hang Shao, Bei Liu, and Yanmin Qian. 2024. One-shot sensitivity-aware mixed sparsity pruning for large language\\nmodels. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE,\\n11296–11300.\\n[308] Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng,\\nEsin Durmus, Zac Hatfield-Dodds, Scott R. Johnston, et al. 2023. Towards understanding sycophancy in language\\nmodels. arXiv:2310.13548. Retrieved from https://arxiv.org/abs/2310.13548\\n[309] Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, and Nael Abu-Ghazaleh. 2023. Survey of\\nvulnerabilities in large language models revealed by adversarial attacks. arXiv:2310.10844. Retrieved from https:\\n//arxiv.org/abs/2310.10844\\n[310] Noam Shazeer. 2019. Fast transformer decoding: One write-head is all you need. arXiv:1911.02150. Retrieved from\\nhttps://arxiv.org/abs/1911.02150\\n[311] Noam Shazeer. 2020. Glu variants improve transformer. arXiv:2002.05202. Retrieved from https://arxiv.org/abs/2002.\\n05202\\n[312] Bowen Shen, Zheng Lin, Yuanxin Liu, Zhengxiao Liu, Lei Wang, and Weiping Wang. 2022. COST-EFF: Collaborative\\noptimization of spatial and temporal efficiency with slenderized multi-exit language models. In Proceedings of the 2022\\nConference on Empirical Methods in Natural Language Processing, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang\\n(Eds.). Association for Computational Linguistics, 1719–1730. https://doi.org/10.18653/v1/2022.emnlp-main.112\\n[313] Bowen Shen, Zheng Lin, Daren Zha, Wei Liu, Jian Luan, Bin Wang, and Weiping Wang. 2024. Pruning Large Language\\nModels to Intra-module Low-rank Architecture with Transitional Activations. In Findings of the Association for\\nComputational Linguistics: ACL 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for\\nComputational Linguistics, 9781–9793. https://doi.org/10.18653/v1/2024.findings-acl.582\\n[314] Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen, Ji Zhang, and Fei Huang.\\n2024. Small LLMs are weak tool learners: A Multi-LLM agent. In Proceedings of the 2024 Conference on Empirical\\nMethods in Natural Language Processing. Association for Computational Linguistics, 16658–16680. https://doi.org/10.\\n18653/v1/2024.emnlp-main.929\\n[315] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré,\\nIon Stoica, and Ce Zhang. 2023. Flexgen: High-throughput generative inference of large language models with a\\nsingle gpu. In International Conference on Machine Learning. PMLR, 31094–31116.\\n[316] Wentao Shi, Xiangnan He, Yang Zhang, Chongming Gao, Xinyue Li, Jizhi Zhang, Qifan Wang, and Fuli Feng. 2024.\\nLarge language models are learnable planners for long-term recommendation. In Proceedings of the 47th International\\nACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 1893–1903.\\n[317] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019.\\nMegatron-LM: Training multi-billion parameter language models using model parallelism. arXiv:1909.08053. Re-\\ntrieved from https://arxiv.org/abs/1909.08053\\n[318] Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, and Chandan K. Reddy. 2024. LLM-SR:\\nScientific equation discovery via programming with large language models. arXiv:2404.18400. Retrieved from\\nhttps://arxiv.org/abs/2404.18400\\n[319] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. 2022.\\nTest-time prompt tuning for zero-shot generalization in vision-language models. Adv. Neural Inf. Process. Syst. 35,\\n(2022), 14274–14289. DOI: https://doi.org/10.5555/3600270.3601308\\n[320] Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay\\nTanwani, Heather Cole-Lewis, Stephen Pfohl, et al. 2023. Large language models encode clinical knowledge. Nature\\n620, 7972 (2023), 172–180. DOI: https://doi.org/10.1038/s41586-023-06291-2\\n[321] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R. Steeves, Joel Hestness, and Nolan Dey. 2023. SlimPajama:\\nA 627B token cleaned and deduplicated version of RedPajama. Retrieved from https://cerebras.ai/blog/slimpajama-a-\\n627b-token-cleaned-and-deduplicated-version-of-redpajama; https://huggingface.co/datasets/cerebras/SlimPajama-\\n627B\\n[322] Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi\\nChandu, Jennifer Dumas, Yanai Elazar, et al. 2024. Dolma: An Open Corpus of Three Trillion Tokens for Language\\nModel Pretraining Research. arXiv:2402.00159. Retrieved from https://arxiv.org/abs/2402.00159\\n[323] Sofia Eleni Spatharioti, David M. Rothschild, Daniel G. Goldstein, and Jake M. Hofman. 2023. Comparing traditional\\nand LLM-based search for consumer choice: A randomized experiment. arXiv:2307.03744. Retrieved from https:\\n//arxiv.org/abs/2307.03744\\n[324] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer\\nwith rotary position embedding. Neurocomputing 568, (2024), 127063. DOI: https://doi.org/10.1016/j.neucom.2023.\\n127063\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 80}, page_content='145:80\\nF. Wang et al.\\n[325] Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. 2024. Scieval: A\\nmulti-level large language model evaluation benchmark for scientific research. In Proceedings of the AAAI Conference\\non Artificial Intelligence, Vol. 38, 19053–19061.\\n[326] Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan\\nZhang, Xiner Li, et al. 2024. TrustLLM: Trustworthiness in large language models. arXiv:2401.05561. Retrieved from\\nhttps://arxiv.org/abs/2401.05561\\n[327] Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. 2024. A simple and effective pruning approach for large\\nlanguage models. In Proceedings of the Twelfth International Conference on Learning Representations. ICLR.\\n[328] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. 2020. Mobilebert: A compact\\ntask-agnostic BERT for resource-limited devices. arXiv:2004.02984. Retrieved from https://arxiv.org/abs/2004.02984\\n[329] Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A. Smith, and\\nYejin Choi. 2020. Dataset cartography: Mapping and diagnosing datasets with training dynamics. In Proceedings of\\nthe 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 9275–9293.\\n[330] Alon Talmor, and Jonathan Berant. 2018. The web as a Knowledge-Base for answering complex questions. In\\nProceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies, Volume 1 (Long Papers), Marilyn Walker, Heng Ji, and Amanda Stent (Eds.). Association\\nfor Computational Linguistics, 641–651. DOI: https://doi.org/10.18653/v1/N18-1059\\n[331] Jiejun Tan, Zhicheng Dou, Yutao Zhu, Peidong Guo, Kun Fang, and Ji-Rong Wen. 2024. Small models, big insights:\\nleveraging slim proxy models to decide when and what to retrieve for LLMs. arXiv:2402.12052. Retrieved from\\nhttps://arxiv.org/abs/2402.12052\\n[332] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun. 2023. Toolalpaca: Generalized\\ntool learning for language models with 3000 simulated cases. arXiv:2306.05301. Retrieved from https://arxiv.org/abs/\\n2306.05301\\n[333] Xuemei Tang, Jun Wang, and Qi Su. 2024. Small language model is a good guide for large language model in Chinese\\nentity relation extraction. arXiv:2402.14373. Retrieved from https://arxiv.org/abs/2402.14373\\n[334] Yehui Tang, Fangcheng Liu, Yunsheng Ni, Yuchuan Tian, Zheyuan Bai, Yi-Qi Hu, Sichao Liu, Shangling Jui, Kai\\nHan, and Yunhe Wang. 2024. Rethinking optimization and architecture for tiny language models. arXiv:2402.02791.\\nRetrieved from https://arxiv.org/abs/2402.02791\\n[335] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.\\nHashimoto. 2023. Stanford Alpaca: An instruction-following LLaMA model. Retrieved from https://github.com/tatsu-\\nlab/stanford_alpaca\\n[336] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton,\\nViktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. arXiv:2211.09085. Retrieved\\nfrom https://arxiv.org/abs/2211.09085\\n[337] CodeGemma Team. 2024. CodeGemma: Open code models based on Gemma. arXiv:2406.11409. Retrieved from\\nhttps://arxiv.org/abs/2406.11409\\n[338] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre,\\nMorgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. 2024. Gemma: Open models based on gemini research and\\ntechnology. arXiv:2403.08295. Retrieved from https://arxiv.org/abs/2403.08295\\n[339] Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas\\nMesnard, Bobak Shahriari, Alexandre Ramé, et al. 2024. Gemma 2: Improving open language models at a practical\\nsize. arXiv:2408.00118. Retrieved from https://arxiv.org/abs/2408.00118\\n[340] TensorOpera Team. 2024. TensorOpera Unveils Fox Foundation Model: A Pioneering Small Language Model (SLM) for\\nCloud and Edge. Retrieved from https://blog.tensoropera.ai/tensoropera-unveils-fox-foundation-model-a-pioneering-\\nopen-source-slm-leading-the-way-against-tech-giants/. Accessed: June 13, 2024.\\n[341] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. 2016. Branchynet: Fast inference via early exiting\\nfrom deep neural networks. In 2016 23rd International Conference on Pattern Recognition (ICPR). IEEE, 2464–2469.\\n[342] Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M. Anwer, Michael Felsberg, Tim Baldwin,\\nEric P. Xing, and Fahad Shahbaz Khan. 2024. Mobillama: Towards accurate and lightweight fully transparent GPT.\\narXiv:2402.16840. Retrieved from https://arxiv.org/abs/2402.16840\\n[343] Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. 2024. Toward self-improvement\\nof LLMs via imagination, searching, and criticizing. arXiv:2404.12253. Retrieved from https://arxiv.org/abs/arXiv:\\n2404.12253.\\n[344] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste\\nRozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models.\\narXiv:2302.13971. Retrieved from https://arxiv.org/abs/2302.13971\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 81}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:81\\n[345] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.\\narXiv:2307.09288. Retrieved from https://arxiv.org/abs/2307.09288\\n[346] Jonathan Tow, Marco Bellagente, Dakota Mahan, and Carlos Riquelme. 2024. StableLM 3B 4E1T. Retrieved from\\nhttps://huggingface.co/stabilityai/stablelm-3b-4e1t\\n[347] Trieu H. Trinh, and Quoc V. Le. 2018. A simple method for commonsense reasoning. arXiv:1806.02847. Retrieved\\nfrom https://arxiv.org/abs/1806.02847\\n[348] Adina Trufinescu. 2024. Discover the New Multi-Lingual High-Quality Phi-3.5 SLMs. Retrieved from\\nhttps://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/discover-the-new-multi-lingual-high-quality-phi-\\n3-5-slms/ba-p/4225280\\n[349] Dennis Ulmer, Martin Gubri, Hwaran Lee, Sangdoo Yun, and Seong Joon Oh. 2024. Calibrating large language models\\nusing their generations only. arXiv:2403.05973. Retrieved from https://arxiv.org/abs/2403.05973\\n[350] Sander Van Der Linden. 2022. Misinformation: Susceptibility, spread, and interventions to immunize the public. Nat.\\nMed. 28, 3 (2022), 460–467. DOI: https://doi.org/10.1038/s41591-022-01713-6\\n[351] Chien Van Nguyen, Xuan Shen, Ryan Aponte, Yu Xia, Samyadeep Basu, Zhengmian Hu, Jian Chen, Mihir Parmar,\\nSasidhar Kunapuli, Joe Barrow, et al. 2024. A survey of small language models. arXiv:2410.20011. Retrieved from\\nhttps://arxiv.org/abs/2410.20011\\n[352] A. Vaswani. 2017. Attention is all you need. Adv. Neural Inf. Process. Syst. 30 (2017), 5998–6008. DOI: https://doi.org/\\n10.5555/3295222.3295349\\n[353] Olga Veksler. 2023. Test time adaptation with regularized loss for weakly supervised salient object detection. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 7360–7369.\\n[354] Paul Voigt, and Axel Von Dem Bussche. 2017. The EU general data protection regulation (GDPR). A Practical Guide\\n(1st ed.). Springer International Publishing, Cham.\\n[355] Yuxian Wan, Wenlin Zhang, and Zhen Li. 2023. Multi-Task feature Self-Distillation for Semi-Supervised machine\\ntranslation. In International Conference on Neural Information Processing. Springer, 238–254.\\n[356] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A Multi-Task\\nbenchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop\\nBlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, 353–355.\\n[357] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik\\nDutta, Rylan Schaeffer, et al. 2023. DecodingTrust: A comprehensive assessment of trustworthiness in GPT models.\\nIn Proceedings of the Annual Conference on Neural Information Processing Systems.\\n[358] Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, and Bo Li.\\n2021. Adversarial glue: A multi-task benchmark for robustness evaluation of language models. arXiv:2111.02840.\\nRetrieved from https://arxiv.org/abs/2111.02840\\n[359] Fali Wang, Minhua Lin, Yao Ma, Hui Liu, Qi He, Xianfeng Tang, Jiliang Tang, Jian Pei, and Suhang Wang. 2025. A\\nsurvey on small language models in the era of large language models: Architecture, capabilities, and trustworthiness.\\nIn. Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2 (KDD ’25). ACM,\\n6173–6183. DOI: https://doi.org/10.1145/3711896.3736563\\n[360] Fali Wang, Hui Liu, Zhenwei Dai, Jingying Zeng, Zhiwei Zhang, Zongyu Wu, Chen Luo, Zhen Li, Xianfeng Tang, Qi\\nHe, et al. 2025. AgentTTS: Large language model agent for test-time compute-optimal scaling strategy in complex\\ntasks. arXiv:2508.00890. Retrieved from https://arxiv.org/abs/2508.00890\\n[361] Guan Wang, Sijie Cheng, Qiying Yu, and Changling Liu. 2023. OpenLLMs: Less is More for Open-Source Models.\\nDOI: https://doi.org/10.5281/zenodo.8105775\\n[362] Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2024. OpenChat: Advancing open-\\nsource language models with Mixed-Quality data. In The Twelfth International Conference on Learning Representations.\\nRetrieved from https://openreview.net/forum?id=AOJyfhWYHf\\n[363] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi\\nWu, and Furu Wei. 2023. Bitnet: Scaling 1-bit transformers for large language models. arXiv:2310.11453. Retrieved\\nfrom https://arxiv.org/abs/2310.11453\\n[364] Jindong Wang, H. U. Xixu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Wei Ye, Haojun Huang,\\nXiubo Geng, et al. n.d. On the robustness of ChatGPT: An adversarial and out-of-distribution perspective. In ICLR\\n2023 Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models.\\n[365] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024. Improving text\\nembeddings with large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational\\nLinguistics, Vol. 1 Long Papers. Association for Computational Linguistics, 11897–11916. https://doi.org/10.18653/v1/\\n2024.acl-long.642\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 82}, page_content='145:82\\nF. Wang et al.\\n[366] Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, and Xiang Ren. 2023. SCOTT: Self-Consistent\\nchain-of-thought distillation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics,\\nVol. 1 Long Papers, Association for Computational Linguistics, 5546–5558.\\n[367] Wenxiao Wang, Wei Chen, Yicong Luo, Yongliu Long, Zhengkai Lin, Liye Zhang, Binbin Lin, Deng Cai, and Xiaofei\\nHe. 2024. Model compression and efficient inference for large language models: A survey. arXiv:2402.09748. Retrieved\\nfrom https://arxiv.org/abs/2402.09748\\n[368] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. MiniLM: Deep self-attention\\ndistillation for task-agnostic compression of pre-trained transformers. arXiv:2002.10957. Retrieved from https:\\n//arxiv.org/abs/2002.10957\\n[369] Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang\\nZhang, Yizhou Sun, and Wei Wang. 2024. SciBench: Evaluating College-Level scientific Problem-Solving abilities\\nof large language models. In Forty-First International Conference on Machine Learning. Retrieved from https://\\nopenreview.net/forum?id=bq1JEgioLr\\n[370] Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy Baldwin. 2024. Do-Not-Answer: Evaluating Safe-\\nguards in LLMs. In Findings of the Association for Computational Linguistics: EACL ’24. Association for Computational\\nLinguistics, 896–911. Retrieved from https://aclanthology.org/2024.findings-eacl.61\\n[371] Yile Wang, Peng Li, Maosong Sun, and Yang Liu. 2023. Self-Knowledge guided retrieval augmentation for large lan-\\nguage models. In Findings of the Association for Computational Linguistics: EMNLP ’23. Association for Computational\\nLinguistics, 10303–10315.\\n[372] Yubo Wang, Xueguang Ma, and Wenhu Chen. 2023. Augmenting black-box LLMS with medical textbooks for clinical\\nquestion answering. arXiv:2309.02233. Retrieved from https://arxiv.org/abs/2309.02233\\n[373] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaeiandet al. 2022. Super-\\nNaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In EMNLP ’22. Association for\\nComputational Linguistics, 5085–5109. DOI: https://doi.org/10.18653/v1/2022.emnlp-main.340\\n[374] Yuyan Wang, Mohit Sharma, Can Xu, Sriraj Badam, Qian Sun, Lee Richardson, Lisa Chung, Ed. H. Chi, and Minmin\\nChen. 2022. Surrogate for Long-Term user experience in recommender systems. In Proceedings of the 28th ACM\\nSIGKDD Conference on Knowledge Discovery and Data Mining. ACM, 4100–4109. DOI: https://doi.org/10.1145/3534678.\\n3539073\\n[375] Yuling Wang, Changxin Tian, Binbin Hu, Yanhua Yu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou, Liang Pang, and Xiao\\nWang. 2024. Can small language models be good reasoners for sequential recommendation? In Proceedings of the\\nACM on Web Conference 2024. ACM, 3876–3887.\\n[376] Yuqing Wang, and Yun Zhao. 2024. RUPBench: Benchmarking reasoning under perturbations for robustness evalua-\\ntion in large language models. arXiv:2406.11020. Retrieved from https://arxiv.org/abs/2406.11020\\n[377] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai,\\nand Quoc V. Le. 2022. Finetuned language models are Zero-Shot learners. In International Conference on Learning\\nRepresentations. Retrieved from https://openreview.net/forum?id=gEZrGCozdqR\\n[378] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2023. Magicoder: Source code is all you\\nneed. arXiv:2312.02120. Retrieved from https://arxiv.org/abs/2312.02120\\n[379] Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne Hendricks, Kirsty\\nAnderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang. 2021. Challenges in detoxifying language models. In\\nFindings of the Association for Computational Linguistics: EMNLP ’21. Association for Computational Linguistics,\\n2447–2469. DOI: https://doi.org/10.18653/v1/2021.findings-emnlp.210\\n[380] Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice science questions. In\\nProceedings of the 3rd Workshop on Noisy User-Generated Text, 94–106.\\n[381] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang,\\nand Yunxin Liu. 2024. AutoDroid: LLM-powered task automation in android. arXiv:2308.15272. Retrieved from\\nhttps://arxiv.org/abs/2308.15272\\n[382] Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. 2024. QuRating: Selecting High-Quality data\\nfor training language models. In Forty-First International Conference on Machine Learning. Retrieved from https:\\n//openreview.net/forum?id=GLGYYqPwjy\\n[383] Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A Broad-Coverage challenge corpus for sentence\\nunderstanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies, Vol. 1 Long Papers, Association for Computational\\nLinguistics, 1112–1122. DOI: https://doi.org/10.18653/v1/N18-1101\\n[384] Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2021. Empowering news recommendation with pre-trained\\nlanguage models. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in\\nInformation Retrieval. ACM, 1652–1656.\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 83}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:83\\n[385] Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Fikri Aji. 2024. Minghao LaMini-LM: A\\ndiverse herd of distilled models from Large-Scale instructions. In Proceedings of the 18th Conference of the European\\nChapter of the Association for Computational Linguistics (Volume 1: Long Papers), Yvette Graham and Matthew Purver\\n(Eds.). Association for Computational Linguistics, 944–964. Retrieved from https://aclanthology.org/2024.eacl-long.57\\n[386] Taiqiang Wu, Cheng Hou, Shanshan Lao, Jiayi Li, Ngai Wong, Zhe Zhao, and Yujiu Yang. 2024. Weight-Inherited\\nDistillation for Task-Agnostic BERT Compression. In Findings of the Association for Computational Linguistics:\\nNAACL 2024, Kevin Duh, Helena Gomez, and Steven Bethard (Eds.). Association for Computational Linguistics,\\n13–28. https://doi.org/10.18653/v1/2024.findings-naacl.2\\n[387] Xuansheng Wu, Huachi Zhou, Yucheng Shi, Wenlin Yao, Xiao Huang, and Ninghao Liu. 2024. Could small language\\nmodels serve as recommenders? Towards data-centric cold-start recommendation. In Proceedings of the ACM on Web\\nConference 2024. ACM, 3566–3575.\\n[388] Zhuofeng Wu, He Bai, Aonan Zhang, Jiatao Gu, V. G. Vydiswaran, Navdeep Jaitly, and Yizhe Zhang. 2024. Divide-or-\\nconquer? Which part should you distill your LLM? arXiv:2402.15000. Retrieved from https://arxiv.org/abs/2402.15000\\n[389] Nuwa Xi, Yuhan Chen, Sendong Zhao, Haochun Wang, Bing Qin, and Ting Liu. 2024. AS-ES learning: Towards\\nefficient CoT learning in small models. arXiv:2403.01969. Retrieved from https://arxiv.org/abs/2403.01969\\n[390] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. 2024. Sheared LLaMA: Accelerating language model\\npre-training via structured pruning. In The Twelfth International Conference on Learning Representations. Retrieved\\nfrom https://openreview.net/forum?id=09iOdaeOzp\\n[391] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. 2023. Smoothquant: Accurate\\nand efficient post-training quantization for large language models. In International Conference on Machine Learning.\\nPMLR, 38087–38099.\\n[392] Tinghao Xie, Xiangyu Qi, Yi Zeng, Yangsibo Huang, Udari Madhushani Sehwag, Kaixuan Huang, Luxi He, Boyi Wei,\\nDacheng Li, Ying Sheng, et al. 2024. Sorry-bench: Systematically evaluating large language model safety refusal\\nbehaviors. arXiv:2406.14598. Retrieved from https://arxiv.org/abs/2406.14598\\n[393] Tong Xie, Yuwei Wan, Wei Huang, Zhenyu Yin, Yixuan Liu, Shaozhou Wang, Qingyuan Linghu, Chunyu Kit, Clara\\nGrazian, Wenjie Zhang, et al. 2023. Darwin series: Domain specific large language models for natural science.\\narXiv:2308.13565. Retrieved from https://arxiv.org/abs/2308.13565\\n[394] Weikai Xie, Li Zhang, Shihe Wang, Rongjie Yi, and Mengwei Xu. 2024. DroidCall: A dataset for LLM-powered\\nandroid intent invocation. arXiv:2412.00402. Retrieved from https://arxiv.org/abs/2412.00402\\n[395] Xuan Xie, Jiayang Song, Zhehua Zhou, Yuheng Huang, Da Song, and Lei Ma. 2024. Online safety analysis for LLMs:\\nA benchmark, an assessment, and a path forward. arXiv:2404.08517. Retrieved from https://arxiv.org/abs/2404.08517\\n[396] Ji Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin. 2021. BERxiT: Early exiting for BERT with better Fine-Tuning\\nand extension to regression. In Proceedings of the 16th Conference of the European Chapter of the Association for\\nComputational Linguistics: Main Volume, Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty (Eds.). Association for\\nComputational Linguistics, Online, 91–104. https://doi.org/10.18653/v1/2021.eacl-main.8\\n[397] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023.\\nWizardlm: Empowering large language models to follow complex instructions. arXiv:2304.12244. Retrieved from\\nhttps://arxiv.org/abs/2304.12244\\n[398] Canwen Xu, Yichong Xu, Shuohang Wang, Yang Liu, Chenguang Zhu, and Julian McAuley. 2023. Small models are\\nvaluable plug-ins for large language models. arXiv:2305.08848. Retrieved from https://arxiv.org/abs/2305.08848\\n[399] Daliang Xu, Wangsong Yin, Xin Jin, Ying Zhang, Shiyun Wei, Mengwei Xu, and Xuanzhe Liu. 2023. LLMCad: Fast and\\nscalable on-device large language model inference. arXiv:2309.04255. Retrieved from https://arxiv.org/abs/2309.04255\\n[400] Daliang Xu, Hao Zhang, Liming Yang, Ruiqi Liu, Gang Huang, Mengwei Xu, and Xuanzhe Liu. 2024. Empowering\\n1000 tokens/second on-device LLM prefilling with MLLM-NPU. arXiv:2407.05858. Retrieved from https://arxiv.org/\\nabs/2407.05858\\n[401] Jiaming Xu, Peng Wang, Guanhua Tian, Bo Xu, Jun Zhao, Fangyuan Wang, and Hongwei Hao. 2015. Short text\\nclustering via convolutional neural networks. In Proceedings of the 1st Workshop on Vector Space Modeling for\\nNatural Language Processing, Phil Blunsom, Shay Cohen, Paramveer Dhillon, and Percy Liang (Eds.). Association for\\nComputational Linguistics, 62–69. DOI: https://doi.org/10.3115/v1/W15-1509\\n[402] Minrui Xu, Niyato Dusit, Jiawen Kang, Zehui Xiong, Shiwen Mao, Zhu Han, Dong In Kim, and Khaled B. Letaief.\\n2024. When large language model agents meet 6G networks: Perception, grounding, and alignment. arXiv:2401.07764.\\nRetrieved from https://arxiv.org/abs/2401.07764\\n[403] Mengwei Xu, Wangsong Yin, Dongqi Cai, Rongjie Yi, Daliang Xu, Qipeng Wang, Bingyang Wu, Yihao Zhao, Chen\\nYang, Shihe Wang, et al. 2024. A survey of resource-efficient llm and multimodal foundation models. arXiv:2401.08092.\\nRetrieved from https://arxiv.org/abs/2401.08092\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 84}, page_content='145:84\\nF. Wang et al.\\n[404] Weijia Xu, Sweta Agrawal, Eleftheria Briakou, Marianna Martindale, and Marine Carpuat. 2023. Understanding and\\ndetecting hallucinations in neural machine translation via model introspection. Trans. Assoc. Comput. Linguist. 11,\\n(2023), 546–564. DOI: https://doi.org/10.1162/tacl_a_00563\\n[405] Yuzhuang Xu, Xu Han, Zonghan Yang, Shuo Wang, Qingfu Zhu, Zhiyuan Liu, Weidong Liu, and Wanxiang Che.\\n2024. OneBit: Towards extremely low-bit large language models. arXiv:2402.11295. Retrieved from https://arxiv.org/\\nabs/2402.11295\\n[406] Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective retrieval augmented generation.\\narXiv:2401.15884. Retrieved from https://arxiv.org/abs/2401.15884\\n[407] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng\\nLiu, Fei Huang, et al. 2024. Qwen2 Technical Report. arXiv:2407.10671. Retrieved from https://arxiv.org/abs/2407.10671\\n[408] Chuanpeng Yang, Wang Lu, Yao Zhu, Yidong Wang, Qian Chen, Chenlong Gao, Bingjie Yan, and Yiqiang Chen. 2024.\\nSurvey on knowledge distillation for large language models: Methods, evaluation, and application. arXiv:2407.01885.\\nRetrieved from https://arxiv.org/abs/2407.01885\\n[409] Cheng Yang, Yang Sui, Jinqi Xiao, Lingyi Huang, Yu Gong, Yuanlin Duan, Wenqi Jia, Miao Yin, Yu Cheng, and\\nBo Yuan. 2024. MoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert\\nLow-Rank Decomposition. In Findings of the Association for Computational Linguistics: EMNLP 2024, Yaser Al-\\nOnaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, 10456–10466. DOI:\\nhttps://doi.org/10.18653/v1/2024.findings-emnlp.612\\n[410] Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, and Yuandong Tian. 2024. RLCD: Reinforcement learning\\nfrom contrastive distillation for LM alignment. In The Twelfth International Conference on Learning Representations.\\nRetrieved from https://openreview.net/forum?id=v3XXtxWKi6\\n[411] Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, Jimin Huang, and Sophia Ananiadou. 2024. MentaLLaMA:\\nInterpretable mental health analysis on social media with large language models. In Proceedings of the ACM on Web\\nConference 2024, 4489–4500.\\n[412] Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue Zhang.\\n2023. GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-Distribution Generalization\\nPerspective. In Findings of the Association for Computational Linguistics: ACL ’23. Association for Computational\\nLinguistics, 12731–12750. DOI: https://doi.org/10.18653/v1/2023.findings-acl.806\\n[413] Linyi Yang, Shuibai Zhang, Zhuohao Yu, Guangsheng Bao, Yidong Wang, Jindong Wang, Ruochen Xu, Wei Ye, Xing\\nXie, Weizhu Chen, and Yue Zhang. 2024. Supervised knowledge makes large language models better In-context\\nlearners. In The Twelfth International Conference on Learning Representations. Retrieved from https://openreview.net/\\nforum?id=bAMPOUF227\\n[414] Runming Yang, Taiqiang Wu, Jiahao Wang, Pengfei Hu, Ngai Wong, and Yujiu Yang. 2024. LLM-Neo: Parameter\\nefficient knowledge distillation for large language models. arXiv:2411.06839. Retrieved from https://arxiv.org/abs/\\n2411.06839\\n[415] Yifei Yang, Zouying Cao, and Hai Zhao. 2024. Laco: Large language model pruning via layer collapse. arXiv:2402.11187.\\nRetrieved from https://arxiv.org/abs/2402.11187\\n[416] Yu Yang, Siddhartha Mishra, Jeffrey N. Chiang, and Baharan Mirzasoleiman. 2024. SmallToLarge (S2L): Scalable\\ndata selection for fine-tuning large language models by summarizing training trajectories of small models. In The\\nThirty-Eighth Annual Conference on Neural Information Processing Systems. Retrieved from https://openreview.net/\\nforum?id=K9IGlMQpif\\n[417] Yizhe Yang, Huashan Sun, Jiawei Li, Runheng Liu, Yinghao Li, Yuhang Liu, Heyan Huang, and Yang Gao. 2023.\\nMindllm: Pre-training lightweight large language model from scratch, evaluations and domain applications.\\narXiv:2310.15777. Retrieved from https://arxiv.org/abs/2310.15777\\n[418] Zhou Yang, Zhaochun Ren, Wang Yufeng, Shizhong Peng, Haizhou Sun, Xiaofei Zhu, and Xiangwen Liao. 2024. En-\\nhancing empathetic response generation by augmenting LLMs with small-scale empathetic models. arXiv:2402.11801.\\nRetrieved from https://arxiv.org/abs/2402.11801\\n[419] Yunzhi Yao, Shaohan Huang, Wenhui Wang, Li Dong, and Furu Wei. 2021. Adapt-and-distill: Developing small, fast and\\neffective pretrained language models for domains. arXiv:2106.13474. Retrieved from https://arxiv.org/abs/2106.13474.\\n[420] Mert Yazan, Suzan Verberne, and Frederik Situmeang. 2024. The impact of quantization on retrieval-augmented\\ngeneration: An analysis of small LLMs. arXiv:2406.10251. Retrieved from https://arxiv.org/abs/2406.10251\\n[421] Rongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shangguang Wang, and Mengwei Xu. 2023. EdgeMoE: Fast on-device\\ninference of MoE-based large language models. arXiv:2308.14352. Retrieved from https://arxiv.org/abs/2308.14352\\n[422] Rongjie Yi, Xiang Li, Weikai Xie, Zhenyan Lu, Chenghua Wang, Ao Zhou, Shangguang Wang, Xiwen Zhang, and\\nMengwei Xu. 2024. PhoneLM: An efficient and capable small language model family through principled pre-training.\\narXiv:2411.05046. Retrieved from https://arxiv.org/abs/2411.05046\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 85}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:85\\n[423] Wen-Tau Yih, Matthew Richardson, Chris Meek, Ming-Wei Chang, and Jina Suh. 2016. The value of semantic parse\\nlabeling for knowledge base question answering. In Proceedings of the 54th Annual Meeting of the Association for Com-\\nputational Linguistics (Volume 2: Short Papers), Katrin Erk and Noah A. Smith (Eds.). Association for Computational\\nLinguistics, 201–206. DOI: https://doi.org/10.18653/v1/P16-2033\\n[424] Wangsong Yin, Mengwei Xu, Yuanchun Li, and Xuanzhe Liu. 2024. LLM as a system service on mobile devices.\\narXiv:240311805. Retrieved from https://arxiv.org/abs/2403.11805\\n[425] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller,\\nand Weiyang Liu. 2024. MetaMath: Bootstrap your own mathematical questions for large language models. In The\\nTwelfth International Conference on Learning Representations. Retrieved from https://openreview.net/forum?id=\\nN8N0hgNDRt\\n[426] Yue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, Mohammad Shoeybi, and Bryan Catanzaro.\\n2024. Rankrag: Unifying context ranking with retrieval-augmented generation in LLMS. arXiv:2407.02485. Retrieved\\nfrom https://arxiv.org/abs/2407.02485\\n[427] Zhongzhi Yu, Zheng Wang, Yuhan Li, Haoran You, Ruijie Gao, Xiaoya Zhou, Sreenidhi Reedy Bommu, Yang Katie\\nZhao, and Yingyan Celine Lin. 2024. EDGE-LLM: Enabling efficient large language model adaptation on edge\\ndevices via layerwise unified compression and adaptive layer tuning and voting. arXiv:2406.15758. Retrieved from\\nhttps://arxiv.org/abs/2406.15758\\n[428] Jinliang Yuan, Chen Yang, Dongqi Cai, Shihe Wang, Xin Yuan, Zeling Zhang, Xiang Li, Dingge Zhang, Hanzi Mei,\\nXianqing Jia, et al. 2024. Mobile foundation model as firmware. In Proceedings of the 30th Annual International\\nConference on Mobile Computing and Networking, 279–295.\\n[429] Sha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding, Xiao Liu, Yukuo Cen, Xu Zou, Zhilin Yang, and Jie Tang. 2021.\\nWudaocorpora: A super large-scale Chinese Corpora for pre-training language models. AI Open 2, (2021), 65–68.\\nDOI: https://doi.org/10.1016/j.aiopen.2021.06.001\\n[430] Xiaohan Yuan, Jinfeng Li, Dongxia Wang, Yuefeng Chen, Xiaofeng Mao, Longtao Huang, Hui Xue, Wenhai Wang,\\nKui Ren, and Jingyi Wang. 2024. S-Eval: Automatic and adaptive test generation for benchmarking safety evaluation\\nof large language models. arXiv:2405.14191. Retrieved from https://arxiv.org/abs/2405.14191\\n[431] Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen Tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. 2024.\\nGPT-4 is too smart to Be safe: Stealthy chat with LLMs via cipher. In The Twelfth International Conference on Learning\\nRepresentations. Retrieved from https://openreview.net/forum?id=MbfAK4s61A.\\n[432] Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen, Shujun Liu, Yuxuan Zhou, Yao Xiao, Song Yun,\\nWei Lin, et al. 2023. Disc-lawllm: Fine-tuning large language models for intelligent legal services. arXiv:2309.11325.\\nRetrieved from https://arxiv.org/abs/2309.11325\\n[433] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2024. Mammoth:\\nBuilding math generalist models through hybrid instruction tuning. In The Twelfth International Conference on\\nLearning Representations. Retrieved from https://openreview.net/forum?id=yLClGs770I\\n[434] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a machine really\\nfinish your sentence?. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.\\nAssociation for Computational Linguistics, 4791–4800.\\n[435] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019.\\nDefending against neural fake news. Adv. Neural Inf. Process. Syst. 32, (2019). DOI: https://doi.org/10.5555/3454287.\\n3455099\\n[436] Biao Zhang, and Rico Sennrich. 2019. Root mean square layer normalization. Adv. Neural Inf. Process. Syst. 32, (2019),\\n12381–12392. DOI: https://doi.org/10.5555/3454287.3455397\\n[437] Cheng Zhang, Jianyi Cheng, George A. Constantinides, and Yiren Zhao. 2024. LQER: low-rank quantization error\\nreconstruction for LLMs. arXiv:2402.02446. Retrieved from https://arxiv.org/abs/2402.02446\\n[438] Collin Zhang, John X. Morris, and Vitaly Shmatikov. 2024. Extracting prompts by inverting LLM outputs.\\narXiv:2405.15012. Retrieved from https://arxiv.org/abs/2405.15012\\n[439] Chen Zhang, Dawei Song, Zheyu Ye, and Yan Gao. 2023. Towards the law of capacity gap in distilling language\\nmodels. arXiv:2311.07052. Retrieved from https://arxiv.org/abs/2311.07052\\n[440] Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, and\\nJie Tang. 2024. Sciglm: Training scientific language models with self-reflective instruction annotation and tuning.\\narXiv:2401.07950. Retrieved from https://arxiv.org/abs/2401.07950\\n[441] Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue,\\nDongzhan Zhou, et al. 2024. ChemLLM: A chemical large language model. arXiv:2402.06852. Retrieved from https:\\n//arxiv.org/abs/2402.06852\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 86}, page_content='145:86\\nF. Wang et al.\\n[442] Kaiyan Zhang, Jianyu Wang, Ermo Hua, Biqing Qi, Ning Ding, and Bowen Zhou. 2024. Cogenesis: A framework\\ncollaborating large and small language models for secure context-aware instruction following. arXiv:2403.03129.\\nRetrieved from https://arxiv.org/abs/2403.03129\\n[443] Mingyang Zhang, Hao Chen, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, and Bohan Zhuang. 2023. Loraprune:\\nPruning meets low-rank parameter-efficient fine-tuning. arXiv:2305.18403. Retrieved from https://arxiv.org/abs/\\n2305.18403\\n[444] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. 2024. TinyLlama: An open-source small language\\nmodel. arXiv:240102385. Retrieved from https://arxiv.org/abs/2401.02385\\n[445] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona\\nDiab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv:2205.01068.\\nRetrieved from https://arxiv.org/abs/2205.01068\\n[446] Xinran Zhang, Xin Yuan, Yunwei Li, and Yanru Zhang. 2019. Cold-Start representation learning: A recommendation\\napproach with bert4Movie and movie2Vec. In Proceedings of the 27th ACM International Conference on Multimedia.\\nACM, 2612–2616.\\n[447] Yingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu Hou, and Carlo Vittorio Cannistraci. 2024. Plug-and-play:\\nAn efficient post-training pruning method for large language models. In The Twelfth International Conference on\\nLearning Representations.\\n[448] Yiming Zhang, Nicholas Carlini, and Daphne Ippolito. 2024. Effective prompt extraction from language models.\\narXiv:230706865. Retrieved from https://arxiv.org/abs/2307.06865\\n[449] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian,\\nChristopher Ré, Clark Barrett, et al. 2024. H2O: Heavy-hitter oracle for efficient generative inference of large language\\nmodels. Adv. Neural Inf. Process. Syst. 36, (2024), 34661–34710. DOI: https://doi.org/10.5555/3666122.3667628\\n[450] Bowen Zhao, Hannaneh Hajishirzi, and Qingqing Cao. 2024. APT: Adaptive pruning and tuning pretrained language\\nmodels for efficient training and inference. In Forty-First International Conference on Machine Learning.\\n[451] Junchen Zhao, Yurun Song, Simeng Liu, Ian G. Harris, and Sangeetha Abdu Jyothi. 2023. LinguaLinked: A distributed\\nlarge language model inference system for mobile devices. arXiv:2312.00388. Retrieved from https://arxiv.org/abs/\\n2312.00388\\n[452] Juntao Zhao, Borui Wan, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. LLM-PQ: Serving LLM on heterogeneous\\nclusters with phase-aware partition and adaptive quantization. arXiv:2403.01136. Retrieved from https://arxiv.org/\\nabs/2403.01136\\n[453] Kun Zhao, Bohao Yang, Chen Tang, Chenghua Lin, and Liang Zhan. 2024. SLIDE: A framework integrating small\\nand large language models for open-domain dialogues evaluation. arXiv:2405.15924. Retrieved from https://arxiv.\\norg/abs/2405.15924\\n[454] Theodore Zhao, Mu Wei, J. Samuel Preston, and Hoifung Poon. 2023. Automatic calibration and error correction for\\nlarge language models via Pareto optimal self-supervision. arXiv:2306.16564. Retrieved from https://arxiv.org/abs/\\n2306.16564\\n[455] Youpeng Zhao, Ming Lin, Huadong Tang, Qiang Wu, and Jun Wang. 2024. Merino: entropy-driven design for\\ngenerative language models on IoT devices. arXiv:240307921. Retrieved from https://arxiv.org/abs/2403.07921\\n[456] Zhengyun Zhao, Qiao Jin, Fangyuan Chen, Tuorui Peng, and Sheng Yu. 2022. PMC-patients: A large-scale dataset of\\npatient summaries and relations for benchmarking retrieval-based clinical decision support systems. arXiv:2202.13876.\\nRetrieved from https://arxiv.org/abs/2202.13876\\n[457] Zhanhui Zhou, Zhixuan Liu, Jie Liu, Zhichen Dong, Chao Yang, and Yu Qiao. 2024. Weak-to-strong search: Align\\nlarge language models via searching over small language models. In The Thirty-Eighth Annual Conference on Neural\\nInformation Processing Systems. Retrieved from https://openreview.net/forum?id=dOJ6CqWDf1\\n[458] Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng\\nChua. 2021. TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance. In\\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International\\nJoint Conference on Natural Language Processing, Vol. 1 Long Papers, Association for Computational Linguistics,\\n3277–3287.\\n[459] Jiachen Zhu, Jianghao Lin, Xinyi Dai, Bo Chen, Rong Shan, Jieming Zhu, Ruiming Tang, Yong Yu, and Weinan Zhang.\\n2024. Lifelong personalized low-rank adaptation of large language models for recommendation. arXiv:2408.03533.\\nRetrieved from https://arxiv.org/abs/2408.03533\\n[460] Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue Zhang,\\nNeil Zhenqiang Gong, et al. 2023. Promptbench: Towards evaluating the robustness of large language models on\\nadversarial prompts. arXiv:2306.04528. Retrieved from https://arxiv.org/abs/2306.04528\\n[461] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. 2023. A survey on model compression for large language\\nmodels. arXiv:2308.07633. Retrieved from https://arxiv.org/abs/2308.07633\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.'), Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': '', 'creationdate': '2025-12-23T10:51:41-08:00', 'source': 'data/pdf/SLMS.pdf', 'file_path': 'data/pdf/SLMS.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness', 'author': '', 'subject': 'ACM Trans. Intell. Syst. Technol. 2025.16:1-87', 'keywords': '', 'moddate': '2025-12-23T10:51:41-08:00', 'trapped': '', 'modDate': \"D:20251223105141-08'00'\", 'creationDate': \"D:20251223105141-08'00'\", 'page': 87}, page_content='Survey of Small Language Models in the Era of Large Language Models\\n145:87\\n[462] Yun Zhu, Yinxiao Liu, Felix Stahlberg, Shankar Kumar, Yu Hui Chen, Liangchen Luo, Lei Shu, Renjie Liu, Jindong\\nChen, and Lei Meng. 2023. Towards an on-device agent for text rewriting. arXiv:2308.11807. Retrieved from https:\\n//arxiv.org/abs/2308.11807\\n[463] Yuanyuan Zhuang, and Jaekyeong Kim. 2021. A BERT-based multi-criteria recommender system for hotel promotion\\nmanagement. Sustainability 13, 14 (2021), 8039. DOI: https://doi.org/10.3390/su13148039\\n[464] Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. 2023. Red teaming chatGPT via jailbreaking:\\nBias, robustness, reliability and toxicity. arXiv:2301.12867. Retrieved from https://arxiv.org/abs/2301.12867\\n[465] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, Zico Kolter, and Matt Fredrikson. 2023. J Universal and\\ntransferable adversarial attacks on aligned language models. arXiv:2307.15043. Retrieved from https://arxiv.org/abs/\\n2307.15043\\n[466] Lixin Zou, Weixue Lu, Yiding Liu, Hengyi Cai, Xiaokai Chu, Dehong Ma, Daiting Shi, Yu Sun, Zhicong Cheng, Simiu\\nGu, et al. 2022. Pre-trained language model-based retrieval and ranking for web search. ACM Trans. Web 17, 1 (2022),\\n1–36. DOI: https://doi.org/10.1145/3568681\\n[467] Jingwei Zuo, Maksim Velikanov, Dhia Eddine Rhaiem, Ilyas Chahed, Younes Belkada, Guillaume Kunsch, and Hakim\\nHacid. 2024. Falcon Mamba: The first competitive attention-free 7B language model. arXiv:2410.05355. Retrieved\\nfrom https://arxiv.org/abs/2410.05355\\nReceived 2 January 2025; revised 18 August 2025; accepted 23 August 2025\\nACM Transactions on Intelligent Systems and Technology, Vol. 16, No. 6, Article 145. Publication date: November 2025.')]\n",
      "1st page pdf content is\n",
      ".\n",
      ".\n",
      "Latest updates: hps://dl.acm.org/doi/10.1145/3768165\n",
      ".\n",
      ".\n",
      "SURVEY\n",
      "A Comprehensive Survey of Small Language Models\n",
      "in the Era of Large Language Models: Techniques,\n",
      "Enhancements, Applications, Collaboration with LLMs,\n",
      "and Trustworthiness\n",
      "FALI WANG, Pennsylvania State University, University Park, PA, United\n",
      "States\n",
      ".\n",
      "ZHIWEI ZHANG, Pennsylvania State University, University Park, PA,\n",
      "United States\n",
      ".\n",
      "XIANREN ZHANG, Pennsylvania State University, University Park, PA,\n",
      "United States\n",
      ".\n",
      "ZONGYU WU, Pennsylvania State University, University Park, PA, United\n",
      "States\n",
      ".\n",
      "TZU HAO MO, University of Pennsylvania, Philadelphia, PA, United\n",
      "States\n",
      ".\n",
      "QIUHAO LU, University of Texas Health Science Center at Houston,\n",
      "Houston, TX, United States\n",
      ".\n",
      "View all\n",
      ".\n",
      ".\n",
      "Open Access Support provided by:\n",
      ".\n",
      "Pennsylvania State University\n",
      ".\n",
      "Rensselaer Polytechnic Institute\n",
      ".\n",
      "University of Texas Health Science Center at Houston\n",
      ".\n",
      "University of Pennsylvania\n",
      ".\n",
      "Amazon.com, Inc.\n",
      ".\n",
      "PDF Download\n",
      "3768165.pdf\n",
      "23 December 2025\n",
      "Total Citations: 12\n",
      "Total Downloads:\n",
      "2364\n",
      ".\n",
      ".\n",
      "Published: 24 November 2025\n",
      "Online AM: 18 September\n",
      "2025\n",
      "Accepted: 23 August 2025\n",
      "Revised: 18 August 2025\n",
      "Received: 02 January 2025\n",
      ".\n",
      ".\n",
      "Citation in BibTeX format\n",
      ".\n",
      ".\n",
      "ACM Transactions on Intelligent Systems and Technology, Volume 16, Issue 6 (December 2025)\n",
      "hps://doi.org/10.1145/3768165\n",
      "EISSN: 2157-6912\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# data parsing using pdfmupdfloader\n",
    "\n",
    "try:\n",
    "    pymuPDFLoader = PyMuPDFLoader(\"data/pdf/SLMS.pdf\")\n",
    "    pymupdf_docs = pymuPDFLoader.load()\n",
    "    print(pymupdf_docs)\n",
    "    print(\"1st page pdf content is\")\n",
    "    print(pymupdf_docs[0].page_content)\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"error {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d78da4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text= \"\"\"\n",
    "Here   is   a    small    paragraph\n",
    "with   irregular spacing,\n",
    "random   line breaks,     and    extra   spaces\n",
    "\n",
    "that     don’t   quite   make   sense,\n",
    "but   still   read   fine.\n",
    "\n",
    "This     is     the     second     paragraph,\n",
    "it    also   has     odd      spacing\n",
    "and line   changes\n",
    "\n",
    "scattered      throughout      to   make\n",
    "cleaning     slightly      annoying.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9739515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic text cleaner\n",
    "def clean_text(text):\n",
    "    text = \" \".join(text.split())\n",
    "    # replacing ligatures with normal text:\n",
    "    # text.replace() => write whatever replacements you need\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1c34fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw text:\n",
      "\n",
      "Here   is   a    small    paragraph\n",
      "with   irregular spacing,\n",
      "random   line breaks,     and    extra   spaces\n",
      "\n",
      "that     don’t   quite   make   sense,\n",
      "but   still   read   fine.\n",
      "\n",
      "This     is     the     second     paragraph,\n",
      "it    also   has     odd      spacing\n",
      "and line   changes\n",
      "\n",
      "scattered      throughout      to   make\n",
      "cleaning     slightly      annoying.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------\n",
      "cleaned text: \n",
      "Here is a small paragraph with irregular spacing, random line breaks, and extra spaces that don’t quite make sense, but still read fine. This is the second paragraph, it also has odd spacing and line changes scattered throughout to make cleaning slightly annoying.\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = clean_text(raw_text)\n",
    "print(\"raw text:\")\n",
    "print(raw_text)\n",
    "\n",
    "print(\"-----------------------\")\n",
    "print(\"cleaned text: \")\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12d2777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "class SmartPdfProcessor:\n",
    "    \"Advanced pdf processing with error handling\"\n",
    "    def __init__(self,chunk_size=1000,chunk_overlap=100):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size = chunk_size,\n",
    "            chunk_overlap = chunk_overlap,\n",
    "            separators=[\" \"]\n",
    "        )\n",
    "         \n",
    "    def process_pdf(self,pdf_path: str) -> List[Document] :\n",
    "        \"\"\"Process pdf with smart chunking and metadata enhancement\"\"\"\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        pages= loader.load()\n",
    "\n",
    "        processed_chunks=[]\n",
    "\n",
    "        for page_num,page in enumerate(pages):\n",
    "            cleaned_text=self.clean_text(page.page_content)\n",
    "\n",
    "            if(len(cleaned_text.strip()) < 50):\n",
    "                continue\n",
    "\n",
    "            chunk = self.text_splitter.create_documents(\n",
    "                texts=[cleaned_text],\n",
    "                metadatas=[{\n",
    "                    **page.metadata,\n",
    "                    \"page\": page_num+1,\n",
    "                    \"total_pages\": len(pages),\n",
    "                    \"chunk_method\": \"smart_pdf_processor\",\n",
    "                    \"char_count\" : len(cleaned_text)\n",
    "                }]\n",
    "            )\n",
    "            processed_chunks.extend(chunk)\n",
    "        return processed_chunks\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean the extracted text\"\"\"\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        # Common typographic ligatures found in PDFs\n",
    "        ligatures = {\n",
    "            \"ﬀ\": \"ff\",\n",
    "            \"ﬁ\": \"fi\",\n",
    "            \"ﬂ\": \"fl\",\n",
    "            \"ﬃ\": \"ffi\",\n",
    "            \"ﬄ\": \"ffl\",\n",
    "            \"ﬅ\": \"ft\",\n",
    "            \"ﬆ\": \"st\",\n",
    "        }\n",
    "\n",
    "        for ligature, replacement in ligatures.items():\n",
    "            text = text.replace(ligature, replacement)\n",
    "\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad8080b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = SmartPdfProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "360fad9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample data for smart chunks\n",
      "producer : iText 4.2.0 by 1T3XT\n",
      "creator : PyPDF\n",
      "creationdate : 2025-12-23T10:51:41-08:00\n",
      "moddate : 2025-12-23T10:51:41-08:00\n",
      "subject : ACM Trans. Intell. Syst. Technol. 2025.16:1-87\n",
      "title : A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness\n",
      "source : data/pdf/SLMS.pdf\n",
      "total_pages : 88\n",
      "page : 1\n",
      "page_label : 1\n",
      "chunk_method : smart_pdf_processor\n",
      "char_count : 1342\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    smart_chunks = preprocessor.process_pdf(\"data/pdf/SLMS.pdf\")\n",
    "    if smart_chunks:\n",
    "        print(\"sample data for smart chunks\")\n",
    "        for key,value in smart_chunks[0].metadata.items():\n",
    "            print(f\"{key} : {value}\")\n",
    "\n",
    "except Exception as r:\n",
    "    print(r)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
